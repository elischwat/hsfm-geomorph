{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely\n",
    "import shapely\n",
    "import rasterio\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import datetime\n",
    "import profiling_tools\n",
    "import altair as alt\n",
    "from altair import datum\n",
    "import os\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "\n",
    "Provide:\n",
    "\n",
    "* Input file path to file with cross section lines/polygons to extract low points/stream profile from\n",
    "* Output file path where low points will be saved\n",
    "* Input directory path to location of DEMs\n",
    "* Parameter LINE_COMPLEXITY which is the number of points that each cross-section line is split into. LINE_COMPLEXITY elevation points will be extracted from the DEM for each cross section line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the arg, you must run from CLI like this\n",
    "\n",
    "```\n",
    "HSFM_GEOMORPH_INPUT_FILE='inputs/mazama_inputs.json' jupyter nbconvert --execute --to html dem-analysis/mt_baker_mass_wasted/xsections.ipynb  --output outputs/xsections_mazama.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or set an env arg:\n",
    "if os.environ.get('HSFM_GEOMORPH_INPUT_FILE'):\n",
    "    json_file_path = os.environ['HSFM_GEOMORPH_INPUT_FILE']\n",
    "else:\n",
    "    json_file_path = 'inputs/mazama_inputs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_path, 'r') as j:\n",
    "     params = json.loads(j.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_DROP = params['inputs']['TO_DROP']\n",
    "valley_name = params['inputs']['valley_name']\n",
    "# if this is defined, only data from these dates are analyed\n",
    "XSECTIONS_INCLUDE = params[\"inputs\"][\"XSECTIONS_INCLUDE\"]\n",
    "input_xsections_file = params['xsections']['input_xsections_file']\n",
    "output_lowpoints_file = params['xsections']['output_lowpoints_file']\n",
    "output_streamlines_file = params['xsections']['output_streamlines_file']\n",
    "input_dems_path = params['inputs']['dems_path']\n",
    "glacier_polygons_file = params['inputs']['glacier_polygons_file']\n",
    "LINE_COMPLEXITY = params['xsections']['line_complexity']\n",
    "\n",
    "group_slope_meters = params['xsections']['group_slope_meters']\n",
    "\n",
    "# Used to strip date from dem file names\n",
    "strip_time_format = params['inputs']['strip_time_format']\n",
    "\n",
    "reference_dem_date = datetime.datetime.strptime(\n",
    "    params['inputs']['reference_dem_date'], \n",
    "    strip_time_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_fns = glob.glob(os.path.join(input_dems_path, \"*.tif\"))\n",
    "if XSECTIONS_INCLUDE:\n",
    "    raster_fns = [fn for fn in raster_fns if Path(fn).stem in XSECTIONS_INCLUDE]\n",
    "else:\n",
    "    raster_fns = [fn for fn in raster_fns if Path(fn).stem not in TO_DROP]\n",
    "raster_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract profiles from DEMs\n",
    "\n",
    "Along each cross-section, extract point with lowest elevation and calculate \"path distance\", the distance from the furthest downstream cross section line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cross sections file into GeoDataframe\n",
    "gdf = gpd.read_file(input_xsections_file)\n",
    "# Increase the number of points in each line\n",
    "gdf.geometry = gdf.geometry.apply(lambda g: profiling_tools.increase_line_complexity(g, LINE_COMPLEXITY))\n",
    "# Find the centroid of each line\n",
    "gdf['centroid'] = gdf.geometry.apply(lambda x: x.centroid)\n",
    "# Get all points from the cross section lines and create a row for each point. \n",
    "gdf['coords'] = gdf.geometry.apply(lambda x: list(x.coords))\n",
    "gdf = gdf.explode('coords', ignore_index=True)\n",
    "# Make the coords column a shapely.geometry.Point type and drop the cross section geometries which we no longer need.\n",
    "gdf['coords'] = gdf['coords'].apply(shapely.geometry.Point)\n",
    "gdf.drop(columns=[\"geometry\"])\n",
    "\n",
    "combined_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "for raster in raster_fns:\n",
    "    print(raster)\n",
    "    # Extract an elevation value for each point\n",
    "    with rasterio.open(raster) as src:\n",
    "        new_gdf = gdf.copy()\n",
    "        # pnts['values'] = [sample[0] for sample in src.sample(coords)]\n",
    "        # gdf['elevation'] = gdf['coords'].apply(lambda x: [sample for sample in src.sample(x)])\n",
    "        new_gdf['elevation'] = pd.Series([sample[0] for sample in src.sample(new_gdf[\"coords\"].apply(lambda x: x.xy))])\n",
    "        new_gdf['elevation'] = new_gdf['elevation'].apply(lambda x: np.nan if x == src.nodata else x)\n",
    "        \n",
    "    # Convert file name to datetime as per the provided format\n",
    "    date = datetime.datetime.strptime(Path(raster).stem, strip_time_format)\n",
    "    new_gdf['time'] = date\n",
    "\n",
    "    # Find the point in each cross section line (identified by the ID column, with 0 meaning furthest downstream) with the lowest elevation\n",
    "    new_gdf = new_gdf.sort_values('elevation').groupby('id').apply(pd.DataFrame.head, n=1)\n",
    "    new_gdf['low_point_coords'] = new_gdf.apply(lambda row: None if np.isnan(row['elevation']) else row['coords'], axis=1)\n",
    "\n",
    "    # Set the geometry to the centroid (of the cross-section lines) to calculate \"path distance\", distance upstream from the furthest downstream cross-section\n",
    "    new_gdf.geometry = new_gdf[\"centroid\"]\n",
    "    new_gdf['path_distance'] = pd.Series(new_gdf.distance(\n",
    "            gpd.GeoDataFrame(new_gdf.shift(1), crs=new_gdf.crs)\n",
    "        ).fillna(0)).cumsum()\n",
    "    \n",
    "    combined_gdf = combined_gdf.append(new_gdf)\n",
    "\n",
    "combined_gdf = combined_gdf.set_crs(crs=gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_dist_from_glaciers(df):\n",
    "    path_distance_at_glacier = df.loc[df['n_from_glacial_max']==0, 'path_distance'].iloc[0]\n",
    "    df['path_distance_from_glacier'] = path_distance_at_glacier - df['path_distance']\n",
    "    return df\n",
    "combined_gdf = combined_gdf.groupby('time').apply(create_path_dist_from_glaciers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark points as (non)glacial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_gdf = gpd.read_file(glacier_polygons_file)\n",
    "glaciers_gdf = glaciers_gdf.to_crs(combined_gdf.crs)\n",
    "glaciers_gdf['time'] = glaciers_gdf['year'].apply(lambda d: datetime.datetime.strptime(d, strip_time_format))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_gdf['glacial'] = combined_gdf.apply(\n",
    "    lambda row: any(glaciers_gdf.loc[glaciers_gdf['time'] == row[\"time\"], 'geometry'].apply(lambda g: g.contains(row['coords']))),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot elevation profiles (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = combined_gdf[[ \"time\", \"path_distance_from_glacier\", \"elevation\", \"glacial\"]].reset_index()\n",
    "src['time'] = src['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "\n",
    "alt.Chart(\n",
    "    src\n",
    ").mark_line().encode(\n",
    "    alt.X(\"path_distance_from_glacier:Q\", title=\"Distance downstream from observed glacial maximum\"),\n",
    "    alt.Y(\"elevation:Q\", scale=alt.Scale(zero=False), impute=alt.ImputeParams(value=None), title=\"Valley floor elevation, in meters\"),\n",
    "    alt.Color(\"time:O\"),\n",
    "    alt.StrokeDash('glacial:N')\n",
    ").properties(\n",
    "    width = 600,\n",
    "    # height = 600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = combined_gdf[[ \"time\", \"path_distance_from_glacier\", \"elevation\", \"glacial\"]].reset_index()\n",
    "src['time'] = src['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "alt.Chart(\n",
    "    src\n",
    ").mark_line().transform_filter(\n",
    "    (datum.glacial == False)\n",
    ").encode(\n",
    "    alt.X(\"path_distance_from_glacier:Q\", title=\"Distance downstream from observed glacial maximum\"),\n",
    "    alt.Y(\"elevation:Q\", scale=alt.Scale(zero=False), impute=alt.ImputeParams(value=None), title=\"Valley floor elevation, in meters\"),\n",
    "    alt.Color(\"time:O\"),\n",
    "    alt.StrokeDash('glacial:N')\n",
    ").properties(\n",
    "    width = 600,\n",
    "    # height = 600\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Residuals\n",
    "\n",
    "Calculate so accumulation is always positive with time, erosion is negative with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.DataFrame()\n",
    "\n",
    "combined_gdf_grouped = combined_gdf.groupby(\"time\")\n",
    "reference_group = combined_gdf_grouped.get_group(reference_dem_date)\n",
    "\n",
    "for timestamp, df in combined_gdf_grouped:    \n",
    "    if timestamp != reference_dem_date:\n",
    "        print(timestamp)\n",
    "        this_diff_df = df.copy()\n",
    "        merged = df.merge(reference_group, on='path_distance_from_glacier')\n",
    "        if timestamp > reference_dem_date:\n",
    "            residual_values = merged['elevation_x'] - merged['elevation_y']\n",
    "        else:\n",
    "            residual_values = merged['elevation_y'] - merged['elevation_x']\n",
    "        assert len(this_diff_df) == len(residual_values)\n",
    "        this_diff_df['elevation_residual'] = list(residual_values)\n",
    "        diff_df = diff_df.append(this_diff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot elevation residuals, exclude glacier signals (large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = diff_df[['elevation', 'time', 'path_distance_from_glacier', 'glacial', 'elevation_residual', 'n_from_glacial_max']].reset_index().dropna()\n",
    "src['time'] = src['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "alt.Chart(\n",
    "    src\n",
    ").transform_filter(\n",
    "    (datum.glacial == False)\n",
    ").mark_circle().encode(\n",
    "    alt.X(\"path_distance_from_glacier:Q\"),\n",
    "    alt.Y(\"elevation_residual:Q\", scale=alt.Scale(zero=False), title='Elevation Residuals (rolling mean, 10 meter window', impute=alt.ImputeParams(value=None)),\n",
    "    alt.Color(\"time:O\", scale=alt.Scale(scheme='viridis')),\n",
    "    alt.StrokeDash('glacial:N'),\n",
    "    tooltip=['n_from_glacial_max', 'time']\n",
    "\n",
    ").properties(\n",
    "    width = 1400,\n",
    "    height = 600,\n",
    "    title=\"Elevation Residuals, relative to 2015 data.\"\n",
    ").configure_legend(\n",
    "    titleColor='black', \n",
    "    titleFontSize=12, \n",
    "    labelFontSize=16, \n",
    "    symbolStrokeWidth=4\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot elevation residuals, exclude glacier signals (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = diff_df[['elevation', 'time', 'path_distance_from_glacier', 'glacial', 'elevation_residual']].reset_index().dropna()\n",
    "src['time'] = src['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "alt.Chart(\n",
    "    src\n",
    ").transform_filter(\n",
    "    (datum.glacial == False)\n",
    ").mark_line().encode(\n",
    "    alt.X(\"path_distance_from_glacier:Q\", title=\"Distance downstream from observed glacial maximum\"),\n",
    "    alt.Y(\"elevation_residual:Q\", scale=alt.Scale(zero=False), title=\"Valley floor elevation residuals relative to 2015 data, in meters\", impute=alt.ImputeParams(value=None)),\n",
    "    \n",
    "    alt.Color(\"time:O\", scale=alt.Scale(scheme='viridis')),\n",
    "    alt.StrokeDash('glacial:N')\n",
    ").properties(\n",
    "    width = 600,\n",
    "    # height = 600,\n",
    "    title=\"Valley floor elevation residuals relative to 2015 data, in meters\"\n",
    ").configure_legend(\n",
    "    titleColor='black', \n",
    "    titleFontSize=12, \n",
    "    labelFontSize=16, \n",
    "    symbolStrokeWidth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot elevation residuals, exclude glacier signals, rolling mean (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = diff_df[['elevation', 'time', 'path_distance_from_glacier', 'glacial', 'elevation_residual']].reset_index().dropna()\n",
    "src['time'] = src['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "alt.Chart(\n",
    "    src\n",
    ").transform_filter(\n",
    "    (datum.glacial == False)\n",
    ").transform_window(\n",
    "    rolling_mean='mean(elevation_residual)',\n",
    "    groupby=['time'],\n",
    "    frame=[-5,5]\n",
    ").mark_line().encode(\n",
    "    alt.X(\"path_distance_from_glacier:Q\", title=\"Distance downstream from observed glacial maximum\"),\n",
    "    alt.Y(\n",
    "        \"rolling_mean:Q\", \n",
    "        scale=alt.Scale(zero=False), \n",
    "        title=['Elevation residuals relative to 2015 data, in meters,',  '(rolling mean, 10 meter window)'],\n",
    "        impute=alt.ImputeParams(value=None)\n",
    "    ),\n",
    "    alt.Color(\"time:O\", scale=alt.Scale(scheme='viridis')),\n",
    "    alt.StrokeDash('glacial:N')\n",
    ").properties(\n",
    "    width = 600,\n",
    "    # height = 600,\n",
    "    title=\"Elevation Residuals, relative to 2015 data.\"\n",
    ").configure_legend(\n",
    "    titleColor='black', \n",
    "    titleFontSize=12, \n",
    "    labelFontSize=16, \n",
    "    symbolStrokeWidth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot elevation residuals, include glacier signals (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = diff_df[['elevation', 'time', 'path_distance_from_glacier', 'glacial', 'elevation_residual']].reset_index().dropna()\n",
    "src['time'] = src['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "alt.Chart(\n",
    "    src\n",
    ").mark_line().encode(\n",
    "    alt.X(\"path_distance_from_glacier:Q\", title=\"Distance downstream from observed glacial maximum\"),\n",
    "    alt.Y(\n",
    "        \"elevation_residual:Q\", \n",
    "        scale=alt.Scale(zero=False), \n",
    "        title=['Elevation residuals relative to 2015 data, in meters'],\n",
    "        impute=alt.ImputeParams(value=None)\n",
    "    ),\n",
    "    alt.Color(\"time:O\", scale=alt.Scale(scheme='viridis')),\n",
    "    alt.StrokeDash('glacial:N')\n",
    ").properties(\n",
    "    width = 600,\n",
    "    # height = 600,\n",
    "    title=\"Elevation Residuals, relative to 2015 data.\"\n",
    ").configure_legend(\n",
    "    titleColor='black', \n",
    "    titleFontSize=12, \n",
    "    labelFontSize=16, \n",
    "    symbolStrokeWidth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot elevation residuals, include glacier signals, rolling mean (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = diff_df[['elevation', 'time', 'path_distance_from_glacier', 'glacial', 'elevation_residual']].reset_index().dropna()\n",
    "src['time'] = src['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "alt.Chart(\n",
    "    src\n",
    ").transform_window(\n",
    "    rolling_mean='mean(elevation_residual)',\n",
    "    groupby=['time'],\n",
    "    frame=[-5,5]\n",
    ").mark_line().encode(\n",
    "    alt.X(\"path_distance_from_glacier:Q\", title=\"Distance downstream from observed glacial maximum\"),\n",
    "        alt.Y(\n",
    "        \"rolling_mean:Q\", \n",
    "        scale=alt.Scale(zero=False), \n",
    "        title=['Elevation residuals relative to 2015 data, in meters,',  '(rolling mean, 10 meter window)'],\n",
    "        impute=alt.ImputeParams(value=None)\n",
    "    ),\n",
    "    \n",
    "    alt.Color(\"time:O\", scale=alt.Scale(scheme='viridis')),\n",
    "    alt.StrokeDash('glacial:N')\n",
    ").properties(\n",
    "    width = 600,\n",
    "    # height = 600,\n",
    "    title=\"Elevation Residuals, relative to 2015 data.\"\n",
    ").configure_legend(\n",
    "    titleColor='black', \n",
    "    titleFontSize=12, \n",
    "    labelFontSize=16, \n",
    "    symbolStrokeWidth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate slope (negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(df):\n",
    "    df['slope'] = - np.gradient(df['elevation'], df['path_distance_from_glacier'])\n",
    "    return df\n",
    "\n",
    "# slope_df = combined_gdf.groupby('time').apply(lambda df: calculate_gradient(df))\n",
    "slope_df = combined_gdf.query('glacial == False').groupby('time').apply(lambda df: calculate_gradient(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot slope (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = slope_df[['elevation', 'time', 'path_distance_from_glacier', 'glacial', 'slope']].reset_index().dropna()\n",
    "src['time'] = src['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "alt.Chart(\n",
    "    src\n",
    ").transform_filter(\n",
    "    (datum.glacial == False)\n",
    ").mark_line().encode(\n",
    "    alt.X(\"path_distance_from_glacier:Q\", title=\"Distance downstream from observed glacial maximum\"),\n",
    "    alt.Y(\"slope:Q\", scale=alt.Scale(zero=False), title='Valley floor slope', impute=alt.ImputeParams(value=None)),\n",
    "    alt.Color(\"time:O\", scale=alt.Scale(scheme='viridis')),\n",
    "    alt.StrokeDash('glacial:N')\n",
    ").properties(\n",
    "    width = 600,\n",
    "    # height = 600,\n",
    "    title=\"Valley floor gradient\"\n",
    ").configure_legend(\n",
    "    titleColor='black', \n",
    "    titleFontSize=12, \n",
    "    labelFontSize=16, \n",
    "    symbolStrokeWidth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot slope, rolling mean (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = slope_df[['elevation', 'time', 'path_distance_from_glacier', 'glacial', 'slope']].reset_index().dropna()\n",
    "src['time'] = src['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "alt.Chart(\n",
    "    src\n",
    ").transform_filter(\n",
    "    (datum.glacial == False)\n",
    ").transform_window(\n",
    "    rolling_mean='mean(slope)',\n",
    "    frame=[-5, 5],\n",
    "    groupby=['time']\n",
    ").mark_line().encode(\n",
    "    alt.X(\"path_distance_from_glacier:Q\", title=\"Distance downstream from observed glacial maximum\"),\n",
    "    alt.Y(\"rolling_mean:Q\", scale=alt.Scale(zero=False), title='Valley floor slope (rolling mean, 10 meter window)', impute=alt.ImputeParams(value=None)),\n",
    "    alt.Color(\"time:O\", scale=alt.Scale(scheme='viridis')),\n",
    "    alt.StrokeDash('glacial:N')\n",
    ").properties(\n",
    "    width = 600,\n",
    "    # height = 600,\n",
    "    title=\"Valley floor gradient\"\n",
    ").configure_legend(\n",
    "    titleColor='black', \n",
    "    titleFontSize=12, \n",
    "    labelFontSize=16, \n",
    "    symbolStrokeWidth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = slope_df[['elevation', 'time', 'path_distance_from_glacier', 'glacial', 'slope']].reset_index().dropna()\n",
    "src['time'] = src['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "alt.Chart(\n",
    "    src\n",
    ").transform_filter(\n",
    "    (datum.glacial == False)\n",
    ").transform_window(\n",
    "    rolling_mean='mean(slope)',\n",
    "    frame=[-5, 5],\n",
    "    groupby=['time']\n",
    ").mark_line().encode(\n",
    "    alt.X(\"path_distance_from_glacier:Q\", title=\"Distance downstream from observed glacial maximum\"),\n",
    "    alt.Y(\"rolling_mean:Q\", scale=alt.Scale(zero=False), title='Valley floor slope (rolling mean, 10 meter window)', impute=alt.ImputeParams(value=None)),\n",
    "    alt.Color(\"time:O\", scale=alt.Scale(scheme='viridis')),\n",
    "    alt.StrokeDash('glacial:N')\n",
    ").properties(\n",
    "    width = 600,\n",
    "    # height = 600,\n",
    "    title=\"Valley floor gradient\"\n",
    ").configure_legend(\n",
    "    titleColor='black', \n",
    "    titleFontSize=12, \n",
    "    labelFontSize=16, \n",
    "    symbolStrokeWidth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by kilometer upslope/downslope from glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_km = slope_df[['elevation', 'time', 'path_distance_from_glacier', 'glacial', 'slope']].reset_index().dropna()\n",
    "grouped_km = grouped_km[~grouped_km.glacial]\n",
    "grouped_km['Kilometer downstream from glacier'] = grouped_km['path_distance_from_glacier'].apply(lambda x: math.floor(x/1000))\n",
    "groups = grouped_km.groupby(['time', 'Kilometer downstream from glacier'])\n",
    "# remove data points if you weren't able to average slope over more than 500 meters\n",
    "grouped_km = groups.filter(lambda df: \n",
    "    (df['path_distance_from_glacier'].max() - df['path_distance_from_glacier'].min()) > 1000*(2/3)\n",
    ")\n",
    "grouped_km = grouped_km.groupby(['time', 'Kilometer downstream from glacier']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(grouped_km).mark_line(point=True).encode(\n",
    "    alt.X('time:T', title=\"\"),\n",
    "    alt.Y('slope:Q', title=\"Valley floor slope\"),\n",
    "    alt.Facet('Kilometer downstream from glacier:O', title='Kilometer downstream from glacier')\n",
    ").properties(width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by provided distance upslope/downslope from glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_halfkm = slope_df[['elevation', 'time', 'path_distance_from_glacier', 'glacial', 'slope']].reset_index().dropna()\n",
    "grouped_halfkm = grouped_halfkm[~grouped_halfkm.glacial]\n",
    "grouped_halfkm['Half kilometer downstream from glacier'] = grouped_halfkm['path_distance_from_glacier'].apply(lambda x: math.floor(x/group_slope_meters))\n",
    "groups = grouped_halfkm.groupby(['time', 'Half kilometer downstream from glacier'])\n",
    "# remove data points if you weren't able to average slope over more than half of the averaging distance\n",
    "grouped_halfkm = groups.filter(lambda df: \n",
    "    (df['path_distance_from_glacier'].max() - df['path_distance_from_glacier'].min()) > group_slope_meters*(2/3)\n",
    ")\n",
    "grouped_halfkm = grouped_halfkm.groupby(['time', 'Half kilometer downstream from glacier']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(grouped_halfkm).mark_line(point=True).encode(\n",
    "    alt.X('time:T', title=\"\"),\n",
    "    alt.Y('slope:Q', title=\"Valley floor slope\"),\n",
    "    alt.Facet('Half kilometer downstream from glacier:O', title='Half kilometer downstream from glacier')\n",
    ").properties(width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export low points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_gdf.geometry = combined_gdf['low_point_coords']\n",
    "\n",
    "combined_gdf_noglacial = combined_gdf.query(\"not glacial\")\n",
    "\n",
    "combined_gdf_noglacial[\n",
    "    ['geometry', 'path_distance_from_glacier', 'elevation', 'id', 'time']\n",
    "].reset_index(drop=True).to_file(\n",
    "    output_lowpoints_file,\n",
    "    driver=\"GeoJSON\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create streamlines from low points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "streamlines = combined_gdf_noglacial.groupby(\"time\").apply(lambda df: LineString([point for point in df.geometry.tolist() if point]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines_gdf = gpd.GeoDataFrame(geometry=streamlines, crs=combined_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines_gdf.to_file(output_streamlines_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_profiles = combined_gdf[[ \"time\", \"path_distance_from_glacier\", \"elevation\", \"glacial\", \"n_from_glacial_max\"]].reset_index()\n",
    "elevation_profiles['time'] = elevation_profiles['time'].apply(lambda x: x.strftime(strip_time_format))\n",
    "\n",
    "\n",
    "dfs = [\n",
    "    grouped_km,\n",
    "    grouped_halfkm,\n",
    "    elevation_profiles\n",
    "]\n",
    "names = [\n",
    "    'slope_grouped_km',\n",
    "    'slope_grouped_halfkm',\n",
    "    'elevation_profiles'\n",
    "]\n",
    "\n",
    "for df,name in zip(dfs, names):\n",
    "    df['valley'] = valley_name\n",
    "    outdir = os.path.join(\"outputs\", name)\n",
    "    outfile = os.path.join(outdir, valley_name + \".pickle\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    print(outfile)\n",
    "    df.to_pickle(outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22b7dc50fb8286be51844dc7799cfbbdb6bfe743b9c42cc7dfa69df0fcb613a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('xdem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
