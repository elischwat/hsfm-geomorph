{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import geoutils as gu\n",
    "import xdem\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from pprint import pprint\n",
    "from rasterio.enums import Resampling\n",
    "import copy\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inputs are written in a JSON.\n",
    "* The inputs file is specified by the `HSFM_GEOMORPH_INPUT_FILE` env var\n",
    "* One input may be overriden with an additional env var - `RUN_LARGER_AREA`. If this env var is set to \"yes\" or \"no\" (exactly that string, it will be used. If the env var is not set, the params file is used to fill in this variable. If some other string is set, a failure is thrown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the arg, you must run from CLI like this\n",
    "\n",
    "```\n",
    "HSFM_GEOMORPH_INPUT_FILE='inputs/mazama_inputs.json' jupyter nbconvert --execute --to html dem-analysis/mt_baker_mass_wasted/xdem.ipynb  --output outputs/xdem_mazama.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('HSFM_GEOMORPH_INPUT_FILE'):\n",
    "    json_file_path = os.environ['HSFM_GEOMORPH_INPUT_FILE']\n",
    "else:\n",
    "    json_file_path = 'inputs/rainbow_inputs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_path, 'r') as j:\n",
    "     params = json.loads(j.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALLEY_BOUNDS_NAME = params[\"inputs\"][\"valley_name\"]\n",
    "dems_path = params[\"inputs\"][\"dems_path\"]\n",
    "gcas_polygon_file = params[\"uncertainty\"][\"gcas_polygon_file\"]\n",
    "valley_bounds_file = params[\"inputs\"][\"valley_bounds_file\"]\n",
    "plot_output_dir = params[\"inputs\"][\"plot_output_dir\"]\n",
    "output_file = params[\"inputs\"][\"uncertainty_file\"]\n",
    "output_file_largerarea = params[\"inputs\"][\"uncertainty_file_largearea\"]\n",
    "TO_DROP = params[\"inputs\"][\"TO_DROP\"]\n",
    "TO_DROP_LARGERAREA = params[\"inputs\"][\"TO_DROP_LARGER_AREA\"]\n",
    "TO_COREGISTER = params[\"inputs\"][\"TO_COREGISTER\"]\n",
    "DATE_FILE_FORMAT = params['inputs']['strip_time_format']\n",
    "FILTER_OUTLIERS = params['inputs']['FILTER_OUTLIERS']\n",
    "SIMPLE_FILTER = params['inputs']['SIMPLE_FILTER']\n",
    "simple_filter_threshold = params['inputs']['simple_filter_threshold']\n",
    "\n",
    "reference_dem_date = datetime.strptime(\n",
    "    params['inputs']['reference_dem_date'], \n",
    "    DATE_FILE_FORMAT\n",
    ")\n",
    "\n",
    "if os.environ.get('RUN_LARGER_AREA'):\n",
    "    print(\"RUN_LARGER_AREA env var read.\")\n",
    "    if os.environ['RUN_LARGER_AREA'] == \"yes\":\n",
    "        print(\"Running larger area\")\n",
    "        RUN_LARGER_AREA = True\n",
    "    elif os.environ['RUN_LARGER_AREA'] == \"no\":\n",
    "        print(\"NOT running larger area\")\n",
    "        RUN_LARGER_AREA = False\n",
    "    else:\n",
    "        raise ValueError(\"Env Var RUN_LARGER_AREA set to an incorrect value. Cannot proceed.\")\n",
    "else:\n",
    "    RUN_LARGER_AREA = params['inputs']['RUN_LARGER_AREA']\n",
    "\n",
    "VARIOGRAM_SUBSAMPLE = params[\"uncertainty\"][\"VARIOGRAM_SUBSAMPLE\"]\n",
    "VARIOGRAM_N_VARIOGRAMS = params[\"uncertainty\"][\"VARIOGRAM_N_VARIOGRAMS\"]\n",
    "PARALLELISM = params[\"uncertainty\"][\"PARALLELISM\"]\n",
    "XSCALE_RANGE_SPLIT = params[\"uncertainty\"][\"XSCALE_RANGE_SPLIT\"]\n",
    "MAX_LAG = params[\"uncertainty\"][\"MAX_LAG\"]\n",
    "RESAMPLING_RES = params[\"uncertainty\"][\"RESAMPLING_RES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIOGRAM_SUBSAMPLE, \\\n",
    "VARIOGRAM_N_VARIOGRAMS, \\\n",
    "PARALLELISM, \\\n",
    "XSCALE_RANGE_SPLIT, \\\n",
    "MAX_LAG, \\\n",
    "RESAMPLING_RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(plot_output_dir):\n",
    "    os.makedirs(plot_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get DEM file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_fn_list = glob.glob(os.path.join(dems_path, \"*.tif\"))\n",
    "dem_fn_list = sorted(dem_fn_list)\n",
    "\n",
    "if RUN_LARGER_AREA:\n",
    "    dem_fn_list = [f for f in dem_fn_list if Path(f).stem not in TO_DROP_LARGERAREA]\n",
    "else:\n",
    "    dem_fn_list = [f for f in dem_fn_list if Path(f).stem not in TO_DROP]\n",
    "dem_fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_fn_list = [f for f in dem_fn_list if 'unaligned' not in f]\n",
    "dem_fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [datetime.strptime(Path(f).stem, DATE_FILE_FORMAT) for f in dem_fn_list]\n",
    "datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Valley Bounds Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_bounds = gu.Vector(valley_bounds_file)\n",
    "uncertainty_valley_bounds_vect = valley_bounds.query(f\"name == '{VALLEY_BOUNDS_NAME}' and purpose=='uncertainty'\")\n",
    "uncertainty_valley_bounds_vect.ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DEMCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demcollection_uncertainty = xdem.DEMCollection.from_files(\n",
    "    dem_fn_list, \n",
    "    datetimes, \n",
    "    reference_dem_date, \n",
    "    uncertainty_valley_bounds_vect, \n",
    "    RESAMPLING_RES,\n",
    "    Resampling.cubic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TO_COREGISTER:\n",
    "    for i in range(0, len(demcollection_uncertainty.dems)-1):\n",
    "        early_dem = demcollection_uncertainty.dems[i]\n",
    "        late_dem = demcollection_uncertainty.dems[i+1]\n",
    "\n",
    "        nuth_kaab = xdem.coreg.NuthKaab()\n",
    "        # Order with the future as reference\n",
    "        nuth_kaab.fit(late_dem.data, early_dem.data, transform=late_dem.transform)\n",
    "\n",
    "        # Apply the transformation to the data (or any other data)\n",
    "        aligned_ex = nuth_kaab.apply(early_dem.data, transform=early_dem.transform)\n",
    "\n",
    "        print(F\"For DEM {early_dem.datetime}, transform is {nuth_kaab.to_matrix()}\")\n",
    "\n",
    "        early_dem.data = np.expand_dims(aligned_ex, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = demcollection_uncertainty.subtract_dems_intervalwise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bounding DEMCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_demcollection_uncertainty = xdem.DEMCollection(\n",
    "    [demcollection_uncertainty.dems[0], demcollection_uncertainty.dems[-1]],\n",
    "    [demcollection_uncertainty.timestamps[0], demcollection_uncertainty.timestamps[-1]],\n",
    ")\n",
    "\n",
    "_ = bounding_demcollection_uncertainty.subtract_dems_intervalwise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot dDEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection_uncertainty.plot_ddems(figsize=(30, 10), vmin=-20, vmax=20, interpolation = \"none\")\n",
    "fig.savefig(os.path.join(plot_output_dir, \"dod_gallery.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Bounding dDEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = bounding_demcollection_uncertainty.plot_ddems(figsize=(30, 10), vmin=-20, vmax=20, interpolation = \"none\")\n",
    "fig.savefig(os.path.join(plot_output_dir, \"bounding_dod_gallery.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open ground control polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcas_vector = gu.Vector(gcas_polygon_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to perform an uncertainty analysis:\n",
    "* Plot ground control area DH\n",
    "* Sample dataset and plot empirical variogram\n",
    "* Fit spherical model and plot empirical variogram + fitted model\n",
    "* Print comprehensive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertainty_helpers import uncertainty_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the results as we create them\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_interval_string(interval):\n",
    "    return interval.left.strftime(\"%y_%m_%d\") + \"__\" + interval.right.strftime(\"%y_%m_%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis_plot_and_return_results(ddem):\n",
    "    results, figs = uncertainty_analysis(\n",
    "        ddem,\n",
    "        gcas_vector,\n",
    "        subsample = VARIOGRAM_SUBSAMPLE,\n",
    "        n_variograms = VARIOGRAM_N_VARIOGRAMS,\n",
    "        xscale_range_split = XSCALE_RANGE_SPLIT,\n",
    "        parallelism=PARALLELISM,\n",
    "        maxlag=MAX_LAG,\n",
    "        FILTER_OUTLIERS = FILTER_OUTLIERS,\n",
    "        SIMPLE_FILTER = SIMPLE_FILTER,\n",
    "        simple_filter_threshold = simple_filter_threshold\n",
    "    )\n",
    "    interval_string = clean_interval_string(ddem.interval)\n",
    "    figs[0].savefig(os.path.join(plot_output_dir, f\"dod_uncertainty_static_areas_{interval_string}.png\"))\n",
    "    figs[1].savefig(os.path.join(plot_output_dir, f\"dod_uncertainty_empirical_variogram_{interval_string}.png\"))\n",
    "    figs[2].savefig(os.path.join(plot_output_dir, f\"dod_uncertainty_fit_variogram_{interval_string}.png\"))\n",
    "    pprint(results, width=1)\n",
    "    return results\n",
    "    \n",
    "\n",
    "for ddem in demcollection_uncertainty.ddems:\n",
    "    results = run_analysis_plot_and_return_results(ddem)\n",
    "    results['bounding'] = False\n",
    "    results_dict[results[\"Interval\"]] = results\n",
    "\n",
    "for ddem in bounding_demcollection_uncertainty.ddems:\n",
    "    results = run_analysis_plot_and_return_results(ddem)\n",
    "    results['bounding'] = True\n",
    "    results_dict[results[\"Interval\"]] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze all uncertainty results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict).transpose().reset_index(drop=True)\n",
    "results_df['Start Date'] = results_df['Interval'].apply(lambda x: x.left)\n",
    "results_df['End Date'] = results_df['Interval'].apply(lambda x: x.right)\n",
    "results_df['NMAD'] = pd.to_numeric(results_df['NMAD'])\n",
    "results_df['Mean'] = pd.to_numeric(results_df['Mean'])\n",
    "results_df['RMSE'] = pd.to_numeric(results_df['RMSE'])\n",
    "results_df['Range'] = pd.to_numeric(results_df['Range'])\n",
    "results_df['Sill'] = pd.to_numeric(results_df['Sill'])\n",
    "results_df['StdDev'] = pd.to_numeric(results_df['StdDev'])\n",
    "results_df['90% CI'] = results_df.apply(lambda row: stats.norm.interval(0.90, loc=row['Mean'], scale=row['StdDev']), axis=1)\n",
    "results_df['90% CI Lower Bound'] = pd.to_numeric(results_df['90% CI'].apply(lambda x: x[0]))\n",
    "results_df['90% CI Upper Bound'] = pd.to_numeric(results_df['90% CI'].apply(lambda x: x[1]))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(results_df.query(\"bounding == False\").drop(columns=[\"Interval\", \"90% CI\"])).mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X(\"Start Date:T\"),\n",
    "    alt.X2(\"End Date:T\"),\n",
    "    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
    ").properties(\n",
    "    # width=200,\n",
    "    height=150\n",
    ").repeat(\n",
    "    row=['NMAD', 'Mean', 'RMSE', 'Range', 'Sill', 'StdDev', '90% CI Lower Bound', '90% CI Upper Bound']\n",
    ")\n",
    "# chart.save(os.path.join(plot_output_dir, \"uncertainty_results.png\"), scale_factor=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(results_df.query(\"bounding == True\").drop(columns=[\"Interval\", \"90% CI\"])).mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X(\"Start Date:T\"),\n",
    "    alt.X2(\"End Date:T\"),\n",
    "    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
    ").properties(\n",
    "    # width=200,\n",
    "    height=150\n",
    ").repeat(\n",
    "    row=['NMAD', 'Mean', 'RMSE', 'Range', 'Sill', 'StdDev', '90% CI Lower Bound', '90% CI Upper Bound']\n",
    ")\n",
    "# chart.save(os.path.join(plot_output_dir, \"uncertainty_results.png\"), scale_factor=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LARGER_AREA:\n",
    "    results_df.to_pickle(output_file_largerarea)\n",
    "else:\n",
    "    results_df.to_pickle(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22b7dc50fb8286be51844dc7799cfbbdb6bfe743b9c42cc7dfa69df0fcb613a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('xdem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
