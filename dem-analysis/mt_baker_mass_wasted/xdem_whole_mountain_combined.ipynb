{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code included for option:\n",
    "* 1947, 1977/1979 mixed, 2015 based intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import geoutils as gu\n",
    "import xdem\n",
    "from pprint import pprint\n",
    "import altair as alt\n",
    "from rasterio.enums import Resampling\n",
    "import json \n",
    "import seaborn as sns\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gully_data = '/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/gully.shp'\n",
    "mwasting_data = '/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/mass_wasting.shp'\n",
    "debutressing_data = '/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/debutressing.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_fn_list1 = [\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/1947_09_14.tif\",\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/1977_09_27_clipped.tif\",\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/2015_09_01.tif\"\n",
    "]\n",
    "\n",
    "dem_fn_list2 = [\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/1947_09_14.tif\",\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/1979_10_06_clipped.tif\",\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/2015_09_01.tif\"\n",
    "]\n",
    "\n",
    "valley_list1 = ['Park','Rainbow']\n",
    "valley_list2 = ['Deming', 'Boulder', 'Squak', 'Thunder', \n",
    "       'Talum', 'Easton', 'Coleman', 'Mazama']\n",
    "\n",
    "timestamps1 = ['1947_09_14', '1977_09_27', '2015_09_01']\n",
    "timestamps2 = ['1947_09_14', '1979_10_06', '2015_09_01']\n",
    "\n",
    "areas1 = [\"rainbow\", \"park\"]\n",
    "\n",
    "glacier_files = glob.glob(\"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/*/glaciers.geojson\")\n",
    "\n",
    "dods_output_path = \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dods/\"\n",
    "\n",
    "strip_time_format = \"%Y_%m_%d\"\n",
    "reference_dem_date = \"2015_09_01\"\n",
    "reference_dem_date = datetime.strptime(\n",
    "    reference_dem_date, \n",
    "    strip_time_format\n",
    ")\n",
    "\n",
    "uncertainty_file = \"outputs/uncertainty_wholemountain.pcl\"\n",
    "\n",
    "FILTER_OUTLIERS = True\n",
    "SIMPLE_FILTER = True\n",
    "simple_filter_threshold = 50\n",
    "MASK_EXTRA_SIGNALS = False\n",
    "INTERPOLATE = True\n",
    "interpolation_max_search_distance = 50\n",
    "\n",
    "RESAMPLING_RES = 2\n",
    "\n",
    "SAVE_DDEMS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes1 = [datetime.strptime(f, strip_time_format) for f in timestamps1]\n",
    "datetimes2 = [datetime.strptime(f, strip_time_format) for f in timestamps2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_df = pd.read_pickle(uncertainty_file)\n",
    "uncertainty_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DEMCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demcollection1 = xdem.DEMCollection.from_files(\n",
    "    dem_fn_list1, \n",
    "    datetimes1, \n",
    "    reference_dem_date, \n",
    "    None, \n",
    "    RESAMPLING_RES,\n",
    "    Resampling.cubic\n",
    ")\n",
    "\n",
    "demcollection2 = xdem.DEMCollection.from_files(\n",
    "    dem_fn_list2, \n",
    "    datetimes2, \n",
    "    reference_dem_date, \n",
    "    None, \n",
    "    RESAMPLING_RES,\n",
    "    Resampling.cubic\n",
    ")\n",
    "\n",
    "bounding_demcollection = xdem.DEMCollection.from_files(\n",
    "    [dem_fn_list1[0], dem_fn_list1[-1]], \n",
    "    [datetimes1[0], datetimes1[-1]], \n",
    "    reference_dem_date, \n",
    "    None, \n",
    "    RESAMPLING_RES,\n",
    "    Resampling.cubic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open glacier polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_glacier_files = glacier_files\n",
    "all_glacier_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_gdf = pd.concat([gpd.read_file(f) for f in all_glacier_files])\n",
    "glaciers_gdf = glaciers_gdf[glaciers_gdf.year.isin(timestamps1 + timestamps2)]\n",
    "glaciers_gdf['date'] = glaciers_gdf['year'].apply(lambda x: datetime.strptime(x, strip_time_format))\n",
    "glaciers_gdf = glaciers_gdf.to_crs(demcollection1.dems[0].crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_gdf.year.unique(), glaciers_gdf.date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_gdf = glaciers_gdf.dropna(subset=['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract DEMs/Create DoDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = demcollection1.subtract_dems_intervalwise()\n",
    "_ = demcollection2.subtract_dems_intervalwise()\n",
    "_ = bounding_demcollection.subtract_dems_intervalwise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER_OUTLIERS:\n",
    "    for dh in demcollection1.ddems + demcollection2.ddems + bounding_demcollection.ddems:\n",
    "        if SIMPLE_FILTER:\n",
    "            dh.data = np.ma.masked_where(np.abs(dh.data) > simple_filter_threshold, dh.data)\n",
    "        else:\n",
    "            all_values_masked = dh.data.copy()\n",
    "            all_values = all_values_masked.filled(np.nan)\n",
    "            low = np.nanmedian(all_values) - 4*xdem.spatialstats.nmad(all_values)\n",
    "            high = np.nanmedian(all_values) + 4*xdem.spatialstats.nmad(all_values)\n",
    "            print(np.nanmax(dh.data))\n",
    "            print(np.nanmin(dh.data))\n",
    "            print(dh.interval)\n",
    "            print(low)\n",
    "            print(high)\n",
    "            all_values_masked = np.ma.masked_greater(all_values_masked, high)\n",
    "            all_values_masked = np.ma.masked_less(all_values_masked, low)\n",
    "            dh.data = all_values_masked\n",
    "            print(np.nanmax(dh.data))\n",
    "            print(np.nanmin(dh.data))\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERPOLATE:\n",
    "    _ = demcollection1.interpolate_ddems(max_search_distance=interpolation_max_search_distance)\n",
    "    demcollection1.set_ddem_filled_data()\n",
    "\n",
    "    _ = demcollection2.interpolate_ddems(max_search_distance=interpolation_max_search_distance)\n",
    "    demcollection2.set_ddem_filled_data()\n",
    "\n",
    "    _ = bounding_demcollection.interpolate_ddems(max_search_distance=interpolation_max_search_distance)\n",
    "    bounding_demcollection.set_ddem_filled_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask Glacier Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ddem in demcollection1.ddems + demcollection2.ddems + bounding_demcollection.ddems:\n",
    "    relevant_glaciers_gdf = glaciers_gdf[glaciers_gdf['date'].isin([ddem.interval.left, ddem.interval.right])]\n",
    "    relevant_glaciers_mask = gu.Vector(relevant_glaciers_gdf).create_mask(ddem).squeeze()\n",
    "    ddem.data.mask = np.logical_or(ddem.data.mask, relevant_glaciers_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare erosion polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all erosion polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_erosion_files = glob.glob(\"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/*/erosion.geojson\")\n",
    "hillslope_fluvial_erosion_vector = gu.Vector(gpd.GeoDataFrame(pd.concat([gpd.read_file(f) for f in all_erosion_files])))\n",
    "\n",
    "erosion_vector = gu.Vector(\n",
    "    pd.concat([\n",
    "        gu.Vector(gully_data).ds,\n",
    "        gu.Vector(mwasting_data).ds,\n",
    "        gu.Vector(debutressing_data).ds,\n",
    "        hillslope_fluvial_erosion_vector.ds\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erosion_vector.ds.plot(edgecolor='black', alpha=0.3, column='type', figsize=(15,15), legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract glacier polygons from erosion polygons\n",
    "\n",
    "For each dDEM time interval, get the two relevant glacier polygons, and subtract them from each erosion polygon, so that each erosion polygon multiplies to become one erosion polygon per time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_erosion_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "def subtract_multiple_geoms(polygon, cutting_geometries):\n",
    "        new_polygon = polygon\n",
    "        for cutting_geom in cutting_geometries:\n",
    "            new_polygon = new_polygon.difference(cutting_geom)\n",
    "        return new_polygon\n",
    "\n",
    "for ddem in demcollection1.ddems:\n",
    "    relevant_glacier_polygons = glaciers_gdf.loc[glaciers_gdf.date.isin([ddem.interval.left, ddem.interval.right])]\n",
    "    local_erosion_vector = erosion_vector.ds[erosion_vector.ds['name'].isin(valley_list1)]\n",
    "    differenced_geoms = local_erosion_vector.geometry.apply(\n",
    "        lambda geom: subtract_multiple_geoms(geom, relevant_glacier_polygons.geometry)\n",
    "    )\n",
    "    new_erosion_gdf = pd.concat([\n",
    "        new_erosion_gdf,\n",
    "        gpd.GeoDataFrame({\n",
    "            'geometry': differenced_geoms,\n",
    "            'type': local_erosion_vector['type'],\n",
    "            'interval': np.full(len(differenced_geoms), ddem.interval),\n",
    "            'name': local_erosion_vector['name']\n",
    "        })\n",
    "    ])\n",
    "\n",
    "for ddem in demcollection2.ddems:\n",
    "    relevant_glacier_polygons = glaciers_gdf.loc[glaciers_gdf.date.isin([ddem.interval.left, ddem.interval.right])]\n",
    "    local_erosion_vector = erosion_vector.ds[erosion_vector.ds['name'].isin(valley_list2)]\n",
    "    differenced_geoms = local_erosion_vector.geometry.apply(\n",
    "        lambda geom: subtract_multiple_geoms(geom, relevant_glacier_polygons.geometry)\n",
    "    )\n",
    "    new_erosion_gdf = pd.concat([\n",
    "        new_erosion_gdf,\n",
    "        gpd.GeoDataFrame({\n",
    "            'geometry': differenced_geoms,\n",
    "            'type': local_erosion_vector['type'],\n",
    "            'interval': np.full(len(differenced_geoms), ddem.interval),\n",
    "            'name': local_erosion_vector['name']\n",
    "        })\n",
    "    ])\n",
    "\n",
    "for ddem in bounding_demcollection.ddems:\n",
    "    relevant_glacier_polygons = glaciers_gdf.loc[glaciers_gdf.date.isin([ddem.interval.left, ddem.interval.right])]\n",
    "    differenced_geoms = erosion_vector.ds.geometry.apply(\n",
    "        lambda geom: subtract_multiple_geoms(geom, relevant_glacier_polygons.geometry)\n",
    "    )\n",
    "    new_erosion_gdf = pd.concat([\n",
    "        new_erosion_gdf,\n",
    "        gpd.GeoDataFrame({\n",
    "            'geometry': differenced_geoms,\n",
    "            'type': erosion_vector.ds['type'],\n",
    "            'interval': np.full(len(differenced_geoms), ddem.interval),\n",
    "            'name': erosion_vector.ds['name']\n",
    "        })\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create outlines dictionaries for each demcollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ddem_intervals(demcollection):\n",
    "    return [ddem.interval for ddem in demcollection.ddems]\n",
    "    \n",
    "# separate into different Vector objects\n",
    "normal_erosion_vector_ds1 = new_erosion_gdf[new_erosion_gdf.interval.isin(get_ddem_intervals(demcollection1))]\n",
    "normal_erosion_vector_ds2 = new_erosion_gdf[new_erosion_gdf.interval.isin(get_ddem_intervals(demcollection2))]\n",
    "bounding_erosion_vector_ds = new_erosion_gdf[new_erosion_gdf.interval.isin(get_ddem_intervals(bounding_demcollection))]\n",
    "\n",
    "# Create dictionaries from GDFs, with intervals as keys\n",
    "demcollection1.outlines = {k:gu.Vector(gdf) for k, gdf in dict(list(normal_erosion_vector_ds1.groupby(\"interval\"))).items()}\n",
    "demcollection2.outlines = {k:gu.Vector(gdf) for k, gdf in dict(list(normal_erosion_vector_ds2.groupby(\"interval\"))).items()}\n",
    "bounding_demcollection.outlines = {k:gu.Vector(gdf) for k, gdf in dict(list(bounding_erosion_vector_ds.groupby(\"interval\"))).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot erosion geoms by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in demcollection1.outlines.items():\n",
    "    interval = tup[0]\n",
    "    gdf = tup[1].ds\n",
    "    gdf.plot(edgecolor='black', alpha=0.3, column='type', figsize=(15,15), legend=True)\n",
    "    plt.gca().set_title(str(interval))\n",
    "    plt.show()\n",
    "\n",
    "for tup in demcollection2.outlines.items():\n",
    "    interval = tup[0]\n",
    "    gdf = tup[1].ds\n",
    "    gdf.plot(edgecolor='black', alpha=0.3, column='type', figsize=(15,15), legend=True)\n",
    "    plt.gca().set_title(str(interval))\n",
    "    plt.show()\n",
    "\n",
    "for tup in bounding_demcollection.outlines.items():\n",
    "    interval = tup[0]\n",
    "    gdf = tup[1].ds\n",
    "    gdf.plot(edgecolor='black', alpha=0.3, column='type', figsize=(15,15), legend=True)\n",
    "    plt.gca().set_title(str(interval))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MASK_EXTRA_SIGNALS:\n",
    "    for ddem in demcollection1.ddems + demcollection2.ddems + bounding_demcollection.ddems:\n",
    "        local_erosion_vector = erosion_vector.copy()\n",
    "        local_erosion_vector.ds = local_erosion_vector.ds[local_erosion_vector.ds['interval'] == ddem.interval]\n",
    "        extra_signals_mask = ~local_erosion_vector.create_mask(ddem).squeeze()\n",
    "        ddem.data.mask = np.logical_or(ddem.data.mask, extra_signals_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ddem in demcollection1.ddems + demcollection2.ddems + bounding_demcollection.ddems:\n",
    "    sns.distplot(ddem.data.filled(np.nan))\n",
    "    plt.title(str(ddem.interval))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demcollection1.plot_ddems(figsize=(30, 10), vmin=-20, vmax=20, interpolation = \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demcollection2.plot_ddems(figsize=(30, 10), vmin=-20, vmax=20, interpolation = \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_demcollection.plot_ddems(figsize=(30, 10), vmin=-20, vmax=20, interpolation = \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DDEMS:\n",
    "    # Save all interval dDEMs\n",
    "    os.makedirs(dods_output_path, exist_ok=True)\n",
    "\n",
    "    for ddem in demcollection1.ddems + demcollection2.ddems + bounding_demcollection.ddems:\n",
    "        startt = ddem.start_time.strftime(strip_time_format)\n",
    "        endt = ddem.end_time.strftime(strip_time_format)\n",
    "        fn = f\"{startt}_to_{endt}.tif\"\n",
    "        fn = os.path.join(dods_output_path, fn)\n",
    "        print(fn)\n",
    "        ddem_xr = ddem.to_xarray()\n",
    "        ddem_xr.data = ddem.data.filled(np.nan)\n",
    "        ddem_xr.rio.to_raster(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to threshold and create pos/neg DEMCollections\n",
    "* Make sure to set values equal to 0 instead of actually removing them!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def threshold_ddem(ddem):\n",
    "    ddem = ddem.copy()\n",
    "    sample = ddem.data.compressed()\n",
    "    datum = uncertainty_df.loc[uncertainty_df['Interval'] == ddem.interval]\n",
    "    assert len(datum) == 1\n",
    "    low = datum['90% CI Lower Bound'].iloc[0]\n",
    "    hi = datum['90% CI Upper Bound'].iloc[0]\n",
    "    print((low, hi))\n",
    "    ddem.data[\n",
    "        np.logical_and(ddem.data>low, ddem.data<hi)\n",
    "    ] = 0\n",
    "    \n",
    "    return ddem\n",
    "\n",
    "def create_thresholded_demcollection(old_demcollection):\n",
    "    threshold_demcollection = xdem.DEMCollection(\n",
    "        old_demcollection.dems,\n",
    "        old_demcollection.timestamps\n",
    "    )\n",
    "    threshold_demcollection.ddems_are_intervalwise = True\n",
    "    threshold_demcollection.ddems = [threshold_ddem(ddem) for ddem in old_demcollection.ddems]\n",
    "    threshold_demcollection.outlines = old_demcollection.outlines\n",
    "    return threshold_demcollection\n",
    "\n",
    "def create_positive_and_negative_ddems(ddem):\n",
    "    pos = ddem.copy()\n",
    "    neg = ddem.copy()\n",
    "    pos.data = np.ma.masked_less(pos.data, 0)\n",
    "    neg.data = np.ma.masked_greater(neg.data, 0)\n",
    "    return pos, neg\n",
    "\n",
    "def create_positive_and_negative_demcollection(demcollection):\n",
    "    pos_ddems, neg_ddems = zip(*[create_positive_and_negative_ddems(ddem) for ddem in demcollection.ddems])\n",
    "    pos_ddemcollection = xdem.DEMCollection(\n",
    "        demcollection.dems,\n",
    "        demcollection.timestamps\n",
    "    )\n",
    "    pos_ddemcollection.ddems_are_intervalwise = True\n",
    "    pos_ddemcollection.ddems = pos_ddems\n",
    "    pos_ddemcollection.outlines = demcollection.outlines\n",
    "\n",
    "    neg_ddemcollection = xdem.DEMCollection(\n",
    "        demcollection.dems,\n",
    "        demcollection.timestamps\n",
    "    )\n",
    "    neg_ddemcollection.ddems_are_intervalwise = True\n",
    "    neg_ddemcollection.ddems = neg_ddems\n",
    "    neg_ddemcollection.outlines = demcollection.outlines\n",
    "\n",
    "    return pos_ddemcollection, neg_ddemcollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create thresholded DEM collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_demcollection1, neg_demcollection1 = create_positive_and_negative_demcollection(create_thresholded_demcollection(demcollection1))\n",
    "pos_demcollection2, neg_demcollection2 = create_positive_and_negative_demcollection(create_thresholded_demcollection(demcollection2))\n",
    "pos_bounding_demcollection, neg_bounding_demcollection = create_positive_and_negative_demcollection(create_thresholded_demcollection(bounding_demcollection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass wasting calculations - not differentiated by process (ie use hillslope + fluvial polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Net mass wasted by valley\n",
    "\n",
    "Loop through the two demcollections with the two valley lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hillslope_and_fluvial_filter(name):\n",
    "    return f\"name == '{name}' and (type == 'hillslope' or type == 'fluvial')\"\n",
    "\n",
    "dv_df_by_valley = pd.DataFrame()\n",
    "for name in valley_list1:\n",
    "    results_df = demcollection1.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    results_df['bounding'] = False\n",
    "    bounding_results_df = bounding_demcollection.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    bounding_results_df['bounding'] = True\n",
    "    new_df = pd.concat([results_df, bounding_results_df])\n",
    "    new_df['name'] = name\n",
    "    dv_df_by_valley = dv_df_by_valley.append(new_df)\n",
    "\n",
    "for name in valley_list2:\n",
    "    results_df = demcollection2.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    results_df['bounding'] = False\n",
    "    bounding_results_df = bounding_demcollection.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    bounding_results_df['bounding'] = True\n",
    "    new_df = pd.concat([results_df, bounding_results_df])\n",
    "    new_df['name'] = name\n",
    "    dv_df_by_valley = dv_df_by_valley.append(new_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Gross mass wasted by valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_pos_dv_df_by_valley = pd.DataFrame()\n",
    "thresh_neg_dv_df_by_valley = pd.DataFrame()\n",
    "\n",
    "for name in valley_list1:\n",
    "    pos_results_df = pos_demcollection1.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    pos_results_df['bounding'] = False\n",
    "    pos_bounding_results_df = pos_bounding_demcollection.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    pos_bounding_results_df['bounding'] = True\n",
    "    new_df = pd.concat([pos_results_df, pos_bounding_results_df])\n",
    "    new_df['name'] = name\n",
    "    thresh_pos_dv_df_by_valley = thresh_pos_dv_df_by_valley.append(new_df)\n",
    "\n",
    "    neg_results_df = neg_demcollection1.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    neg_results_df['bounding'] = False\n",
    "    neg_bounding_results_df = neg_bounding_demcollection.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    neg_bounding_results_df['bounding'] = True\n",
    "    new_df = pd.concat([neg_results_df, neg_bounding_results_df])\n",
    "    new_df['name'] = name\n",
    "    thresh_neg_dv_df_by_valley = thresh_neg_dv_df_by_valley.append(new_df)\n",
    "\n",
    "for name in valley_list2:\n",
    "    pos_results_df = pos_demcollection2.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    pos_results_df['bounding'] = False\n",
    "    pos_bounding_results_df = pos_bounding_demcollection.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    pos_bounding_results_df['bounding'] = True\n",
    "    new_df = pd.concat([pos_results_df, pos_bounding_results_df])\n",
    "    new_df['name'] = name\n",
    "    thresh_pos_dv_df_by_valley = thresh_pos_dv_df_by_valley.append(new_df)\n",
    "\n",
    "    neg_results_df = neg_demcollection2.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    neg_results_df['bounding'] = False\n",
    "    neg_bounding_results_df = neg_bounding_demcollection.get_dv_series(return_area=True, outlines_filter=get_hillslope_and_fluvial_filter(name)).reset_index()\n",
    "    neg_bounding_results_df['bounding'] = True\n",
    "    new_df = pd.concat([neg_results_df, neg_bounding_results_df])\n",
    "    new_df['name'] = name\n",
    "    thresh_neg_dv_df_by_valley = thresh_neg_dv_df_by_valley.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_volume_data(df, pixel_area, pixel_side_length, uncertainty_df):\n",
    "    \"\"\"Modify the resulting dataframe of `demcollection.get_dv_series` by \n",
    "    adding a bunch of useful data. Calculates volumetric uncertainty as well.\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        pixel_area (_type_): _description_\n",
    "    \"\"\"\n",
    "    df[\"n_pixels\"] = df[\"area\"]/pixel_area\n",
    "\n",
    "    df[\"volumetric_uncertainty\"] = df.apply(\n",
    "        lambda row: xdem.spatialstats.volumetric_uncertainty(\n",
    "            n_pixels = row[\"n_pixels\"],\n",
    "            pixel_side_length = pixel_side_length,\n",
    "            rmse = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['RMSE'].iloc[0],\n",
    "            mean = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['Mean'].iloc[0],\n",
    "            range_val = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['Range'].iloc[0],\n",
    "            sill_val = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['Sill'].iloc[0],\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    df['start_time'] = df['index'].apply(lambda x: x.left)\n",
    "    df['end_time'] = df['index'].apply(lambda x: x.right)\n",
    "    df['time_difference_years'] = df.apply(\n",
    "        lambda row: round((row['end_time'] - row['start_time']).days/365.25),\n",
    "        axis=1\n",
    "    )\n",
    "    df['Annual Mass Wasted'] = df['volume']/df['time_difference_years']\n",
    "    df[\"Upper CI\"] = (df['volume'] + df['volumetric_uncertainty'])/df['time_difference_years']\n",
    "    df[\"Lower CI\"] = (df['volume'] - df['volumetric_uncertainty'])/df['time_difference_years']\n",
    "    df[\"Average Date\"] = df['start_time'] + ((df['end_time'] - df['start_time']) / 2).dt.ceil('D')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_df_by_valley = enrich_volume_data(\n",
    "    dv_df_by_valley,\n",
    "    pixel_area = demcollection1.reference_dem.res[0] * demcollection1.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection1.reference_dem.res[0],\n",
    "    uncertainty_df=uncertainty_df\n",
    ")\n",
    "thresh_pos_dv_df_by_valley = enrich_volume_data(\n",
    "    thresh_pos_dv_df_by_valley,\n",
    "    pixel_area = demcollection1.reference_dem.res[0] * demcollection1.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection1.reference_dem.res[0],\n",
    "    uncertainty_df=uncertainty_df\n",
    ")\n",
    "thresh_neg_dv_df_by_valley = enrich_volume_data(\n",
    "    thresh_neg_dv_df_by_valley,\n",
    "    pixel_area = demcollection1.reference_dem.res[0] * demcollection1.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection1.reference_dem.res[0],\n",
    "    uncertainty_df=uncertainty_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Convenience Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_sorting = [\n",
    "            'Mazama',\n",
    "            'Deming',\n",
    "            'Coleman',\n",
    "            'Rainbow',\n",
    "            'Park',\n",
    "            'Easton',\n",
    "            'Squak',\n",
    "            'Talum',\n",
    "            'Boulder',\n",
    "            'Thunder'\n",
    "        ]\n",
    "\n",
    "def get_bars(df, color=None, value=\"Annual Mass Wasted:Q\", x_axis = 'name:O'):\n",
    "    if color:\n",
    "        bars_mark = alt.Chart(df).mark_bar(\n",
    "            strokeWidth = 1.5,\n",
    "            stroke=\"white\",\n",
    "            color=color,\n",
    "            opacity=0.8\n",
    "        )\n",
    "    else:\n",
    "        bars_mark = alt.Chart(df).mark_bar(\n",
    "            strokeWidth = 1.5,\n",
    "            stroke=\"white\",\n",
    "            opacity=0.8\n",
    "        )\n",
    "    bars = bars_mark.encode(\n",
    "        alt.X(x_axis, sort=valley_sorting),\n",
    "        alt.Y(value)\n",
    "    ).properties(\n",
    "        # width=300, \n",
    "        # height=300\n",
    "    )\n",
    "    return bars \n",
    "def bars_and_error_bars(df, color=None, value=\"Annual Mass Wasted:Q\", bars_2_cols = [\"Lower CI\", \"Upper CI\"], x_axis = 'name:O', \n",
    "    x_title = 'Valley', y_title = \"Annualized rate of volumetric change, in m³/yr\"\n",
    "):\n",
    "    bars = get_bars(df, color, value, x_axis)\n",
    "    error_bars = alt.Chart(df).mark_bar(\n",
    "        color=\"black\",\n",
    "        width=2\n",
    "    ).encode(\n",
    "        alt.X(x_axis, sort=valley_sorting, title=x_title),\n",
    "        alt.Y(bars_2_cols[0], title=y_title),\n",
    "        alt.Y2(bars_2_cols[1])\n",
    "    )\n",
    "    return (bars + error_bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net mass wasted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = dv_df_by_valley.drop(columns='index')\n",
    "bounding = src[src.bounding==True]\n",
    "non_bounding = src[src.bounding==False]\n",
    "\n",
    "bounding_chart = bars_and_error_bars(bounding)\n",
    "early_chart = bars_and_error_bars(non_bounding[non_bounding.start_time == datetime(1947, 9, 14)])\n",
    "late_chart = bars_and_error_bars(non_bounding[non_bounding.end_time == datetime(2015, 9, 1)])\n",
    "\n",
    "(\n",
    "    bounding_chart.properties(title=\"1947-2015\") | early_chart.properties(title=\"1947-1977/79 \") | late_chart.properties(title=\"1977/79-2015\")\n",
    ").resolve_scale(y='shared').properties(title='Net erosion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross mass wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_neg = thresh_neg_dv_df_by_valley.drop(columns='index')\n",
    "bounding_neg = src_neg[src_neg.bounding==True]\n",
    "non_bounding_neg = src_neg[src_neg.bounding==False]\n",
    "\n",
    "bounding_chart_neg = bars_and_error_bars(bounding_neg)\n",
    "early_chart_neg = bars_and_error_bars(non_bounding_neg[non_bounding_neg.start_time == datetime(1947, 9, 14)])\n",
    "late_chart_neg = bars_and_error_bars(non_bounding_neg[non_bounding_neg.end_time == datetime(2015, 9, 1)])\n",
    "\n",
    "src_pos = thresh_pos_dv_df_by_valley.drop(columns='index')\n",
    "bounding_pos = src_pos[src_pos.bounding==True]\n",
    "non_bounding_pos = src_pos[src_pos.bounding==False]\n",
    "\n",
    "bounding_chart_pos = bars_and_error_bars(bounding_pos, color='red')\n",
    "early_chart_pos = bars_and_error_bars(non_bounding_pos[non_bounding_pos.start_time == datetime(1947, 9, 14)], color='red')\n",
    "late_chart_pos = bars_and_error_bars(non_bounding_pos[non_bounding_pos.end_time == datetime(2015, 9, 1)], color='red')\n",
    "\n",
    "(\n",
    "    (bounding_chart_neg + bounding_chart_pos).properties(title=\"1947-2015\") | \n",
    "    (early_chart_neg + early_chart_pos).properties(title=\"1947-1977/79 \") | \n",
    "    (late_chart_neg + late_chart_pos).properties(title=\"1977/79-2015\")\n",
    ").resolve_scale(y='shared').properties(title='Gross erosion and deposition')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incision Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on net erosion, by valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_df_by_valley['Annual Incision Rate'] = dv_df_by_valley['Annual Mass Wasted']/dv_df_by_valley['area']\n",
    "dv_df_by_valley['Incision Lower CI'] = dv_df_by_valley['Lower CI']/dv_df_by_valley['area']\n",
    "dv_df_by_valley['Incision Upper CI'] = dv_df_by_valley['Upper CI']/dv_df_by_valley['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = dv_df_by_valley.drop(columns='index')\n",
    "bounding = src[src.bounding==True]\n",
    "non_bounding = src[src.bounding==False]\n",
    "\n",
    "bounding_chart = bars_and_error_bars(\n",
    "    bounding, \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI'],\n",
    "    y_title = \"Annualized incision rate, in m/yr\"\n",
    ")\n",
    "early_chart = bars_and_error_bars(\n",
    "    non_bounding[non_bounding.start_time == datetime(1947, 9, 14)], \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI'],\n",
    "    y_title = \"Annualized incision rate, in m/yr\"\n",
    ")\n",
    "late_chart = bars_and_error_bars(\n",
    "    non_bounding[non_bounding.end_time == datetime(2015, 9, 1)], \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI'],\n",
    "    y_title = \"Annualized incision rate, in m/yr\"\n",
    ")\n",
    "\n",
    "(\n",
    "    bounding_chart.properties(title=\"1947-2015\") | early_chart.properties(title=\"1947-1977/79 \") | late_chart.properties(title=\"1977/79-2015\")\n",
    ").resolve_scale(y='shared').properties(title='Net erosion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on gross erosion, by valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_neg_dv_df_by_valley['Annual Incision Rate'] = thresh_neg_dv_df_by_valley['Annual Mass Wasted']/thresh_neg_dv_df_by_valley['area']\n",
    "thresh_neg_dv_df_by_valley['Incision Lower CI'] = thresh_neg_dv_df_by_valley['Lower CI']/thresh_neg_dv_df_by_valley['area']\n",
    "thresh_neg_dv_df_by_valley['Incision Upper CI'] = thresh_neg_dv_df_by_valley['Upper CI']/thresh_neg_dv_df_by_valley['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = thresh_neg_dv_df_by_valley.drop(columns='index')\n",
    "bounding = src[src.bounding==True]\n",
    "non_bounding = src[src.bounding==False]\n",
    "\n",
    "bounding_chart = bars_and_error_bars(\n",
    "    bounding, \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI'],\n",
    "    y_title = \"Annualized incision rate, in m/yr\"\n",
    ")\n",
    "early_chart = bars_and_error_bars(\n",
    "    non_bounding[non_bounding.start_time == datetime(1947, 9, 14)], \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI'],\n",
    "    y_title = \"Annualized incision rate, in m/yr\"\n",
    ")\n",
    "late_chart = bars_and_error_bars(\n",
    "    non_bounding[non_bounding.end_time == datetime(2015, 9, 1)], \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI'],\n",
    "    y_title = \"Annualized incision rate, in m/yr\"\n",
    ")\n",
    "\n",
    "(\n",
    "    bounding_chart.properties(title=\"1947-2015\") | early_chart.properties(title=\"1947-1977/79 \") | late_chart.properties(title=\"1977/79-2015\")\n",
    ").resolve_scale(y='shared').properties(title='Net erosion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass wasting calculations - differentiated by process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make calculations, organized by valley and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_thresh_neg_dv_df_by_valley = pd.DataFrame()\n",
    "\n",
    "def get_valley_filter(name):\n",
    "    return f\"name == '{name}'\"\n",
    "\n",
    "unique_processes = erosion_vector.ds['type'].unique()\n",
    "\n",
    "for name in valley_list1 + valley_list2:\n",
    "    for process in unique_processes:\n",
    "        print(name)\n",
    "        print(process)\n",
    "\n",
    "        if name in valley_list1:\n",
    "            neg_demcollection_to_use = neg_demcollection1\n",
    "        elif name in valley_list2:\n",
    "            neg_demcollection_to_use = neg_demcollection2\n",
    "        else:\n",
    "            raise ValueError(\"impossible!\")\n",
    "        \n",
    "        try:\n",
    "            new_neg_results_df = neg_demcollection_to_use.get_dv_series(\n",
    "                return_area=True, outlines_filter=f\"name == '{name}' and type == '{process}'\"\n",
    "            ).reset_index()\n",
    "            new_neg_results_df['process'] = process\n",
    "            new_neg_results_df['bounding'] = False\n",
    "            new_neg_results_df['name'] = name\n",
    "        except:\n",
    "            print('Got exception for {name} {process} but continuing anyways...')\n",
    "            new_neg_results_df = pd.DataFrame()\n",
    "\n",
    "        new_bounding_neg_results_df = neg_bounding_demcollection.get_dv_series(\n",
    "            return_area=True, outlines_filter=f\"name == '{name}' and type == '{process}'\"\n",
    "        ).reset_index()\n",
    "        new_bounding_neg_results_df['process'] = process\n",
    "        new_bounding_neg_results_df['bounding'] = True\n",
    "        new_bounding_neg_results_df['name'] = name\n",
    "        \n",
    "        process_thresh_neg_dv_df_by_valley = process_thresh_neg_dv_df_by_valley.append(new_neg_results_df).append(new_bounding_neg_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_thresh_neg_dv_df_by_valley = enrich_volume_data(\n",
    "    process_thresh_neg_dv_df_by_valley,\n",
    "    pixel_area = demcollection1.reference_dem.res[0] * demcollection1.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection1.reference_dem.res[0],\n",
    "    uncertainty_df=uncertainty_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_thresh_neg_dv_df_by_valley.groupby(\"index\").apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross erosion by process, by valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src = process_thresh_neg_dv_df_by_valley[process_thresh_neg_dv_df_by_valley.name.isin(['Mazama', 'Coleman', 'Rainbow', 'Deming'])]\n",
    "alt.Chart(src.drop(columns='index')).transform_filter(\n",
    "    alt.datum.bounding==True\n",
    ").mark_bar().encode(\n",
    "    alt.X('name:O'),\n",
    "    alt.Y(\"volume:Q\"),\n",
    "    alt.Color(\"process:N\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = process_thresh_neg_dv_df_by_valley[process_thresh_neg_dv_df_by_valley.name.isin(['Mazama', 'Coleman', 'Rainbow', 'Deming'])]\n",
    "src['hillslope'] = src['process'] == 'hillslope'\n",
    "alt.Chart(src.drop(columns='index')).transform_filter(\n",
    "    alt.datum.bounding==True\n",
    ").mark_bar().encode(\n",
    "    alt.X('hillslope:O'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\"),\n",
    "    alt.Facet('name:O'),\n",
    "    alt.Color(\"process:N\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = process_thresh_neg_dv_df_by_valley[process_thresh_neg_dv_df_by_valley.name.isin(['Mazama', 'Coleman', 'Rainbow', 'Deming'])]\n",
    "alt.Chart(src.drop(columns='index')).transform_filter(alt.datum.bounding==True).mark_bar().encode(\n",
    "    alt.X('process:O'),\n",
    "    alt.Y(\"volume:Q\"),\n",
    "    alt.Facet(\"name:O\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = process_thresh_neg_dv_df_by_valley[process_thresh_neg_dv_df_by_valley.name.isin(['Mazama', 'Coleman', 'Rainbow', 'Deming'])]\n",
    "src = src[src.process != 'hillslope']\n",
    "alt.Chart(src.drop(columns='index')).transform_filter(alt.datum.bounding==True).mark_bar().encode(\n",
    "    alt.X('process:O'),\n",
    "    alt.Y(\"volume:Q\"),\n",
    "    alt.Facet(\"name:O\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross erosion by process, by valley, annualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = process_thresh_neg_dv_df_by_valley.drop(columns='index')\n",
    "src = src[src.name.isin(['Mazama', 'Coleman', 'Rainbow', 'Deming'])]\n",
    "bounding = src[src.bounding==True]\n",
    "non_bounding = src[src.bounding==False]\n",
    "\n",
    "bounding_chart = bars_and_error_bars(bounding)\n",
    "early_chart = bars_and_error_bars(non_bounding[non_bounding.start_time == datetime(1947, 9, 14)])\n",
    "late_chart = bars_and_error_bars(non_bounding[non_bounding.end_time == datetime(2015, 9, 1)])\n",
    "\n",
    "(\n",
    "    bounding_chart.encode(alt.Color(\"process\")).properties(title=\"1947-2015\") | early_chart.encode(alt.Color(\"process\")).properties(title=\"1947-1977/79 \") | late_chart.encode(alt.Color(\"process\")).properties(title=\"1977/79-2015\")\n",
    ").resolve_scale(y='shared').properties(title='Net erosion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = process_thresh_neg_dv_df_by_valley.drop(columns='index')\n",
    "src = src[src.name.isin(['Mazama', 'Coleman', 'Rainbow', 'Deming'])]\n",
    "bounding = src[src.bounding==True]\n",
    "non_bounding = src[src.bounding==False]\n",
    "\n",
    "bounding_chart = bars_and_error_bars(bounding)\n",
    "early_chart = bars_and_error_bars(non_bounding[non_bounding.start_time == datetime(1947, 9, 14)])\n",
    "late_chart = bars_and_error_bars(non_bounding[non_bounding.end_time == datetime(2015, 9, 1)])\n",
    "\n",
    "(\n",
    "    bounding_chart.properties(height=150).facet(row='process').properties(title=\"1947-2015\") | early_chart.properties(height=150).facet(row='process').properties(title=\"1947-1977/79 \") | late_chart.properties(height=150).facet(row='process').properties(title=\"1977/79-2015\")\n",
    ").resolve_scale(y='shared').properties(title='Net erosion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = process_thresh_neg_dv_df_by_valley.drop(columns='index')\n",
    "bounding = src[src.bounding==True]\n",
    "non_bounding = src[src.bounding==False]\n",
    "\n",
    "bounding_chart = bars_and_error_bars(bounding)\n",
    "early_chart = bars_and_error_bars(non_bounding[non_bounding.start_time == datetime(1947, 9, 14)])\n",
    "late_chart = bars_and_error_bars(non_bounding[non_bounding.end_time == datetime(2015, 9, 1)])\n",
    "\n",
    "(\n",
    "    bounding_chart.facet('process').properties(title=\"1947-2015\") | early_chart.facet('process').properties(title=\"1947-1977/79 \") | late_chart.facet('process').properties(title=\"1977/79-2015\")\n",
    ").resolve_scale(y='shared').properties(title='Net erosion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross erosion by process, annualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = process_thresh_neg_dv_df_by_valley.drop(columns='index')\n",
    "bounding = src[src.bounding==True]\n",
    "non_bounding = src[src.bounding==False]\n",
    "\n",
    "early_period = non_bounding[non_bounding['start_time'] == datetime(1947, 9, 14)].groupby(\"process\").sum().reset_index()\n",
    "late_period = non_bounding[non_bounding['end_time'] == datetime(2015, 9, 1)].groupby(\"process\").sum().reset_index()\n",
    "bounding = bounding.groupby(\"process\").sum().reset_index()\n",
    "\n",
    "bounding_chart = bars_and_error_bars(bounding, x_axis='process:N', x_title='')\n",
    "early_chart = bars_and_error_bars(early_period, x_axis='process:N', x_title='')\n",
    "late_chart = bars_and_error_bars(late_period, x_axis='process:N', x_title='')\n",
    "\n",
    "(\n",
    "    bounding_chart.properties(title=\"1947-2015\") | early_chart.properties(title=\"1947-1977/79 \") | late_chart.properties(title=\"1977/79-2015\")\n",
    ").resolve_scale(y='shared').properties(title='Net erosion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incision rates by process, annualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_chart = bars_and_error_bars(bounding, x_axis='process:N')\n",
    "early_chart = bars_and_error_bars(early_period, x_axis='process:N')\n",
    "late_chart = bars_and_error_bars(late_period, x_axis='process:N')\n",
    "\n",
    "bounding['Annual Incision Rate'] = bounding['Annual Mass Wasted']/bounding['area']\n",
    "bounding['Incision Lower CI'] = bounding['Lower CI']/bounding['area']\n",
    "bounding['Incision Upper CI'] = bounding['Upper CI']/bounding['area']\n",
    "\n",
    "early_period['Annual Incision Rate'] = early_period['Annual Mass Wasted']/early_period['area']\n",
    "early_period['Incision Lower CI'] = early_period['Lower CI']/early_period['area']\n",
    "early_period['Incision Upper CI'] = early_period['Upper CI']/early_period['area']\n",
    "\n",
    "late_period['Annual Incision Rate'] = late_period['Annual Mass Wasted']/late_period['area']\n",
    "late_period['Incision Lower CI'] = late_period['Lower CI']/late_period['area']\n",
    "late_period['Incision Upper CI'] = late_period['Upper CI']/late_period['area']\n",
    "\n",
    "bounding_chart = bars_and_error_bars(\n",
    "    bounding, \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI'],\n",
    "    x_axis='process:N',\n",
    "    y_title = \"Annualized incision rate, in m/yr\"\n",
    ")\n",
    "early_chart = bars_and_error_bars(\n",
    "    early_period, \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI'],\n",
    "    x_axis='process:N',\n",
    "    y_title = \"Annualized incision rate, in m/yr\"\n",
    ")\n",
    "late_chart = bars_and_error_bars(\n",
    "    late_period, \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI'],\n",
    "    x_axis='process:N',\n",
    "    y_title = \"Annualized incision rate, in m/yr\"\n",
    ")\n",
    "\n",
    "(\n",
    "    bounding_chart.properties(title=\"1947-2015\") | early_chart.properties(title=\"1947-1977/79 \") | late_chart.properties(title=\"1977/79-2015\")\n",
    ").resolve_scale(y='shared').properties(title='Net erosion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incision rates by process, by valley, annualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_thresh_neg_dv_df_by_valley['Annual Incision Rate'] = process_thresh_neg_dv_df_by_valley['Annual Mass Wasted']/process_thresh_neg_dv_df_by_valley['area']\n",
    "process_thresh_neg_dv_df_by_valley['Incision Lower CI'] = process_thresh_neg_dv_df_by_valley['Lower CI']/process_thresh_neg_dv_df_by_valley['area']\n",
    "process_thresh_neg_dv_df_by_valley['Incision Upper CI'] = process_thresh_neg_dv_df_by_valley['Upper CI']/process_thresh_neg_dv_df_by_valley['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = process_thresh_neg_dv_df_by_valley.drop(columns='index')\n",
    "bounding = src[src.bounding==True]\n",
    "non_bounding = src[src.bounding==False]\n",
    "\n",
    "bounding_chart = bars_and_error_bars(\n",
    "    bounding, \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI']\n",
    ")\n",
    "early_chart = bars_and_error_bars(\n",
    "    non_bounding[non_bounding.start_time == datetime(1947, 9, 14)], \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI']\n",
    ")\n",
    "late_chart = bars_and_error_bars(\n",
    "    non_bounding[non_bounding.end_time == datetime(2015, 9, 1)], \n",
    "    value='Annual Incision Rate:Q', \n",
    "    bars_2_cols=['Incision Lower CI', 'Incision Upper CI']\n",
    ")\n",
    "\n",
    "(\n",
    "    bounding_chart.facet('process').properties(title=\"1947-2015\") | early_chart.facet('process').properties(title=\"1947-1977/79 \") | late_chart.facet('process').properties(title=\"1977/79-2015\")\n",
    ").resolve_scale(y='shared').properties(title='Net erosion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    dv_df_by_valley,\n",
    "    thresh_neg_dv_df_by_valley,\n",
    "    thresh_pos_dv_df_by_valley,\n",
    "    process_thresh_neg_dv_df_by_valley\n",
    "]\n",
    "\n",
    "names = [\n",
    "    'dv_df_by_valley',\n",
    "    'thresh_neg_dv_df_by_valley',\n",
    "    'thresh_pos_dv_df_by_valley',\n",
    "    'process_thresh_neg_dv_df_by_valley'\n",
    "]\n",
    "for df,name in zip(dfs, names):\n",
    "    outdir = os.path.join(\"outputs\", 'xdem_whole_mountain_combined')\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    outfile = os.path.join(outdir, name + \".pickle\")\n",
    "    print(outfile)\n",
    "    df.to_pickle(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22b7dc50fb8286be51844dc7799cfbbdb6bfe743b9c42cc7dfa69df0fcb613a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('xdem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
