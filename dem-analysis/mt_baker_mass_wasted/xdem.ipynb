{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493100b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import geoutils as gu\n",
    "import xdem\n",
    "from pprint import pprint\n",
    "import altair as alt\n",
    "from rasterio.enums import Resampling\n",
    "import json \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10707554",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e2d6d",
   "metadata": {},
   "source": [
    "* Inputs are written in a JSON.\n",
    "* The inputs file is specified by the `HSFM_GEOMORPH_INPUT_FILE` env var\n",
    "* One input may be overriden with an additional env var - `RUN_LARGER_AREA`. If this env var is set to \"yes\" or \"no\" (exactly that string, it will be used. If the env var is not set, the params file is used to fill in this variable. If some other string is set, a failure is thrown)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a41622",
   "metadata": {},
   "source": [
    "If you use the arg, you must run from CLI like this\n",
    "\n",
    "```\n",
    "HSFM_GEOMORPH_INPUT_FILE='inputs/mazama_inputs.json' jupyter nbconvert --execute --to html dem-analysis/mt_baker_mass_wasted/xdem.ipynb  --output outputs/xdem_mazama.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or set an env arg:\n",
    "if os.environ.get('HSFM_GEOMORPH_INPUT_FILE'):\n",
    "    json_file_path = os.environ['HSFM_GEOMORPH_INPUT_FILE']\n",
    "else:\n",
    "    json_file_path = 'inputs/mazama_inputs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_path, 'r') as j:\n",
    "     params = json.loads(j.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfd4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read inputs from params\n",
    "valley_name = params['inputs']['valley_name']\n",
    "TO_DROP = params['inputs']['TO_DROP']\n",
    "TO_DROP_LARGER_AREA = params['inputs']['TO_DROP_LARGER_AREA']\n",
    "TO_COREGISTER = params['inputs']['TO_COREGISTER']\n",
    "SAVE_DDEMS = params['inputs']['SAVE_DDEMS']\n",
    "EROSION_BY_DATE = params['inputs']['EROSION_BY_DATE']\n",
    "INTERPOLATE = params['inputs']['INTERPOLATE']\n",
    "FILTER_OUTLIERS = params['inputs']['FILTER_OUTLIERS']\n",
    "glacier_polygons_file = params['inputs']['glacier_polygons_file']\n",
    "dems_path = params['inputs']['dems_path']\n",
    "valley_bounds_file = params['inputs']['valley_bounds_file']\n",
    "strip_time_format = params['inputs']['strip_time_format']\n",
    "plot_output_dir = params['inputs']['plot_output_dir']\n",
    "uncertainty_file = params['inputs']['uncertainty_file']\n",
    "uncertainty_file_largerarea = params[\"inputs\"][\"uncertainty_file_largearea\"]\n",
    "SIMPLE_FILTER = params['inputs']['SIMPLE_FILTER']\n",
    "simple_filter_threshold = params['inputs']['simple_filter_threshold']\n",
    "\n",
    "plot_figsize = params['inputs']['plot_figsize']\n",
    "plot_vmin = params['inputs']['plot_vmin']\n",
    "plot_vmax = params['inputs']['plot_vmax']\n",
    "MASK_GLACIER_SIGNALS = params['inputs']['MASK_GLACIER_SIGNALS']\n",
    "MASK_EXTRA_SIGNALS = params['inputs']['MASK_EXTRA_SIGNALS']\n",
    "\n",
    "\n",
    "if os.environ.get('RUN_LARGER_AREA'):\n",
    "    print(\"RUN_LARGER_AREA env var read.\")\n",
    "    if os.environ['RUN_LARGER_AREA'] == \"yes\":\n",
    "        print(\"Running larger area\")\n",
    "        RUN_LARGER_AREA = True\n",
    "    elif os.environ['RUN_LARGER_AREA'] == \"no\":\n",
    "        print(\"NOT running larger area\")\n",
    "        RUN_LARGER_AREA = False\n",
    "    else:\n",
    "        raise ValueError(\"Env Var RUN_LARGER_AREA set to an incorrect value. Cannot proceed.\")\n",
    "else:\n",
    "    RUN_LARGER_AREA = params['inputs']['RUN_LARGER_AREA']\n",
    "\n",
    "\n",
    "dem_target_resolution = params[\"inputs\"]['dem_target_resolution']\n",
    "\n",
    "interpolation_max_search_distance = params['inputs']['interpolation_max_search_distance']\n",
    "\n",
    "if EROSION_BY_DATE:\n",
    "    erosion_polygon_file = params['inputs']['erosion_by_date_polygon_file']\n",
    "else:\n",
    "    erosion_polygon_file = params['inputs']['erosion_polygon_file']\n",
    "\n",
    "# Read output inputs from params\n",
    "erosion_polygons_cropped_by_glaciers_output_file = params['outputs']['erosion_polygons_cropped_by_glaciers_output_file']\n",
    "dods_output_path = params['outputs']['dods_output_path']\n",
    "\n",
    "reference_dem_date = datetime.strptime(\n",
    "    params['inputs']['reference_dem_date'], \n",
    "    strip_time_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c11a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LARGER_AREA:\n",
    "    uncertainty_df = pd.read_pickle(uncertainty_file_largerarea)\n",
    "else:\n",
    "    uncertainty_df = pd.read_pickle(uncertainty_file)\n",
    "uncertainty_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(plot_output_dir):\n",
    "    os.makedirs(plot_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44e9e7",
   "metadata": {},
   "source": [
    "## Get DEM file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cad4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_fn_list = glob.glob(os.path.join(dems_path, \"*.tif\"))\n",
    "dem_fn_list = sorted(dem_fn_list)\n",
    "\n",
    "if RUN_LARGER_AREA:\n",
    "    dem_fn_list = [f for f in dem_fn_list if Path(f).stem not in TO_DROP_LARGER_AREA]\n",
    "else:\n",
    "    dem_fn_list = [f for f in dem_fn_list if Path(f).stem not in TO_DROP]\n",
    "dem_fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_fn_list = [f for f in dem_fn_list if 'unaligned' not in f]\n",
    "dem_fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [datetime.strptime(Path(f).stem, strip_time_format) for f in dem_fn_list]\n",
    "datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8fc170",
   "metadata": {},
   "source": [
    "## Open valley bounds polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_bounds = gu.Vector(valley_bounds_file)\n",
    "if RUN_LARGER_AREA:\n",
    "    valley_bounds_vect = valley_bounds.query(f\"name == '{valley_name}' and purpose=='analysis large'\")\n",
    "else:\n",
    "    valley_bounds_vect = valley_bounds.query(f\"name == '{valley_name}' and purpose=='analysis'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe246c2",
   "metadata": {},
   "source": [
    "## Create DEMCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44790788",
   "metadata": {},
   "outputs": [],
   "source": [
    "demcollection = xdem.DEMCollection.from_files(\n",
    "    dem_fn_list, \n",
    "    datetimes, \n",
    "    reference_dem_date, \n",
    "    valley_bounds_vect, \n",
    "    dem_target_resolution,\n",
    "    resampling = Resampling.cubic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee41e5",
   "metadata": {},
   "source": [
    "## Open glacier polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_gdf = gpd.read_file(glacier_polygons_file).to_crs(demcollection.reference_dem.crs)\n",
    "glaciers_gdf['date'] = glaciers_gdf['year'].apply(lambda x: datetime.strptime(x, strip_time_format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc181b",
   "metadata": {},
   "source": [
    "## Plot DEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection.plot_dems(hillshade=True, interpolation = \"none\", figsize=plot_figsize)\n",
    "fig.savefig(os.path.join(plot_output_dir, \"dem_gallery.png\"))\n",
    "plt.suptitle(\"DEM Gallery\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf8f5c",
   "metadata": {},
   "source": [
    "## Coregister DEMs or Do Not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4eb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TO_COREGISTER:\n",
    "    for i in range(0, len(demcollection.dems)-1):\n",
    "        early_dem = demcollection.dems[i]\n",
    "        late_dem = demcollection.dems[i+1]\n",
    "\n",
    "        nuth_kaab = xdem.coreg.NuthKaab()\n",
    "        # Order with the future as reference\n",
    "        nuth_kaab.fit(late_dem.data, early_dem.data, transform=late_dem.transform, \n",
    "            inlier_mask = ~gu.Vector(glaciers_gdf).create_mask(early_dem).squeeze()\n",
    "        )\n",
    "\n",
    "        # Apply the transformation to the data (or any other data)\n",
    "        aligned_ex = nuth_kaab.apply(early_dem.data, transform=early_dem.transform)\n",
    "\n",
    "        print(F\"For DEM {early_dem.datetime}, transform is {nuth_kaab.to_matrix()}\")\n",
    "\n",
    "        early_dem.data = np.expand_dims(aligned_ex, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6aaddb",
   "metadata": {},
   "source": [
    "## Subtract DEMs/Create DoDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b91f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = demcollection.subtract_dems_intervalwise()\n",
    "# _ = demcollection_large.subtract_dems_intervalwise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d72a0f",
   "metadata": {},
   "source": [
    "## Plot DoDs (pre processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection.plot_ddems(\n",
    "    figsize=plot_figsize, vmin=-30, vmax=30, \n",
    "    interpolation = \"none\", \n",
    "    plot_outlines=False,\n",
    "    hillshade=True,\n",
    "    cmap_alpha=0.15\n",
    ")\n",
    "plt.suptitle(\"dDEM Gallery\")\n",
    "fig.savefig(os.path.join(plot_output_dir, \"dod_gallery_preprocessing.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection.plot_ddems(\n",
    "    figsize=plot_figsize, vmin=plot_vmin, vmax=plot_vmax, \n",
    "    interpolation = \"none\", \n",
    "    plot_outlines=False,\n",
    "    hillshade=True,\n",
    "    cmap_alpha=0.15\n",
    ")\n",
    "plt.suptitle(\"dDEM Gallery\")\n",
    "fig.savefig(os.path.join(plot_output_dir, \"dod_gallery_preprocessing.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27475df7",
   "metadata": {},
   "source": [
    "## Save DoDs without Cropping stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DDEMS:\n",
    "    # Save all interval dDEMs\n",
    "    os.makedirs(dods_output_path, exist_ok=True)\n",
    "\n",
    "    for ddem in demcollection.ddems:\n",
    "        startt = ddem.start_time.strftime(strip_time_format)\n",
    "        endt = ddem.end_time.strftime(strip_time_format)\n",
    "        if RUN_LARGER_AREA:\n",
    "            fn = f\"{startt}_to_{endt}_largerarea_asis.tif\"\n",
    "        else:\n",
    "            fn = f\"{startt}_to_{endt}_asis.tif\"\n",
    "        fn = os.path.join(dods_output_path, fn)\n",
    "        print(fn)\n",
    "        ddem_copy = ddem.copy()\n",
    "        filled_data = ddem_copy.interpolate(\n",
    "            method=\"linear\", \n",
    "            reference_elevation=demcollection.reference_dem, \n",
    "            max_search_distance=interpolation_max_search_distance\n",
    "        )\n",
    "        ddem_copy.set_filled_data()\n",
    "        ddem_xr = ddem_copy.to_xarray()\n",
    "        ddem_xr.data = ddem_copy.data.filled(np.nan)\n",
    "        ddem_xr.rio.to_raster(fn)\n",
    "\n",
    "    # Save bounding dDEM\n",
    "\n",
    "    bounding_ddem = xdem.dDEM(  \n",
    "        demcollection.dems[-1] - demcollection.dems[0],\n",
    "        demcollection.timestamps[0], \n",
    "        demcollection.timestamps[-1]\n",
    "    )\n",
    "    filled_data = bounding_ddem.interpolate(\n",
    "        method=\"linear\", \n",
    "        reference_elevation=demcollection.reference_dem, \n",
    "        max_search_distance=interpolation_max_search_distance\n",
    "    )\n",
    "    bounding_ddem.set_filled_data()\n",
    "    bounding_ddem_xr = bounding_ddem.to_xarray()\n",
    "    bounding_ddem_xr.data = bounding_ddem.data.filled(np.nan)\n",
    "    startt = pd.Timestamp(bounding_ddem.start_time).strftime(strip_time_format)\n",
    "    endt = pd.Timestamp(bounding_ddem.end_time).strftime(strip_time_format)\n",
    "    if RUN_LARGER_AREA:\n",
    "        fn = f\"{startt}_to_{endt}_largerarea_asis.tif\"\n",
    "    else:\n",
    "        fn = f\"{startt}_to_{endt}_asis.tif\"\n",
    "    fn = os.path.join(dods_output_path, fn)\n",
    "    print(fn)\n",
    "    # bounding_ddem_copy = bounding_ddem.copy()\n",
    "    bounding_ddem_xr.rio.to_raster(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a7fd1",
   "metadata": {},
   "source": [
    "## Mask Glacier Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MASK_GLACIER_SIGNALS:\n",
    "    for ddem in demcollection.ddems:\n",
    "        ddem\n",
    "        relevant_glaciers_gdf = glaciers_gdf[glaciers_gdf['date'].isin([ddem.interval.left, ddem.interval.right])]\n",
    "        relevant_glaciers_mask = gu.Vector(relevant_glaciers_gdf).create_mask(ddem).squeeze()\n",
    "        ddem.data.mask = np.logical_or(ddem.data.mask, relevant_glaciers_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c037d8b",
   "metadata": {},
   "source": [
    "## Filter outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER_OUTLIERS:\n",
    "    if SIMPLE_FILTER:\n",
    "        for dh in demcollection.ddems:\n",
    "            dh.data = np.ma.masked_where(np.abs(dh.data) > simple_filter_threshold, dh.data)\n",
    "    else:\n",
    "        for dh in demcollection.ddems:\n",
    "            all_values_masked = dh.data.copy()\n",
    "            all_values = all_values_masked.filled(np.nan)\n",
    "            low = np.nanmedian(all_values) - 4*xdem.spatialstats.nmad(all_values)\n",
    "            high = np.nanmedian(all_values) + 4*xdem.spatialstats.nmad(all_values)\n",
    "            print(np.nanmax(dh.data))\n",
    "            print(np.nanmin(dh.data))\n",
    "            print(dh.interval)\n",
    "            print(low)\n",
    "            print(high)\n",
    "            all_values_masked = np.ma.masked_greater(all_values_masked, high)\n",
    "            all_values_masked = np.ma.masked_less(all_values_masked, low)\n",
    "            dh.data = all_values_masked\n",
    "            print(np.nanmax(dh.data))\n",
    "            print(np.nanmin(dh.data))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c0e38",
   "metadata": {},
   "source": [
    "## Prepare erosion polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f3ec9",
   "metadata": {},
   "source": [
    "### Load erosion polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "erosion_vector = gu.Vector(erosion_polygon_file)\n",
    "erosion_vector.ds = erosion_vector.ds.to_crs(demcollection.reference_dem.crs)\n",
    "erosion_vector.ds.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d856d1",
   "metadata": {},
   "source": [
    "### Subtract glacier polygons from erosion polygons\n",
    "\n",
    "Only applies if not EROSION_BY_DATE\n",
    "\n",
    "For each dDEM time interval, get the two relevant glacier polygons, and subtract them from each erosion polygon, so that each erosion polygon multiplies to become one erosion polygon per time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead03de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not EROSION_BY_DATE:    \n",
    "    new_erosion_gdf = []\n",
    "\n",
    "    def subtract_multiple_geoms(polygon, cutting_geometries):\n",
    "            new_polygon = polygon\n",
    "            for cutting_geom in cutting_geometries:\n",
    "                new_polygon = new_polygon.difference(cutting_geom)\n",
    "            return new_polygon\n",
    "\n",
    "    for ddem in demcollection.ddems:\n",
    "        relevant_glacier_polygons = glaciers_gdf.loc[glaciers_gdf.date.isin([ddem.interval.left, ddem.interval.right])]\n",
    "        print(f\"Cropping with {len(relevant_glacier_polygons)} glacier polygons.\")\n",
    "        differenced_geoms = erosion_vector.ds.geometry.apply(\n",
    "            lambda geom: subtract_multiple_geoms(geom, relevant_glacier_polygons.geometry)\n",
    "        )\n",
    "        new_erosion_gdf.append(\n",
    "            gpd.GeoDataFrame(\n",
    "                {\n",
    "                    'geometry': differenced_geoms,\n",
    "                    'type': erosion_vector.ds['type'],\n",
    "                    'interval': np.full(\n",
    "                        len(differenced_geoms),\n",
    "                        ddem.interval\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    ## also do it for bounding dataset\n",
    "    relevant_glacier_polygons = glaciers_gdf.loc[glaciers_gdf.date.isin([demcollection.ddems[0].interval.left, demcollection.ddems[-1].interval.right])]\n",
    "    differenced_geoms = erosion_vector.ds.geometry.apply(\n",
    "        lambda geom: subtract_multiple_geoms(geom, relevant_glacier_polygons.geometry)\n",
    "    )\n",
    "    new_erosion_gdf.append(\n",
    "            gpd.GeoDataFrame(\n",
    "                {\n",
    "                    'geometry': differenced_geoms,\n",
    "                    'type': erosion_vector.ds['type'],\n",
    "                    'interval': np.full(\n",
    "                        len(differenced_geoms),\n",
    "                        pd.Interval(demcollection.ddems[0].interval.left, demcollection.ddems[-1].interval.right)\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    \n",
    "\n",
    "    ## also do it for the bounding dataset \n",
    "    relevant_glacier_polygons = glaciers_gdf.loc[glaciers_gdf.date.isin([demcollection.ddems[0].interval.left, demcollection.ddems[-1].interval.right])]\n",
    "    differenced_geoms = erosion_vector.ds.geometry.apply(\n",
    "        lambda geom: subtract_multiple_geoms(geom, relevant_glacier_polygons.geometry)\n",
    "    )\n",
    "    new_erosion_gdf.append(\n",
    "            gpd.GeoDataFrame(\n",
    "                {\n",
    "                    'geometry': differenced_geoms,\n",
    "                    'type': erosion_vector.ds['type'],\n",
    "                    'interval': np.full(\n",
    "                        len(differenced_geoms),\n",
    "                        pd.Interval(demcollection.ddems[0].interval.left, demcollection.ddems[-1].interval.right)\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \n",
    "    erosion_vector.ds = new_erosion_gdf = pd.concat(new_erosion_gdf)\n",
    "\n",
    "    src = new_erosion_gdf.copy()\n",
    "    src['interval'] = src['interval'].apply(lambda x: x.left.strftime(strip_time_format))\n",
    "    src.to_file(erosion_polygons_cropped_by_glaciers_output_file, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e244d7",
   "metadata": {},
   "source": [
    "### Split erosion vector into dictionary that organizes erosion polygons by a pd.Interval(start_date, end_Date)\n",
    "\n",
    "We do this so that DEMCollection.get_dv_series assigns the correct polygons to the correct dDEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07419a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EROSION_BY_DATE:\n",
    "    # need to create a column \"interval\" for sorting. Columns 'start_date' and 'end_date' should be in the erosion polygons file if `EROSION_BY_DATE`\n",
    "    erosion_vector.ds['interval'] = erosion_vector.ds.apply(\n",
    "        lambda row: pd.Interval(\n",
    "            pd.Timestamp(datetime.strptime(row['start_date'], strip_time_format)),\n",
    "            pd.Timestamp(datetime.strptime(row['end_date'], strip_time_format)),\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "start_date_to_gfd = dict(list(erosion_vector.ds.groupby(\"interval\")))\n",
    "start_date_to_gfd = dict({(key, gu.Vector(gdf)) for key, gdf in start_date_to_gfd.items()})\n",
    "demcollection.outlines = start_date_to_gfd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8866b3d",
   "metadata": {},
   "source": [
    "Plot erosion geoms by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9860afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_erosion_vector_gdf = erosion_vector.ds.groupby('interval')\n",
    "for tup in list(grouped_erosion_vector_gdf):\n",
    "    interval = tup[0]\n",
    "    gdf = tup[1]\n",
    "    gdf.plot()\n",
    "    plt.gca().set_title(str(interval))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b13d3",
   "metadata": {},
   "source": [
    "## Plot DoDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11704415",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection.plot_ddems(\n",
    "    figsize=plot_figsize, vmin=plot_vmin, vmax=plot_vmax, \n",
    "    interpolation = \"none\", \n",
    "    plot_outlines=True,\n",
    "    hillshade=True,\n",
    "    cmap_alpha=0.15\n",
    ")\n",
    "plt.suptitle(\"dDEM Gallery, glacier signals removed\")\n",
    "fig.savefig(os.path.join(plot_output_dir, \"dod_gallery_glaciers_masked.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf56a2",
   "metadata": {},
   "source": [
    "## Mask Extra Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb24c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MASK_EXTRA_SIGNALS:\n",
    "    for ddem in demcollection.ddems:\n",
    "        local_erosion_vector = erosion_vector.copy()\n",
    "        local_erosion_vector.ds = local_erosion_vector.ds[local_erosion_vector.ds['interval'] == ddem.interval]\n",
    "        extra_signals_mask = ~local_erosion_vector.create_mask(ddem).squeeze()\n",
    "        ddem.data.mask = np.logical_or(ddem.data.mask, extra_signals_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b13d3",
   "metadata": {},
   "source": [
    "## Plot DoDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11704415",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection.plot_ddems(\n",
    "    figsize=plot_figsize, vmin=plot_vmin, vmax=plot_vmax, \n",
    "    interpolation = \"none\", \n",
    "    plot_outlines=True,\n",
    "    hillshade=True,\n",
    "    cmap_alpha=0.15\n",
    ")\n",
    "plt.suptitle(\"dDEM Gallery, glacier signals and signals outside of study areas removed\")\n",
    "fig.savefig(os.path.join(plot_output_dir, \"dod_gallery_glaciers_and_extra_masked.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063606ab",
   "metadata": {},
   "source": [
    "## Interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dcbe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERPOLATE:\n",
    "    interpolated_ddems = demcollection.interpolate_ddems(max_search_distance=interpolation_max_search_distance)\n",
    "    demcollection.set_ddem_filled_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c8224",
   "metadata": {},
   "source": [
    "## Mask Extra Signals (again)\n",
    "\n",
    "We need to do this because we may have added some pixels buffered around the erosion polygons during the interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MASK_EXTRA_SIGNALS:\n",
    "    for ddem in demcollection.ddems:\n",
    "        local_erosion_vector = erosion_vector.copy()\n",
    "        local_erosion_vector.ds = local_erosion_vector.ds[local_erosion_vector.ds['interval'] == ddem.interval]\n",
    "        extra_signals_mask = ~local_erosion_vector.create_mask(ddem).squeeze()\n",
    "        ddem.data.mask = np.logical_or(ddem.data.mask, extra_signals_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b13d3",
   "metadata": {},
   "source": [
    "## Plot DoDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d86907",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection.plot_ddems(\n",
    "    figsize=plot_figsize, vmin=plot_vmin, vmax=plot_vmax, \n",
    "    interpolation = \"none\", \n",
    "    plot_outlines=True,\n",
    "    hillshade=True,\n",
    "    cmap_alpha=0.15\n",
    ")\n",
    "plt.suptitle(\"dDEM Gallery with interpolation, glacier signals and signals outside of study areas removed\")\n",
    "fig.savefig(os.path.join(plot_output_dir, \"dods_final_interpolated.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9004a00f",
   "metadata": {},
   "source": [
    "## Plot distributions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(demcollection.ddems), figsize=(6,5), sharex=True, sharey=True)\n",
    "for i, ddem in enumerate(demcollection.ddems):\n",
    "    sns.distplot(ddem.data.filled(np.nan), ax=axes[i], hist=False)\n",
    "    axes[i].set_ylabel(\"\")\n",
    "    axes[i].annotate(str(ddem.interval), xy=(5,0.4))\n",
    "fig.text(-0.02, 0.5, 'common Y', va='center', rotation='vertical')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927bf6c0",
   "metadata": {},
   "source": [
    "## Save dDEMs to tif\n",
    "\n",
    "The datasets generated here are used to help creation of erosion polygons (and others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28157f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DDEMS:\n",
    "    # Save all interval dDEMs\n",
    "    os.makedirs(dods_output_path, exist_ok=True)\n",
    "\n",
    "    for ddem in demcollection.ddems:\n",
    "        startt = ddem.start_time.strftime(strip_time_format)\n",
    "        endt = ddem.end_time.strftime(strip_time_format)\n",
    "        if RUN_LARGER_AREA:\n",
    "            fn = f\"{startt}_to_{endt}_largerarea.tif\"\n",
    "        else:\n",
    "            fn = f\"{startt}_to_{endt}.tif\"\n",
    "        fn = os.path.join(dods_output_path, fn)\n",
    "        print(fn)\n",
    "        ddem_xr = ddem.to_xarray()\n",
    "        ddem_xr.data = ddem.data.filled(np.nan)\n",
    "        ddem_xr.rio.to_raster(fn)\n",
    "\n",
    "    # Save bounding dDEM\n",
    "\n",
    "    bounding_ddem = xdem.dDEM(  \n",
    "        demcollection.dems[-1] - demcollection.dems[0],\n",
    "        demcollection.timestamps[0], \n",
    "        demcollection.timestamps[-1]\n",
    "    )\n",
    "    filled_data = bounding_ddem.interpolate(\n",
    "        method=\"linear\", \n",
    "        reference_elevation=demcollection.reference_dem, \n",
    "        max_search_distance=interpolation_max_search_distance\n",
    "    )\n",
    "    bounding_ddem.set_filled_data()\n",
    "\n",
    "    # Mask out areas that are not within erosion vector. \n",
    "    # We only want to include areas that are within the erosion vectors for both dates.\n",
    "    # We want to mask out areas that are not in the erosion polygons for the start date and not in the erosion polygons for the end date\n",
    "    local_erosion_vector = erosion_vector.copy()\n",
    "    #grab erosion polygons associated with the bounding interval \n",
    "    local_erosion_vector.ds = local_erosion_vector.ds[local_erosion_vector.ds['interval'] == bounding_ddem.interval]\n",
    "    signal_we_want_mask = local_erosion_vector.create_mask(bounding_ddem).squeeze()\n",
    "    bounding_ddem.data.mask = ~signal_we_want_mask\n",
    "    bounding_ddem_xr = bounding_ddem.to_xarray()\n",
    "    bounding_ddem_xr.data = bounding_ddem.data.filled(np.nan)\n",
    "    startt = pd.Timestamp(bounding_ddem.start_time).strftime(strip_time_format)\n",
    "    endt = pd.Timestamp(bounding_ddem.end_time).strftime(strip_time_format)\n",
    "    if RUN_LARGER_AREA:\n",
    "        fn = f\"{startt}_to_{endt}_largerarea.tif\"\n",
    "    else:\n",
    "        fn = f\"{startt}_to_{endt}.tif\"\n",
    "    fn = os.path.join(dods_output_path, fn)\n",
    "    print(fn)\n",
    "    # bounding_ddem_copy = bounding_ddem.copy()\n",
    "    bounding_ddem_xr.rio.to_raster(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1076e",
   "metadata": {},
   "source": [
    "## Mass wasting calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4e3ae",
   "metadata": {},
   "source": [
    "## Create new datasets for our calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb9b41",
   "metadata": {},
   "source": [
    "### Define thresholding function\n",
    "* Make sure to set values equal to 0 instead of actually removing them!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b379df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def threshold_ddem(ddem):\n",
    "    ddem = ddem.copy()\n",
    "    sample = ddem.data.compressed()\n",
    "    datum = uncertainty_df.loc[uncertainty_df['Interval'] == ddem.interval]\n",
    "    assert len(datum) == 1\n",
    "    low = datum['90% CI Lower Bound'].iloc[0]\n",
    "    hi = datum['90% CI Upper Bound'].iloc[0]\n",
    "    print((low, hi))\n",
    "    ddem.data[\n",
    "        np.logical_and(ddem.data>low, ddem.data<hi)\n",
    "    ] = 0\n",
    "    \n",
    "    return ddem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639d90f",
   "metadata": {},
   "source": [
    "### Create thresholded DEM collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_demcollection = xdem.DEMCollection(\n",
    "    demcollection.dems,\n",
    "    demcollection.timestamps\n",
    ")\n",
    "\n",
    "threshold_demcollection.ddems_are_intervalwise = True\n",
    "threshold_demcollection.ddems = [threshold_ddem(ddem) for ddem in demcollection.ddems]\n",
    "threshold_demcollection.outlines = demcollection.outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ce798",
   "metadata": {},
   "source": [
    "### Create positive and negative DEM collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb62511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positive_and_negative_ddems(ddem):\n",
    "    pos = ddem.copy()\n",
    "    neg = ddem.copy()\n",
    "    pos.data = np.ma.masked_less(pos.data, 0)\n",
    "    neg.data = np.ma.masked_greater(neg.data, 0)\n",
    "    return pos, neg\n",
    "\n",
    "pos_ddems, neg_ddems = zip(*[create_positive_and_negative_ddems(ddem) for ddem in demcollection.ddems])\n",
    "\n",
    "pos_ddemcollection = xdem.DEMCollection(\n",
    "    demcollection.dems,\n",
    "    demcollection.timestamps\n",
    ")\n",
    "pos_ddemcollection.ddems_are_intervalwise = True\n",
    "pos_ddemcollection.ddems = pos_ddems\n",
    "pos_ddemcollection.outlines = demcollection.outlines\n",
    "\n",
    "neg_ddemcollection = xdem.DEMCollection(\n",
    "    demcollection.dems,\n",
    "    demcollection.timestamps\n",
    ")\n",
    "neg_ddemcollection.ddems_are_intervalwise = True\n",
    "neg_ddemcollection.ddems = neg_ddems\n",
    "neg_ddemcollection.outlines = demcollection.outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573fdbcd",
   "metadata": {},
   "source": [
    "### Create thresholded positive and negative DEM collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e43e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pos_ddems, threshold_neg_ddems = zip(*[create_positive_and_negative_ddems(ddem) for ddem in threshold_demcollection.ddems])\n",
    "\n",
    "\n",
    "threshold_pos_ddemcollection = xdem.DEMCollection(\n",
    "    threshold_demcollection.dems,\n",
    "    threshold_demcollection.timestamps\n",
    ")\n",
    "threshold_pos_ddemcollection.ddems_are_intervalwise = True\n",
    "threshold_pos_ddemcollection.ddems = threshold_pos_ddems\n",
    "threshold_pos_ddemcollection.outlines = threshold_demcollection.outlines\n",
    "\n",
    "threshold_neg_ddemcollection = xdem.DEMCollection(\n",
    "    threshold_demcollection.dems,\n",
    "    threshold_demcollection.timestamps\n",
    ")\n",
    "threshold_neg_ddemcollection.ddems_are_intervalwise = True\n",
    "threshold_neg_ddemcollection.ddems = threshold_neg_ddems\n",
    "threshold_neg_ddemcollection.outlines = demcollection.outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc052e",
   "metadata": {},
   "source": [
    "### Create bounding DEM collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a664c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bounding interval \n",
    "bounding_interval = pd.Interval(pd.Timestamp(demcollection.timestamps[0]), pd.Timestamp(demcollection.timestamps[-1]))\n",
    "print(f\"Bounding data based on times: {bounding_interval}\")\n",
    "# Get bounding outlines\n",
    "bounding_outlines = demcollection.outlines.get(bounding_interval)\n",
    "#Create bounding dem collection\n",
    "bounding_dem_collection = xdem.DEMCollection(\n",
    "    [demcollection.dems[0], demcollection.dems[-1]],\n",
    "    [demcollection.timestamps[0], demcollection.timestamps[-1]],\n",
    "    outlines = bounding_outlines\n",
    ")\n",
    "\n",
    "_ = bounding_dem_collection.subtract_dems_intervalwise()\n",
    "\n",
    "# filter outliers\n",
    "if FILTER_OUTLIERS:\n",
    "    if SIMPLE_FILTER:\n",
    "        for dh in bounding_dem_collection.ddems:\n",
    "            dh.data = np.ma.masked_where(np.abs(dh.data) > simple_filter_threshold, dh.data)\n",
    "    else:\n",
    "        for dh in bounding_dem_collection.ddems:\n",
    "            all_values_masked = dh.data.copy()\n",
    "            all_values = all_values_masked.filled(np.nan)\n",
    "            low = np.nanmedian(all_values) - 4*xdem.spatialstats.nmad(all_values)\n",
    "            high = np.nanmedian(all_values) + 4*xdem.spatialstats.nmad(all_values)\n",
    "            print(np.nanmax(dh.data))\n",
    "            print(np.nanmin(dh.data))\n",
    "            print(dh.interval)\n",
    "            print(low)\n",
    "            print(high)\n",
    "            all_values_masked = np.ma.masked_greater(all_values_masked, high)\n",
    "            all_values_masked = np.ma.masked_less(all_values_masked, low)\n",
    "            dh.data = all_values_masked\n",
    "            print(np.nanmax(dh.data))\n",
    "            print(np.nanmin(dh.data))\n",
    "            print()\n",
    "\n",
    "# interpolate\n",
    "if INTERPOLATE:\n",
    "    interpolated_ddems = bounding_dem_collection.interpolate_ddems(max_search_distance=interpolation_max_search_distance)\n",
    "    bounding_dem_collection.set_ddem_filled_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db767464",
   "metadata": {},
   "source": [
    "## Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afa58e",
   "metadata": {},
   "source": [
    "### Net mass wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_df = demcollection.get_dv_series(return_area=True).reset_index()\n",
    "dv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_dv_df = bounding_dem_collection.get_dv_series(return_area=True).reset_index()\n",
    "bounding_dv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff63d9c",
   "metadata": {},
   "source": [
    "### Net mass wasted by erosion type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "hillslope_dv_df = demcollection.get_dv_series(return_area=True, outlines_filter=\"type == 'hillslope'\").reset_index()\n",
    "hillslope_dv_df['type'] = 'hillslope'\n",
    "fluvial_dv_df = demcollection.get_dv_series(return_area=True, outlines_filter=\"type == 'fluvial'\").reset_index()\n",
    "fluvial_dv_df['type'] = 'fluvial'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b1f38",
   "metadata": {},
   "source": [
    "### Gross positive and negative mass wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55dbdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dv_df = pos_ddemcollection.get_dv_series(return_area=True).reset_index()\n",
    "neg_dv_df = neg_ddemcollection.get_dv_series(return_area=True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be03b1d",
   "metadata": {},
   "source": [
    "### Gross positive and negative mass wasted, by erosion type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13f819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hillslope_pos_dv_df = pos_ddemcollection.get_dv_series(return_area=True, outlines_filter=\"type == 'hillslope'\").reset_index()\n",
    "hillslope_pos_dv_df['type'] = 'hillslope'\n",
    "\n",
    "fluvial_pos_dv_df = pos_ddemcollection.get_dv_series(return_area=True, outlines_filter=\"type == 'fluvial'\").reset_index()\n",
    "fluvial_pos_dv_df['type'] = 'fluvial'\n",
    "\n",
    "hillslope_neg_dv_df = neg_ddemcollection.get_dv_series(return_area=True, outlines_filter=\"type == 'hillslope'\").reset_index()\n",
    "hillslope_neg_dv_df['type'] = 'hillslope'\n",
    "\n",
    "fluvial_neg_dv_df = neg_ddemcollection.get_dv_series(return_area=True, outlines_filter=\"type == 'fluvial'\").reset_index()\n",
    "fluvial_neg_dv_df['type'] = 'fluvial'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c9559",
   "metadata": {},
   "source": [
    "### Gross positive and negative mass wasted with threshold (1 meter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02098e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pos_dv_df = threshold_pos_ddemcollection.get_dv_series(return_area=True).reset_index()\n",
    "threshold_neg_dv_df = threshold_neg_ddemcollection.get_dv_series(return_area=True).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29fe7d",
   "metadata": {},
   "source": [
    "### Gross positive and negative mass wasted with threshold (1 meter), by erosion type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34410726",
   "metadata": {},
   "outputs": [],
   "source": [
    "hillslope_threshold_pos_dv_df = threshold_pos_ddemcollection.get_dv_series(return_area=True, outlines_filter=\"type == 'hillslope'\").reset_index()\n",
    "hillslope_threshold_pos_dv_df['type'] = 'hillslope'\n",
    "\n",
    "fluvial_threshold_pos_dv_df = threshold_pos_ddemcollection.get_dv_series(return_area=True, outlines_filter=\"type == 'fluvial'\").reset_index()\n",
    "fluvial_threshold_pos_dv_df['type'] = 'fluvial'\n",
    "\n",
    "hillslope_threshold_neg_dv_df = threshold_neg_ddemcollection.get_dv_series(return_area=True, outlines_filter=\"type == 'hillslope'\").reset_index()\n",
    "hillslope_threshold_neg_dv_df['type'] = 'hillslope'\n",
    "\n",
    "fluvial_threshold_neg_dv_df = threshold_neg_ddemcollection.get_dv_series(return_area=True, outlines_filter=\"type == 'fluvial'\").reset_index()\n",
    "fluvial_threshold_neg_dv_df['type'] = 'fluvial'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c48486",
   "metadata": {},
   "source": [
    "### Add metadata to all the dataframes resulting from the calculations\n",
    "\n",
    "Maybe this should be added as functionality to DEMCollection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668683bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_volume_data(df, pixel_area, pixel_side_length, uncertainty_df):\n",
    "    \"\"\"Modify the resulting dataframe of `demcollection.get_dv_series` by \n",
    "    adding a bunch of useful data. Calculates volumetric uncertainty as well.\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        pixel_area (_type_): _description_\n",
    "    \"\"\"\n",
    "    df[\"n_pixels\"] = df[\"area\"]/pixel_area\n",
    "\n",
    "    df[\"volumetric_uncertainty\"] = df.apply(\n",
    "        lambda row: xdem.spatialstats.volumetric_uncertainty(\n",
    "            n_pixels = row[\"n_pixels\"],\n",
    "            pixel_side_length = pixel_side_length,\n",
    "            rmse = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['RMSE'].iloc[0],\n",
    "            mean = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['Mean'].iloc[0],\n",
    "            range_val = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['Range'].iloc[0],\n",
    "            sill_val = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['Sill'].iloc[0],\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    df['start_time'] = df['index'].apply(lambda x: x.left)\n",
    "    df['end_time'] = df['index'].apply(lambda x: x.right)\n",
    "    df['time_difference_years'] = df.apply(\n",
    "        lambda row: round((row['end_time'] - row['start_time']).days/365.25),\n",
    "        axis=1\n",
    "    )\n",
    "    df['Annual Mass Wasted'] = df['volume']/df['time_difference_years']\n",
    "    #### #### #### #### #### #### #### #### #### #### #### #### \n",
    "    #### \n",
    "    #### ToDo: Confirm this is the proper calculation:\n",
    "    #### \n",
    "    #### #### #### #### #### #### #### #### #### #### #### #### \n",
    "    df[\"Upper CI\"] = (df['volume'] + df['volumetric_uncertainty'])/df['time_difference_years']\n",
    "    df[\"Lower CI\"] = (df['volume'] - df['volumetric_uncertainty'])/df['time_difference_years']\n",
    "    df[\"Average Date\"] = df['start_time'] + ((df['end_time'] - df['start_time']) / 2).dt.ceil('D')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773a7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "demcollection.reference_dem.res[0], demcollection.reference_dem.res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea31618",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_dv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c13742",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57129e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_df = enrich_volume_data(\n",
    "    dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0], \n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "\n",
    "bounding_dv_df = enrich_volume_data(\n",
    "    bounding_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0], \n",
    "    uncertainty_df = uncertainty_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49555e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fluvial_dv_df = enrich_volume_data(\n",
    "    fluvial_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0], \n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "\n",
    "hillslope_dv_df = enrich_volume_data(\n",
    "    hillslope_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0], \n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "\n",
    "pos_dv_df = enrich_volume_data(\n",
    "    pos_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0], \n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "\n",
    "neg_dv_df = enrich_volume_data(\n",
    "    neg_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0], \n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "\n",
    "hillslope_pos_dv_df = enrich_volume_data(\n",
    "    hillslope_pos_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0], \n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "fluvial_pos_dv_df = enrich_volume_data(\n",
    "    fluvial_pos_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0], \n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "hillslope_neg_dv_df = enrich_volume_data(\n",
    "    hillslope_neg_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0], \n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "fluvial_neg_dv_df = enrich_volume_data(\n",
    "    fluvial_neg_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0], \n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "\n",
    "threshold_pos_dv_df = enrich_volume_data(\n",
    "    threshold_pos_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0],\n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "threshold_neg_dv_df = enrich_volume_data(\n",
    "    threshold_neg_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0],\n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "hillslope_threshold_pos_dv_df = enrich_volume_data(\n",
    "    hillslope_threshold_pos_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0],\n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "fluvial_threshold_pos_dv_df = enrich_volume_data(\n",
    "    fluvial_threshold_pos_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0],\n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "hillslope_threshold_neg_dv_df = enrich_volume_data(\n",
    "    hillslope_threshold_neg_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0],\n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "fluvial_threshold_neg_dv_df = enrich_volume_data(\n",
    "    fluvial_threshold_neg_dv_df,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0],\n",
    "    uncertainty_df = uncertainty_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d7143",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0cec9",
   "metadata": {},
   "source": [
    "#### Plot net mass wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4898c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart(dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 1.5,\n",
    "    stroke=\"white\",\n",
    "    opacity=0.8\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", \n",
    "    title=\"Annualized rate of volumetric change, in m³/yr\"\n",
    "    )\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "characters = '⁰ ¹ ² ³ ⁴ ⁵'\n",
    "error_bars = alt.Chart(dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", axis=alt.Axis(format=(\"%Y\")), title=\"\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "chart = bars + error_bars\n",
    "\n",
    "# chart.save(os.path.join(plot_output_dir, \"mass_wasted_net.png\"), scale_factor=2.0)\n",
    "\n",
    "chart.properties(\n",
    "    title=\"Net volume change over dDEM intervals\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_dv_df = dv_df.copy()\n",
    "cum_dv_df['cumulative volume'] = dv_df['volume'].cumsum()\n",
    "cum_dv_df['Lower CI'] = 0\n",
    "cum_dv_df['Upper CI'] = 0\n",
    "cum_dv_df.loc[len(cum_dv_df) - 1, 'Lower CI'] = cum_dv_df.loc[len(cum_dv_df) - 1, 'cumulative volume'] - np.sqrt(\n",
    "    (cum_dv_df['volumetric_uncertainty']**2).sum()\n",
    ")\n",
    "cum_dv_df.loc[len(cum_dv_df) - 1, 'Upper CI'] = cum_dv_df.loc[len(cum_dv_df) - 1, 'cumulative volume'] + np.sqrt(\n",
    "    (cum_dv_df['volumetric_uncertainty']**2).sum()\n",
    ")\n",
    "cum_dv_df = cum_dv_df.append({\n",
    "        'cumulative volume': 0,\n",
    "        'end_time': cum_dv_df.iloc[0]['start_time'],\n",
    "        'volumetric_uncertainty': 0\n",
    "    }, ignore_index=True\n",
    ")\n",
    "cum_dv_df['end_time'] = cum_dv_df['end_time'].apply(pd.Timestamp)\n",
    "cum_dv_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68793263",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_cum_dv_df = bounding_dv_df.copy()\n",
    "bounding_cum_dv_df['cumulative volume'] = bounding_dv_df['volume'].cumsum()\n",
    "bounding_cum_dv_df['Lower CI'] = 0\n",
    "bounding_cum_dv_df['Upper CI'] = 0\n",
    "bounding_cum_dv_df.loc[len(bounding_cum_dv_df) - 1, 'Lower CI'] = bounding_cum_dv_df.loc[len(bounding_cum_dv_df) - 1, 'cumulative volume'] - np.sqrt(\n",
    "    (bounding_cum_dv_df['volumetric_uncertainty']**2).sum()\n",
    ")\n",
    "bounding_cum_dv_df.loc[len(bounding_cum_dv_df) - 1, 'Upper CI'] = bounding_cum_dv_df.loc[len(bounding_cum_dv_df) - 1, 'cumulative volume'] + np.sqrt(\n",
    "    (bounding_cum_dv_df['volumetric_uncertainty']**2).sum()\n",
    ")\n",
    "bounding_cum_dv_df = bounding_cum_dv_df.append({\n",
    "        'cumulative volume': 0,\n",
    "        'end_time': bounding_cum_dv_df.iloc[0]['start_time'],\n",
    "        'volumetric_uncertainty': 0\n",
    "    }, ignore_index=True\n",
    ")\n",
    "bounding_cum_dv_df['end_time'] = bounding_cum_dv_df['end_time'].apply(pd.Timestamp)\n",
    "bounding_cum_dv_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8fffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_plot = alt.Chart(cum_dv_df.drop(columns='index')).mark_line(point=True).encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "    title='Cumulative net change, in m³/yr')\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart(cum_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    width=1\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "bounding_point = alt.Chart(bounding_cum_dv_df.drop(columns='index')).mark_circle(shape='diamond', color='red', size=100).encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"volume:Q\"),\n",
    ")\n",
    "\n",
    "bounding_point_error_bars = alt.Chart(bounding_cum_dv_df.drop(columns='index')).mark_bar(\n",
    "    color=\"red\",\n",
    "    width=1\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "(bounding_point + bounding_point_error_bars + error_bars + cum_plot).properties(\n",
    "    height=200,\n",
    "    title=\"Cumulative volume change over dDEM intervals\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba27ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dods_output_path, plot_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb921a",
   "metadata": {},
   "source": [
    "#### Plot cumulative net mass wasted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738868f1",
   "metadata": {},
   "source": [
    "#### Plot net mass wasted by erosion type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart(fluvial_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 1.5,\n",
    "    stroke=\"white\",\n",
    "    opacity=0.8\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    width=300, \n",
    "    height=150\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart(fluvial_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ").properties(\n",
    "    width=300, \n",
    "    height=150\n",
    ")\n",
    "\n",
    "fluvial_chart = bars + error_bars\n",
    "\n",
    "bars = alt.Chart(hillslope_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 1.5,\n",
    "    stroke=\"white\",\n",
    "    opacity=0.8\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    width=300, \n",
    "    height=150\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart(hillslope_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ").properties(\n",
    "    width=300, \n",
    "    height=150\n",
    ")\n",
    "\n",
    "hillslope_chart = bars + error_bars\n",
    "\n",
    "# chart.save(os.path.join(plot_output_dir, \"mass_wasted_net.png\"), scale_factor=2.0)\n",
    "\n",
    "(fluvial_chart.properties(title='fluvial') & hillslope_chart.properties(title='hillslope')).resolve_scale(x='shared').properties(\n",
    "    title=\"Net volume change over dDEM intervals, split by erosion type\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2dd28",
   "metadata": {},
   "source": [
    "#### Plot gross positive and negative mass wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_neg = alt.Chart(neg_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    "    color=\"red\"\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_neg = alt.Chart(neg_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "bars_pos = alt.Chart(pos_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_pos = alt.Chart(pos_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "chart = (bars_pos + error_bars_pos + bars_neg + error_bars_neg)\n",
    "# chart.save(os.path.join(plot_output_dir, \"mass_wasted_gross.png\"), scale_factor=2.0)\n",
    "chart.properties(\n",
    "    title={\n",
    "        'text': [\"Gross positive and negative volume changes over dDEM intervals\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a2988",
   "metadata": {},
   "source": [
    "####  Plot gross positive and negative mass wasted, thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pos_dv_df['Annual Mass Wasted'],pos_dv_df['Annual Mass Wasted'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_neg = alt.Chart(threshold_neg_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    "    color=\"red\"\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_neg = alt.Chart(threshold_neg_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "bars_pos = alt.Chart(threshold_pos_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_pos = alt.Chart(threshold_pos_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "chart = (bars_pos + error_bars_pos + bars_neg + error_bars_neg)\n",
    "# chart.save(os.path.join(plot_output_dir, \"mass_wasted_gross.png\"), scale_factor=2.0)\n",
    "\n",
    "chart = chart.properties(\n",
    "    title={\n",
    "        'text': [\"Gross positive and negative volume changes over dDEM intervals\"],\n",
    "        'subtitle': [f\"Threshold of 90% CI applied\"]\n",
    "    }\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef1a69",
   "metadata": {},
   "source": [
    "#### Plot gross positive and negative mass wasted, by erosion type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_neg = alt.Chart(fluvial_neg_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    "    color=\"red\"\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_neg = alt.Chart(fluvial_neg_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "bars_pos = alt.Chart(fluvial_pos_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_pos = alt.Chart(fluvial_pos_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "chart_fluvial = (bars_pos + error_bars_pos + bars_neg + error_bars_neg).properties(title='fluvial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_neg = alt.Chart(hillslope_neg_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 1.5,\n",
    "    stroke=\"white\",\n",
    "    color=\"red\"\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_neg = alt.Chart(hillslope_neg_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "bars_pos = alt.Chart(hillslope_pos_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 1.5,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_pos = alt.Chart(hillslope_pos_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "chart_hillslope = (bars_pos + error_bars_pos + bars_neg + error_bars_neg).properties(title='hillslope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e559db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(chart_fluvial & chart_hillslope).properties(\n",
    "    title={\n",
    "        'text': [\"Gross positive and negative volume changes over dDEM intervals, split by erosion type\"]    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c5eb24",
   "metadata": {},
   "source": [
    "#### Plot  gros positive and negative mass wasted by erosion type, thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_neg = alt.Chart(fluvial_threshold_neg_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    "    color=\"red\"\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_neg = alt.Chart(fluvial_threshold_neg_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "bars_pos = alt.Chart(fluvial_threshold_pos_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_pos = alt.Chart(fluvial_threshold_pos_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "chart_fluvial = (bars_pos + error_bars_pos + bars_neg + error_bars_neg).properties(title='fluvial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddcd015",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_neg = alt.Chart(hillslope_threshold_neg_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 1.5,\n",
    "    stroke=\"white\",\n",
    "    color=\"red\"\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_neg = alt.Chart(hillslope_threshold_neg_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "bars_pos = alt.Chart(hillslope_threshold_pos_dv_df.drop(columns='index')).mark_bar(\n",
    "    strokeWidth = 1.5,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_pos = alt.Chart(hillslope_threshold_pos_dv_df.drop(columns=\"index\")).mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "chart_hillslope = (bars_pos + error_bars_pos + bars_neg + error_bars_neg).properties(title='hillslope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e8321",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = (chart_fluvial & chart_hillslope).properties(\n",
    "    title={\n",
    "        'text': [\"Gross positive and negative volume changes over dDEM intervals, split by erosion type\"],\n",
    "        'subtitle': [f\"Threshold of 90% CI applied\"]\n",
    "    }\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a48caa3",
   "metadata": {},
   "source": [
    "## Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    cum_dv_df,\n",
    "    bounding_cum_dv_df,\n",
    "\n",
    "    dv_df,\n",
    "    bounding_dv_df,\n",
    "    # fluvial_dv_df,\n",
    "    # hillslope_dv_df,\n",
    "    # threshold_dv_df,\n",
    "    # hillslope_threshold_dv_df,\n",
    "    # fluvial_threshold_dv_df,\n",
    "    \n",
    "    # pos_dv_df,\n",
    "    # neg_dv_df,\n",
    "    # hillslope_pos_dv_df,\n",
    "    # fluvial_pos_dv_df,\n",
    "    # hillslope_neg_dv_df,\n",
    "    # fluvial_neg_dv_df,\n",
    "    \n",
    "    threshold_pos_dv_df,\n",
    "    threshold_neg_dv_df,\n",
    "    hillslope_threshold_pos_dv_df,\n",
    "    fluvial_threshold_pos_dv_df,\n",
    "    hillslope_threshold_neg_dv_df,\n",
    "    fluvial_threshold_neg_dv_df,\n",
    "]\n",
    "\n",
    "names = [\n",
    "    'cum_dv_df',\n",
    "    'bounding_cum_dv_df',\n",
    "\n",
    "    'dv_df',\n",
    "    'bounding_dv_df',\n",
    "    # 'fluvial_dv_df',\n",
    "    # 'hillslope_dv_df',\n",
    "    # 'threshold_dv_df',\n",
    "    # 'hillslope_threshold_dv_df',\n",
    "    # 'fluvial_threshold_dv_df',\n",
    "    \n",
    "    # 'pos_dv_df',\n",
    "    # 'neg_dv_df',\n",
    "    # 'hillslope_pos_dv_df',\n",
    "    # 'fluvial_pos_dv_df',\n",
    "    # 'hillslope_neg_dv_df',\n",
    "    # 'fluvial_neg_dv_df',\n",
    "    'threshold_pos_dv_df',\n",
    "    'threshold_neg_dv_df',\n",
    "    'hillslope_threshold_pos_dv_df',\n",
    "    'fluvial_threshold_pos_dv_df',\n",
    "    'hillslope_threshold_neg_dv_df',\n",
    "    'fluvial_threshold_neg_dv_df',\n",
    "]\n",
    "for df,name in zip(dfs, names):\n",
    "    df['valley'] = valley_name\n",
    "    if RUN_LARGER_AREA:\n",
    "        outdir = os.path.join(\"outputs\", \"larger_area\", name)\n",
    "    else:\n",
    "        outdir = os.path.join(\"outputs\", name)\n",
    "    outfile = os.path.join(outdir, valley_name + \".pickle\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    print(outfile)\n",
    "    df.to_pickle(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f77b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22b7dc50fb8286be51844dc7799cfbbdb6bfe743b9c42cc7dfa69df0fcb613a9"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('xdem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
