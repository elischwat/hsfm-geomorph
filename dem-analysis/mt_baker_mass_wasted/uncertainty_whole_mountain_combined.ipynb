{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code included for option:\n",
    "* 1947, 1977/1979 mixed, 2015 based intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import geoutils as gu\n",
    "import xdem\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from pprint import pprint\n",
    "from rasterio.enums import Resampling\n",
    "import copy\n",
    "import json \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_fn_list1 = [\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/1947_09_14.tif\",\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/1977_09_27_clipped.tif\",\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/2015_09_01.tif\"\n",
    "]\n",
    "\n",
    "dem_fn_list2 = [\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/1947_09_14.tif\",\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/1979_10_06_clipped.tif\",\n",
    "    \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/dems/2015_09_01.tif\"\n",
    "]\n",
    "\n",
    "timestamps1 = ['1947_09_14', '1977_09_27', '2015_09_01']\n",
    "timestamps2 = ['1947_09_14', '1979_10_06', '2015_09_01']\n",
    "\n",
    "DATE_FILE_FORMAT = \"%Y_%m_%d\"\n",
    "reference_dem_date = \"2015_09_01\"\n",
    "reference_dem_date = datetime.strptime(\n",
    "    reference_dem_date, \n",
    "    DATE_FILE_FORMAT\n",
    ")\n",
    "\n",
    "gcas_polygon_file = \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/gcas.geojson\"\n",
    "\n",
    "output_file = \"outputs/uncertainty_wholemountain.pcl\"\n",
    "\n",
    "RESAMPLING_RES = 5\n",
    "\n",
    "FILTER_OUTLIERS = True\n",
    "SIMPLE_FILTER = True\n",
    "simple_filter_threshold = 50\n",
    "\n",
    "VARIOGRAM_SUBSAMPLE = 1000\n",
    "VARIOGRAM_N_VARIOGRAMS = 10\n",
    "PARALLELISM = 64 #32\n",
    "XSCALE_RANGE_SPLIT = [200]\n",
    "MAX_LAG = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes1 = [datetime.strptime(f, DATE_FILE_FORMAT) for f in timestamps1]\n",
    "datetimes2 = [datetime.strptime(f, DATE_FILE_FORMAT) for f in timestamps2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DEMCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demcollection1 = xdem.DEMCollection.from_files(\n",
    "    dem_fn_list1, \n",
    "    datetimes1, \n",
    "    reference_dem_date, \n",
    "    None, \n",
    "    RESAMPLING_RES,\n",
    "    Resampling.cubic\n",
    ")\n",
    "\n",
    "demcollection2 = xdem.DEMCollection.from_files(\n",
    "    dem_fn_list2, \n",
    "    datetimes2, \n",
    "    reference_dem_date, \n",
    "    None, \n",
    "    RESAMPLING_RES,\n",
    "    Resampling.cubic\n",
    ")\n",
    "\n",
    "bounding_demcollection = xdem.DEMCollection.from_files(\n",
    "    [dem_fn_list1[0], dem_fn_list1[-1]], \n",
    "    [datetimes1[0], datetimes1[-1]], \n",
    "    reference_dem_date, \n",
    "    None, \n",
    "    RESAMPLING_RES,\n",
    "    Resampling.cubic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = demcollection1.subtract_dems_intervalwise()\n",
    "_ = demcollection2.subtract_dems_intervalwise()\n",
    "_ = bounding_demcollection.subtract_dems_intervalwise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection1.plot_ddems(figsize=(30, 10), vmin=-20, vmax=20, interpolation = \"none\")\n",
    "# fig.savefig(os.path.join(plot_output_dir, \"dod_gallery.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection2.plot_ddems(figsize=(30, 10), vmin=-20, vmax=20, interpolation = \"none\")\n",
    "# fig.savefig(os.path.join(plot_output_dir, \"dod_gallery.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = bounding_demcollection.plot_ddems(figsize=(30, 10), vmin=-20, vmax=20, interpolation = \"none\")\n",
    "# fig.savefig(os.path.join(plot_output_dir, \"dod_gallery.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcas_vector = gu.Vector(gcas_polygon_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the results as we create them\n",
    "results_dict = {}\n",
    "\n",
    "def clean_interval_string(interval):\n",
    "    return interval.left.strftime(\"%y_%m_%d\") + \"__\" + interval.right.strftime(\"%y_%m_%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertainty_helpers import uncertainty_analysis\n",
    "\n",
    "for ddem in demcollection1.ddems + demcollection2.ddems + bounding_demcollection.ddems:\n",
    "    # try:\n",
    "    results, figs = uncertainty_analysis(\n",
    "        ddem,\n",
    "        gcas_vector,\n",
    "        subsample = VARIOGRAM_SUBSAMPLE,\n",
    "        n_variograms = VARIOGRAM_N_VARIOGRAMS,\n",
    "        xscale_range_split = XSCALE_RANGE_SPLIT,\n",
    "        parallelism=PARALLELISM,\n",
    "        maxlag=MAX_LAG,\n",
    "        FILTER_OUTLIERS = FILTER_OUTLIERS,\n",
    "        SIMPLE_FILTER = SIMPLE_FILTER,\n",
    "        simple_filter_threshold = simple_filter_threshold\n",
    "    )\n",
    "    interval_string = clean_interval_string(ddem.interval)\n",
    "    # figs[0].savefig(os.path.join(plot_output_dir, f\"dod_uncertainty_static_areas_{interval_string}.png\"))\n",
    "    # figs[1].savefig(os.path.join(plot_output_dir, f\"dod_uncertainty_empirical_variogram_{interval_string}.png\"))\n",
    "    # figs[2].savefig(os.path.join(plot_output_dir, f\"dod_uncertainty_fit_variogram_{interval_string}.png\"))\n",
    "    pprint(results, width=1)\n",
    "    results_dict[results[\"Interval\"]] = results\n",
    "    # except Exception as exc:\n",
    "        # print(f\"Failed on ddem: {ddem.interval}\")\n",
    "        # print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict).transpose().reset_index(drop=True)\n",
    "results_df['Start Date'] = results_df['Interval'].apply(lambda x: x.left)\n",
    "results_df['End Date'] = results_df['Interval'].apply(lambda x: x.right)\n",
    "results_df['NMAD'] = pd.to_numeric(results_df['NMAD'])\n",
    "results_df['Mean'] = pd.to_numeric(results_df['Mean'])\n",
    "results_df['RMSE'] = pd.to_numeric(results_df['RMSE'])\n",
    "results_df['Range'] = pd.to_numeric(results_df['Range'])\n",
    "results_df['Sill'] = pd.to_numeric(results_df['Sill'])\n",
    "\n",
    "results_df['StdDev'] = pd.to_numeric(results_df['StdDev'])\n",
    "results_df['90% CI'] = results_df.apply(lambda row: stats.norm.interval(0.90, loc=row['Mean'], scale=row['StdDev']), axis=1)\n",
    "results_df['90% CI Lower Bound'] = pd.to_numeric(results_df['90% CI'].apply(lambda x: x[0]))\n",
    "results_df['90% CI Upper Bound'] = pd.to_numeric(results_df['90% CI'].apply(lambda x: x[1]))\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['bounding'] = results_df['Interval'].apply(lambda x: x.left == datetimes1[0] and x.right == datetimes1[-1])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(results_df.drop(columns=\"Interval\")).transform_filter(\n",
    "   alt.datum.bounding == False\n",
    ").mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X(\"Start Date:T\"),\n",
    "    alt.X2(\"End Date:T\"),\n",
    "    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
    ").properties(\n",
    "    # width=200,\n",
    "    height=150\n",
    ").repeat(\n",
    "    row=['NMAD', 'Mean', 'RMSE', 'Range', 'Sill', 'StdDev', '90% CI Lower Bound', '90% CI Upper Bound']\n",
    ")\n",
    "\n",
    "chart_bounding = alt.Chart(results_df.drop(columns=\"Interval\")).transform_filter(\n",
    "   alt.datum.bounding == True\n",
    ").mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X(\"Start Date:T\"),\n",
    "    alt.X2(\"End Date:T\"),\n",
    "    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
    ").properties(\n",
    "    # width=200,\n",
    "    height=150\n",
    ").repeat(\n",
    "    row=['NMAD', 'Mean', 'RMSE', 'Range', 'Sill', 'StdDev', '90% CI Lower Bound', '90% CI Upper Bound']\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_bounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22b7dc50fb8286be51844dc7799cfbbdb6bfe743b9c42cc7dfa69df0fcb613a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('xdem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
