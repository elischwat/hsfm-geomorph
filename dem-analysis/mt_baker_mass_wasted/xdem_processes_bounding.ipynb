{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493100b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import geoutils as gu\n",
    "import xdem\n",
    "from pprint import pprint\n",
    "import altair as alt\n",
    "from rasterio.enums import Resampling\n",
    "import json \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10707554",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e2d6d",
   "metadata": {},
   "source": [
    "* Inputs are written in a JSON.\n",
    "* The inputs file is specified by the `HSFM_GEOMORPH_INPUT_FILE` env var\n",
    "* One input may be overriden with an additional env var - `RUN_LARGER_AREA`. If this env var is set to \"yes\" or \"no\" (exactly that string, it will be used. If the env var is not set, the params file is used to fill in this variable. If some other string is set, a failure is thrown)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a41622",
   "metadata": {},
   "source": [
    "If you use the arg, you must run from CLI like this\n",
    "\n",
    "```\n",
    "HSFM_GEOMORPH_INPUT_FILE='inputs/mazama_inputs.json' jupyter nbconvert --execute --to html dem-analysis/mt_baker_mass_wasted/xdem.ipynb  --output outputs/xdem_mazama.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or set an env arg:\n",
    "if os.environ.get('HSFM_GEOMORPH_INPUT_FILE'):\n",
    "    json_file_path = os.environ['HSFM_GEOMORPH_INPUT_FILE']\n",
    "else:\n",
    "    json_file_path = 'inputs/coleman_inputs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_path, 'r') as j:\n",
    "     params = json.loads(j.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfd4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gully_data = '/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/gully.shp'\n",
    "mwasting_data = '/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/mass_wasting.shp'\n",
    "debutressing_data = '/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/debutressing.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read inputs from params\n",
    "valley_name = params['inputs']['valley_name']\n",
    "TO_DROP = params['inputs']['TO_DROP']\n",
    "TO_DROP_LARGER_AREA = params['inputs']['TO_DROP_LARGER_AREA']\n",
    "TO_COREGISTER = params['inputs']['TO_COREGISTER']\n",
    "SAVE_DDEMS = params['inputs']['SAVE_DDEMS']\n",
    "EROSION_BY_DATE = params['inputs']['EROSION_BY_DATE']\n",
    "INTERPOLATE = params['inputs']['INTERPOLATE']\n",
    "FILTER_OUTLIERS = params['inputs']['FILTER_OUTLIERS']\n",
    "glacier_polygons_file = params['inputs']['glacier_polygons_file']\n",
    "dems_path = params['inputs']['dems_path']\n",
    "valley_bounds_file = params['inputs']['valley_bounds_file']\n",
    "strip_time_format = params['inputs']['strip_time_format']\n",
    "plot_output_dir = params['inputs']['plot_output_dir']\n",
    "uncertainty_file = params['inputs']['uncertainty_file']\n",
    "uncertainty_file_largerarea = params[\"inputs\"][\"uncertainty_file_largearea\"]\n",
    "SIMPLE_FILTER = params['inputs']['SIMPLE_FILTER']\n",
    "simple_filter_threshold = params['inputs']['simple_filter_threshold']\n",
    "\n",
    "plot_figsize = params['inputs']['plot_figsize']\n",
    "plot_vmin = params['inputs']['plot_vmin']\n",
    "plot_vmax = params['inputs']['plot_vmax']\n",
    "MASK_GLACIER_SIGNALS = params['inputs']['MASK_GLACIER_SIGNALS']\n",
    "MASK_EXTRA_SIGNALS = params['inputs']['MASK_EXTRA_SIGNALS']\n",
    "\n",
    "\n",
    "if os.environ.get('RUN_LARGER_AREA'):\n",
    "    print(\"RUN_LARGER_AREA env var read.\")\n",
    "    if os.environ['RUN_LARGER_AREA'] == \"yes\":\n",
    "        print(\"Running larger area\")\n",
    "        RUN_LARGER_AREA = True\n",
    "    elif os.environ['RUN_LARGER_AREA'] == \"no\":\n",
    "        print(\"NOT running larger area\")\n",
    "        RUN_LARGER_AREA = False\n",
    "    else:\n",
    "        raise ValueError(\"Env Var RUN_LARGER_AREA set to an incorrect value. Cannot proceed.\")\n",
    "else:\n",
    "    RUN_LARGER_AREA = params['inputs']['RUN_LARGER_AREA']\n",
    "\n",
    "\n",
    "dem_target_resolution = params[\"inputs\"]['dem_target_resolution']\n",
    "\n",
    "interpolation_max_search_distance = params['inputs']['interpolation_max_search_distance']\n",
    "\n",
    "if EROSION_BY_DATE:\n",
    "    erosion_polygon_file = params['inputs']['erosion_by_date_polygon_file']\n",
    "else:\n",
    "    erosion_polygon_file = params['inputs']['erosion_polygon_file']\n",
    "\n",
    "# Read output inputs from params\n",
    "erosion_polygons_cropped_by_glaciers_output_file = params['outputs']['erosion_polygons_cropped_by_glaciers_output_file']\n",
    "dods_output_path = params['outputs']['dods_output_path']\n",
    "\n",
    "reference_dem_date = datetime.strptime(\n",
    "    params['inputs']['reference_dem_date'], \n",
    "    strip_time_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c11a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LARGER_AREA:\n",
    "    uncertainty_df = pd.read_pickle(uncertainty_file_largerarea)\n",
    "else:\n",
    "    uncertainty_df = pd.read_pickle(uncertainty_file)\n",
    "uncertainty_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(plot_output_dir):\n",
    "    os.makedirs(plot_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44e9e7",
   "metadata": {},
   "source": [
    "## Get DEM file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cad4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_fn_list = glob.glob(os.path.join(dems_path, \"*.tif\"))\n",
    "dem_fn_list = sorted(dem_fn_list)\n",
    "\n",
    "if RUN_LARGER_AREA:\n",
    "    dem_fn_list = [f for f in dem_fn_list if Path(f).stem not in TO_DROP_LARGER_AREA]\n",
    "else:\n",
    "    dem_fn_list = [f for f in dem_fn_list if Path(f).stem not in TO_DROP]\n",
    "dem_fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb41e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_fn_list = dem_fn_list[0:1] + dem_fn_list[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [datetime.strptime(Path(f).stem, strip_time_format) for f in dem_fn_list]\n",
    "datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8fc170",
   "metadata": {},
   "source": [
    "## Open valley bounds polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_bounds = gu.Vector(valley_bounds_file)\n",
    "if RUN_LARGER_AREA:\n",
    "    valley_bounds_vect = valley_bounds.query(f\"name == '{valley_name}' and purpose=='analysis large'\")\n",
    "else:\n",
    "    valley_bounds_vect = valley_bounds.query(f\"name == '{valley_name}' and purpose=='analysis'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe246c2",
   "metadata": {},
   "source": [
    "## Create DEMCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44790788",
   "metadata": {},
   "outputs": [],
   "source": [
    "demcollection = xdem.DEMCollection.from_files(\n",
    "    dem_fn_list, \n",
    "    datetimes, \n",
    "    reference_dem_date, \n",
    "    valley_bounds_vect, \n",
    "    dem_target_resolution,\n",
    "    resampling = Resampling.cubic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc023e",
   "metadata": {},
   "source": [
    "## Open glacier polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727246cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_gdf = gpd.read_file(glacier_polygons_file).to_crs(demcollection.reference_dem.crs)\n",
    "glaciers_gdf['date'] = glaciers_gdf['year'].apply(lambda x: datetime.strptime(x, strip_time_format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc181b",
   "metadata": {},
   "source": [
    "## Plot DEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection.plot_dems(hillshade=True, interpolation = \"none\", figsize=plot_figsize)\n",
    "plt.suptitle(\"DEM Gallery\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf8f5c",
   "metadata": {},
   "source": [
    "## Coregister DEMs or Do Not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4eb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TO_COREGISTER:\n",
    "    for i in range(0, len(demcollection.dems)-1):\n",
    "        early_dem = demcollection.dems[i]\n",
    "        late_dem = demcollection.dems[i+1]\n",
    "\n",
    "        nuth_kaab = xdem.coreg.NuthKaab()\n",
    "        # Order with the future as reference\n",
    "        nuth_kaab.fit(late_dem.data, early_dem.data, transform=late_dem.transform, \n",
    "            inlier_mask = ~gu.Vector(glaciers_gdf).create_mask(early_dem).squeeze()\n",
    "        )\n",
    "\n",
    "        # Apply the transformation to the data (or any other data)\n",
    "        aligned_ex = nuth_kaab.apply(early_dem.data, transform=early_dem.transform)\n",
    "\n",
    "        print(F\"For DEM {early_dem.datetime}, transform is {nuth_kaab.to_matrix()}\")\n",
    "\n",
    "        early_dem.data = np.expand_dims(aligned_ex, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6aaddb",
   "metadata": {},
   "source": [
    "## Subtract DEMs/Create DoDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b91f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = demcollection.subtract_dems_intervalwise()\n",
    "_ = demcollection.subtract_dems_intervalwise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8e6d5",
   "metadata": {},
   "source": [
    "## Plot DoDs (pre processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bebd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection.plot_ddems(\n",
    "    figsize=plot_figsize, vmin=plot_vmin, vmax=plot_vmax, \n",
    "    interpolation = \"none\", \n",
    "    plot_outlines=False,\n",
    "    hillshade=True,\n",
    "    cmap_alpha=0.15\n",
    ")\n",
    "plt.suptitle(\"dDEM Gallery\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36020ee7",
   "metadata": {},
   "source": [
    "## Mask Glacier Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MASK_GLACIER_SIGNALS:\n",
    "    for ddem in demcollection.ddems:\n",
    "        ddem\n",
    "        relevant_glaciers_gdf = glaciers_gdf[glaciers_gdf['date'].isin([ddem.interval.left, ddem.interval.right])]\n",
    "        relevant_glaciers_mask = gu.Vector(relevant_glaciers_gdf).create_mask(ddem).squeeze()\n",
    "        ddem.data.mask = np.logical_or(ddem.data.mask, relevant_glaciers_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c037d8b",
   "metadata": {},
   "source": [
    "## Filter outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILTER_OUTLIERS:\n",
    "    if SIMPLE_FILTER:\n",
    "        for dh in demcollection.ddems:\n",
    "            dh.data = np.ma.masked_where(np.abs(dh.data) > simple_filter_threshold, dh.data)\n",
    "    else:\n",
    "        for dh in demcollection.ddems:\n",
    "            all_values_masked = dh.data.copy()\n",
    "            all_values = all_values_masked.filled(np.nan)\n",
    "            low = np.nanmedian(all_values) - 4*xdem.spatialstats.nmad(all_values)\n",
    "            high = np.nanmedian(all_values) + 4*xdem.spatialstats.nmad(all_values)\n",
    "            print(np.nanmax(dh.data))\n",
    "            print(np.nanmin(dh.data))\n",
    "            print(dh.interval)\n",
    "            print(low)\n",
    "            print(high)\n",
    "            all_values_masked = np.ma.masked_greater(all_values_masked, high)\n",
    "            all_values_masked = np.ma.masked_less(all_values_masked, low)\n",
    "            dh.data = all_values_masked\n",
    "            print(np.nanmax(dh.data))\n",
    "            print(np.nanmin(dh.data))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c0e38",
   "metadata": {},
   "source": [
    "## Prepare erosion polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f3ec9",
   "metadata": {},
   "source": [
    "### Load erosion polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "hillslope_fluvial_erosion_vector = gu.Vector(erosion_polygon_file)\n",
    "hillslope_fluvial_erosion_vector.ds = hillslope_fluvial_erosion_vector.ds.to_crs(demcollection.reference_dem.crs)\n",
    "\n",
    "erosion_vector = gu.Vector(\n",
    "    pd.concat([\n",
    "        gu.Vector(gully_data).ds,\n",
    "        gu.Vector(mwasting_data).ds,\n",
    "        gu.Vector(debutressing_data).ds\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Remove gully/mass-wasting polygons outside bounds\n",
    "erosion_vector.ds = erosion_vector.ds[erosion_vector.ds.geometry.apply(lambda g: valley_bounds_vect.ds.geometry.iloc[0].contains(g))]\n",
    "erosion_vector.ds = pd.concat([erosion_vector.ds, hillslope_fluvial_erosion_vector.ds.to_crs(erosion_vector.ds.crs)])\n",
    "\n",
    "# Filter datasets\n",
    "erosion_vector = erosion_vector.query(f\"name == '{params['inputs']['valley_name']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b22f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "erosion_vector.ds.plot(edgecolor='black', alpha=0.3, column='type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebd018",
   "metadata": {},
   "source": [
    "### Subtract glacier polygons from erosion polygons\n",
    "\n",
    "Only applies if not EROSION_BY_DATE\n",
    "\n",
    "For each dDEM time interval, get the two relevant glacier polygons, and subtract them from each erosion polygon, so that each erosion polygon multiplies to become one erosion polygon per time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc92fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not EROSION_BY_DATE:    \n",
    "    new_erosion_gdf = []\n",
    "\n",
    "    def subtract_multiple_geoms(polygon, cutting_geometries):\n",
    "            new_polygon = polygon\n",
    "            for cutting_geom in cutting_geometries:\n",
    "                new_polygon = new_polygon.difference(cutting_geom)\n",
    "            return new_polygon\n",
    "\n",
    "    for ddem in demcollection.ddems:\n",
    "        relevant_glacier_polygons = glaciers_gdf.loc[glaciers_gdf.date.isin([ddem.interval.left, ddem.interval.right])]\n",
    "        print(f\"Cropping with {len(relevant_glacier_polygons)} glacier polygons.\")\n",
    "        differenced_geoms = erosion_vector.ds.geometry.apply(\n",
    "            lambda geom: subtract_multiple_geoms(geom, relevant_glacier_polygons.geometry)\n",
    "        )\n",
    "        new_erosion_gdf.append(\n",
    "            gpd.GeoDataFrame(\n",
    "                {\n",
    "                    'geometry': differenced_geoms,\n",
    "                    'type': erosion_vector.ds['type'],\n",
    "                    'interval': np.full(\n",
    "                        len(differenced_geoms),\n",
    "                        ddem.interval\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    ## also do it for bounding dataset\n",
    "    relevant_glacier_polygons = glaciers_gdf.loc[glaciers_gdf.date.isin([demcollection.ddems[0].interval.left, demcollection.ddems[-1].interval.right])]\n",
    "    differenced_geoms = erosion_vector.ds.geometry.apply(\n",
    "        lambda geom: subtract_multiple_geoms(geom, relevant_glacier_polygons.geometry)\n",
    "    )\n",
    "    new_erosion_gdf.append(\n",
    "            gpd.GeoDataFrame(\n",
    "                {\n",
    "                    'geometry': differenced_geoms,\n",
    "                    'type': erosion_vector.ds['type'],\n",
    "                    'interval': np.full(\n",
    "                        len(differenced_geoms),\n",
    "                        pd.Interval(demcollection.ddems[0].interval.left, demcollection.ddems[-1].interval.right)\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    \n",
    "\n",
    "    ## also do it for the almost bounding dataset \n",
    "    relevant_glacier_polygons = glaciers_gdf.loc[glaciers_gdf.date.isin([demcollection.ddems[0].interval.left, demcollection.ddems[-1].interval.right])]\n",
    "    differenced_geoms = erosion_vector.ds.geometry.apply(\n",
    "        lambda geom: subtract_multiple_geoms(geom, relevant_glacier_polygons.geometry)\n",
    "    )\n",
    "    new_erosion_gdf.append(\n",
    "            gpd.GeoDataFrame(\n",
    "                {\n",
    "                    'geometry': differenced_geoms,\n",
    "                    'type': erosion_vector.ds['type'],\n",
    "                    'interval': np.full(\n",
    "                        len(differenced_geoms),\n",
    "                        pd.Interval(demcollection.ddems[0].interval.left, demcollection.ddems[-1].interval.right)\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \n",
    "    erosion_vector.ds = pd.concat(new_erosion_gdf)\n",
    "\n",
    "    #remove any empty geoms\n",
    "    erosion_vector.ds = erosion_vector.ds[~erosion_vector.ds.geometry.is_empty]\n",
    "\n",
    "    src = erosion_vector.ds.copy()\n",
    "    src['interval'] = src['interval'].apply(lambda x: x.left.strftime(strip_time_format))\n",
    "    src.to_file(erosion_polygons_cropped_by_glaciers_output_file, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b020e8",
   "metadata": {},
   "source": [
    "### Split erosion vector into dictionary that organizes erosion polygons by a pd.Interval(start_date, end_Date)\n",
    "\n",
    "We do this so that DEMCollection.get_dv_series assigns the correct polygons to the correct dDEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea072283",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EROSION_BY_DATE:\n",
    "    # need to create a column \"interval\" for sorting. Columns 'start_date' and 'end_date' should be in the erosion polygons file if `EROSION_BY_DATE`\n",
    "    erosion_vector.ds['interval'] = erosion_vector.ds.apply(\n",
    "        lambda row: pd.Interval(\n",
    "            pd.Timestamp(datetime.strptime(row['start_date'], strip_time_format)),\n",
    "            pd.Timestamp(datetime.strptime(row['end_date'], strip_time_format)),\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "start_date_to_gfd = dict(list(erosion_vector.ds.groupby(\"interval\")))\n",
    "start_date_to_gfd = dict({(key, gu.Vector(gdf)) for key, gdf in start_date_to_gfd.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000d1e3",
   "metadata": {},
   "source": [
    "Plot erosion geoms by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_erosion_vector_gdf = erosion_vector.ds.groupby('interval')\n",
    "for tup in list(grouped_erosion_vector_gdf):\n",
    "    interval = tup[0]\n",
    "    gdf = tup[1]\n",
    "    gdf.plot(alpha=0.25)\n",
    "    plt.gca().set_title(str(interval))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca981b0f",
   "metadata": {},
   "source": [
    "### Assign erosion polygons to DEM Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "demcollection.outlines = start_date_to_gfd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57908cb8",
   "metadata": {},
   "source": [
    "## Plot DoDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection.plot_ddems(\n",
    "    figsize=plot_figsize, vmin=plot_vmin, vmax=plot_vmax, \n",
    "    interpolation = \"none\", \n",
    "    plot_outlines=True,\n",
    "    hillshade=True,\n",
    "    cmap_alpha=0.15\n",
    ")\n",
    "plt.suptitle(\"dDEM Gallery, glacier signals removed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a82ff",
   "metadata": {},
   "source": [
    "## Interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERPOLATE:\n",
    "    interpolated_ddems = demcollection.interpolate_ddems(max_search_distance=interpolation_max_search_distance)\n",
    "    demcollection.set_ddem_filled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0612f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = demcollection.plot_ddems(\n",
    "    figsize=plot_figsize, vmin=plot_vmin, vmax=plot_vmax, \n",
    "    interpolation = \"none\", \n",
    "    plot_outlines=True,\n",
    "    hillshade=True,\n",
    "    cmap_alpha=0.15\n",
    ")\n",
    "plt.suptitle(\"dDEM Gallery with interpolation, glacier signals removed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1076e",
   "metadata": {},
   "source": [
    "## Mass wasting calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4e3ae",
   "metadata": {},
   "source": [
    "## Create gross positive and negative, thresholded datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb9b41",
   "metadata": {},
   "source": [
    "### Define thresholding function\n",
    "* Make sure to set values equal to 0 instead of actually removing them!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b379df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def threshold_ddem(ddem):\n",
    "    ddem = ddem.copy()\n",
    "    sample = ddem.data.compressed()\n",
    "    datum = uncertainty_df.loc[uncertainty_df['Interval'] == ddem.interval]\n",
    "    assert len(datum) == 1\n",
    "    low = datum['90% CI Lower Bound'].iloc[0]\n",
    "    hi = datum['90% CI Upper Bound'].iloc[0]\n",
    "    print((low, hi))\n",
    "    ddem.data[\n",
    "        np.logical_and(ddem.data>low, ddem.data<hi)\n",
    "    ] = 0\n",
    "    \n",
    "    return ddem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639d90f",
   "metadata": {},
   "source": [
    "### Create thresholded DEM collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_demcollection = xdem.DEMCollection(\n",
    "    demcollection.dems,\n",
    "    demcollection.timestamps\n",
    ")\n",
    "\n",
    "threshold_demcollection.ddems_are_intervalwise = True\n",
    "threshold_demcollection.ddems = [threshold_ddem(ddem) for ddem in demcollection.ddems]\n",
    "threshold_demcollection.outlines = demcollection.outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573fdbcd",
   "metadata": {},
   "source": [
    "### Create thresholded positive and negative DEM collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e43e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positive_and_negative_ddems(ddem):\n",
    "    pos = ddem.copy()\n",
    "    neg = ddem.copy()\n",
    "    pos.data = np.ma.masked_less(pos.data, 0)\n",
    "    neg.data = np.ma.masked_greater(neg.data, 0)\n",
    "    return pos, neg\n",
    "    \n",
    "threshold_pos_ddems, threshold_neg_ddems = zip(*[create_positive_and_negative_ddems(ddem) for ddem in threshold_demcollection.ddems])\n",
    "\n",
    "\n",
    "threshold_pos_ddemcollection = xdem.DEMCollection(\n",
    "    threshold_demcollection.dems,\n",
    "    threshold_demcollection.timestamps\n",
    ")\n",
    "threshold_pos_ddemcollection.ddems_are_intervalwise = True\n",
    "threshold_pos_ddemcollection.ddems = threshold_pos_ddems\n",
    "threshold_pos_ddemcollection.outlines = threshold_demcollection.outlines\n",
    "\n",
    "threshold_neg_ddemcollection = xdem.DEMCollection(\n",
    "    threshold_demcollection.dems,\n",
    "    threshold_demcollection.timestamps\n",
    ")\n",
    "threshold_neg_ddemcollection.ddems_are_intervalwise = True\n",
    "threshold_neg_ddemcollection.ddems = threshold_neg_ddems\n",
    "threshold_neg_ddemcollection.outlines = demcollection.outlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db767464",
   "metadata": {},
   "source": [
    "## Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afa58e",
   "metadata": {},
   "source": [
    "### Net and gross mass wasted (thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_types = set()\n",
    "for k in list(demcollection.outlines.keys()):\n",
    "    for t in demcollection.outlines[k].ds['type'].unique():\n",
    "        unique_types.add(t)\n",
    "\n",
    "def get_dv_series_groupby_process_sums(demcoll):\n",
    "    dv_df_sep = pd.DataFrame()\n",
    "    for process in unique_types:\n",
    "        data = demcoll.get_dv_series(outlines_filter = f\"type == '{process}'\", return_area=True).reset_index()\n",
    "        data['type'] = process\n",
    "        dv_df_sep = dv_df_sep.append(data)\n",
    "    return dv_df_sep\n",
    "\n",
    "dv_df_process_sums = get_dv_series_groupby_process_sums(demcollection)\n",
    "threshold_neg_dv_df_process_sums = get_dv_series_groupby_process_sums(threshold_neg_ddemcollection)\n",
    "threshold_pos_dv_df_process_sums = get_dv_series_groupby_process_sums(threshold_pos_ddemcollection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c48486",
   "metadata": {},
   "source": [
    "### Add metadata to all the dataframes resulting from the calculations\n",
    "\n",
    "Maybe this should be added as functionality to DEMCollection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d46e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_volume_data(df, pixel_area, pixel_side_length, uncertainty_df):\n",
    "    \"\"\"Modify the resulting dataframe of `demcollection.get_dv_series` by \n",
    "    adding a bunch of useful data. Calculates volumetric uncertainty as well.\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        pixel_area (_type_): _description_\n",
    "    \"\"\"\n",
    "    df[\"n_pixels\"] = df[\"area\"]/pixel_area\n",
    "\n",
    "    df[\"volumetric_uncertainty\"] = df.apply(\n",
    "        lambda row: xdem.spatialstats.volumetric_uncertainty(\n",
    "            n_pixels = row[\"n_pixels\"],\n",
    "            pixel_side_length = pixel_side_length,\n",
    "            rmse = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['RMSE'].iloc[0],\n",
    "            mean = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['Mean'].iloc[0],\n",
    "            range_val = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['Range'].iloc[0],\n",
    "            sill_val = uncertainty_df.loc[uncertainty_df['Interval'] == row['index']]['Sill'].iloc[0],\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    df['start_time'] = df['index'].apply(lambda x: x.left)\n",
    "    df['end_time'] = df['index'].apply(lambda x: x.right)\n",
    "    df['time_difference_years'] = df.apply(\n",
    "        lambda row: round((row['end_time'] - row['start_time']).days/365.25),\n",
    "        axis=1\n",
    "    )\n",
    "    df['Annual Mass Wasted'] = df['volume']/df['time_difference_years']\n",
    "    #### #### #### #### #### #### #### #### #### #### #### #### \n",
    "    #### \n",
    "    #### ToDo: Confirm this is the proper calculation:\n",
    "    #### \n",
    "    #### #### #### #### #### #### #### #### #### #### #### #### \n",
    "    df[\"Upper CI\"] = (df['volume'] + df['volumetric_uncertainty'])/df['time_difference_years']\n",
    "    df[\"Lower CI\"] = (df['volume'] - df['volumetric_uncertainty'])/df['time_difference_years']\n",
    "    df[\"Average Date\"] = df['start_time'] + ((df['end_time'] - df['start_time']) / 2).dt.ceil('D')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49555e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_df_process_sums = enrich_volume_data(\n",
    "    dv_df_process_sums,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0],\n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "\n",
    "threshold_neg_dv_df_process_sums = enrich_volume_data(\n",
    "    threshold_neg_dv_df_process_sums,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0],\n",
    "    uncertainty_df = uncertainty_df\n",
    ")\n",
    "\n",
    "threshold_pos_dv_df_process_sums = enrich_volume_data(\n",
    "    threshold_pos_dv_df_process_sums,\n",
    "    pixel_area = demcollection.reference_dem.res[0] * demcollection.reference_dem.res[1],\n",
    "    pixel_side_length = demcollection.reference_dem.res[0],\n",
    "    uncertainty_df = uncertainty_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5000539",
   "metadata": {},
   "source": [
    "### Mark bounding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in [\n",
    "#         dv_df_process_sums,\n",
    "#         threshold_neg_dv_df_process_sums,\n",
    "#         threshold_pos_dv_df_process_sums\n",
    "# ]:\n",
    "#         df['bounding'] = df.apply(\n",
    "#         lambda row: \n",
    "#                 row['start_time'] == df.start_time.min() and row['end_time'] == df.end_time.max(),\n",
    "#                 axis = 1\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ed865b",
   "metadata": {},
   "source": [
    "### Calculate cumulative net volumetric change, by process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d34ae3",
   "metadata": {},
   "source": [
    "#### Remove bounding data for cumulative calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d035cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_dv_df = dv_df_process_sums.copy().drop(columns='index').reset_index(drop=True)\n",
    "# cum_dv_df = cum_dv_df[cum_dv_df['bounding'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cum_dv_df['volume'] = cum_dv_df['volume'].fillna(0)\n",
    "cum_dv_df['cumulative volume'] = cum_dv_df.groupby([\"type\"])['volume'].cumsum().reset_index(drop=True)\n",
    "cum_dv_df['Lower CI'] = 0\n",
    "cum_dv_df['Upper CI'] = 0 \n",
    "cum_dv_df = cum_dv_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b926150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cumulative_uncertainty(df):\n",
    "    df.loc[len(df) - 1, 'Lower CI'] = df.loc[len(df) - 1, 'cumulative volume'] - np.sqrt(\n",
    "        (df['volumetric_uncertainty']**2).sum()\n",
    "    )\n",
    "\n",
    "    df.loc[len(df) - 1, 'Upper CI'] = df.loc[len(df) - 1, 'cumulative volume'] + np.sqrt(\n",
    "        (df['volumetric_uncertainty']**2).sum()\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "cum_dv_df = pd.concat([\n",
    "    add_cumulative_uncertainty(cum_dv_df[cum_dv_df['type'] == type].reset_index(drop=True))\n",
    "    for type in cum_dv_df.groupby(\"type\").groups\n",
    "])\n",
    "\n",
    "\n",
    "for process in list(cum_dv_df.groupby(['type' ]).groups):\n",
    "    cum_dv_df = cum_dv_df.append({\n",
    "            'cumulative volume': 0,\n",
    "            'end_time': cum_dv_df.iloc[0]['start_time'],\n",
    "            'volumetric_uncertainty': 0,\n",
    "            'type': process\n",
    "        }, ignore_index=True\n",
    "    )\n",
    "cum_dv_df['end_time'] = cum_dv_df['end_time'].apply(pd.Timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d7143",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79abbe",
   "metadata": {},
   "source": [
    "## Plot incision rate per process in time\n",
    "\n",
    "Incision rate is calculated as the gross negative volume change divided by area of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = threshold_neg_dv_df_process_sums.copy()\n",
    "src['Annual Incision Rate'] = src['Annual Mass Wasted'] / src['area']\n",
    "# src = src.query(\"location != 5\")\n",
    "alt.Chart(src.drop(columns='index')).mark_line(\n",
    "    # strokeWidth = 1.5,\n",
    "    # stroke=\"white\",\n",
    "    # opacity=0.8\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Incision Rate:Q\", \n",
    "        title=\"Annualized incision rate, in m/yr\"\n",
    "    ),\n",
    "    alt.Facet(\"type:N\"),\n",
    "    # alt.Color(\"bounding:N\")\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46718dc7",
   "metadata": {},
   "source": [
    "## Plot annual volumetric change rate in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9095e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_neg_dv_df_process_sums['sign'] = 'negative'\n",
    "threshold_pos_dv_df_process_sums['sign'] = 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_neg_dv_df_process_sums['named interval'] = threshold_neg_dv_df_process_sums['index'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548e528",
   "metadata": {},
   "source": [
    "## Make erosion positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe603e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_neg_dv_df_process_sums.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc4b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_neg_dv_df_process_sums.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alt.Chart(\n",
    "    threshold_neg_dv_df_process_sums.drop(columns='index')\n",
    ").mark_bar(\n",
    "\n",
    ").encode(\n",
    "    alt.X('named interval:O', axis=alt.Axis(labelAngle=-45)),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", scale=alt.Scale(reverse=True))\n",
    ").properties(width=300, height = 300).facet(\n",
    "    column='type',\n",
    "    # column=''\n",
    ").configure_legend(titleFontSize=20, labelFontSize=16, orient='top').configure_axis(titleFontSize=20, labelFontSize=16, titleFontWeight='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_neg = alt.Chart().transform_filter(\n",
    "    alt.datum.sign == 'negative'\n",
    ").mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    "    color=\"red\"\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_neg = alt.Chart().transform_filter(\n",
    "    alt.datum.sign == 'negative'\n",
    ").mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "bars_pos = alt.Chart().transform_filter(\n",
    "    alt.datum.sign == 'positive'\n",
    ").mark_bar(\n",
    "    strokeWidth = 3,\n",
    "    stroke=\"white\",\n",
    "    color=\"blue\"\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y(\"Annual Mass Wasted:Q\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    ").properties(\n",
    "    # width=300, \n",
    "    # height=300\n",
    ")\n",
    "\n",
    "error_bars_pos = alt.Chart().transform_filter(\n",
    "    alt.datum.sign == 'positive'\n",
    ").mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"Average Date:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Annualized rate of volumetric change, in m³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a76f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.layer(\n",
    "    bars_neg,\n",
    "    error_bars_neg,\n",
    "    bars_pos,\n",
    "    error_bars_pos,\n",
    "    data = pd.concat([threshold_neg_dv_df_process_sums, threshold_pos_dv_df_process_sums]).drop(columns=\"index\")\n",
    ").properties(height=150).facet(row=\"type:N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b4de6",
   "metadata": {},
   "source": [
    "## Plot cumulative mass wasted per process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bb03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cum_dv_df.loc[4, 'end_time'] = cum_dv_df.loc[4, 'end_time'] - timedelta(days=300)\n",
    "# cum_dv_df.loc[9, 'end_time'] = cum_dv_df.loc[9, 'end_time'] - timedelta(days=100)\n",
    "# cum_dv_df.loc[14, 'end_time'] = cum_dv_df.loc[14, 'end_time'] + timedelta(days=100)\n",
    "# cum_dv_df.loc[19, 'end_time'] = cum_dv_df.loc[19, 'end_time'] + timedelta(days=300)\n",
    "\n",
    "# 4,9,14,19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b068b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_dv_df.query(\"type == 'fluvial'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = alt.Chart(cum_dv_df).transform_filter(\n",
    "    alt.datum.type != 'hillslope'\n",
    ").mark_area().encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        title='Cumulative net change, in m³/yr'\n",
    "    ),\n",
    "    alt.Color(\"type:N\")\n",
    ")\n",
    "\n",
    "\n",
    "cum_plot = alt.Chart().mark_line(point=True).transform_filter(\n",
    "    alt.datum.type == 'hillslope'\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        title='Cumulative net change, in m³/yr'\n",
    "    )\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2,\n",
    "    opacity=0.75\n",
    ").transform_filter(\n",
    "    alt.datum.type == 'hillslope'\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    ")\n",
    "\n",
    "\n",
    "alt.layer(\n",
    "    areas,\n",
    "    error_bars, \n",
    "    cum_plot, \n",
    "    data=cum_dv_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = alt.Chart(cum_dv_df).transform_filter(\n",
    "    alt.datum.type != 'hillslope'\n",
    ").mark_area().encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        title='Cumulative net change, in m³/yr'\n",
    "    ),\n",
    "    alt.Color(\"type:N\")\n",
    ")\n",
    "\n",
    "\n",
    "cum_plot = alt.Chart().mark_line(point=True).transform_filter(\n",
    "    alt.datum.type == 'hillslope'\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        title='Cumulative net change, in m³/yr'\n",
    "    )\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2,\n",
    "    opacity=0.75\n",
    ").transform_filter(\n",
    "    alt.datum.type == 'hillslope'\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    ")\n",
    "\n",
    "\n",
    "alt.layer(\n",
    "    areas,\n",
    "    error_bars, \n",
    "    cum_plot, \n",
    "    data=cum_dv_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f35f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.layer(\n",
    "    alt.Chart(cum_dv_df.query(\"type=='glacial debutressing'\")).transform_filter(alt.datum.bounding).mark_circle().encode(\n",
    "        alt.X('end_time'),\n",
    "        alt.Y(\"cumulative volume\")\n",
    "    ),\n",
    "    alt.Chart(cum_dv_df.query(\"type=='glacial debutressing'\")).transform_filter(~alt.datum.bounding).mark_line().encode(\n",
    "        alt.X('end_time'),\n",
    "        alt.Y(\"cumulative volume\")\n",
    "    ),\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_plot = alt.Chart().mark_line(point=True).transform_filter(\n",
    "    alt.datum.type != 'hillslope'\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        title='Cumulative net change, in m³/yr'\n",
    "    ),\n",
    "    alt.Color(\"type:N\")\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2,\n",
    "    opacity=0.75\n",
    ").transform_filter(\n",
    "    alt.datum.type != 'hillslope'\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    "    alt.Color(\"type:N\")\n",
    "\n",
    ")\n",
    "\n",
    "# bounding_points = alt.Chart().mark_circle(size=200).transform_filter(\n",
    "#     alt.datum.type == 'glacial debutressing'\n",
    "# ).encode(\n",
    "#     alt.X(\"end_time:T\"),\n",
    "#     alt.Y(\"cumulative volume:Q\"),\n",
    "#     alt.Color(\"type:N\")\n",
    "# )\n",
    "\n",
    "alt.layer(\n",
    "    error_bars, \n",
    "    cum_plot,\n",
    "    # bounding_points.transform_filter(alt.datum.bounding == True), \n",
    "    data=cum_dv_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_plot = alt.Chart().mark_line(point=True).transform_filter(\n",
    "    alt.datum.type != 'hillslope'\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        title='Cumulative net change, in m³/yr'\n",
    "    ),\n",
    "    alt.Color(\"type:N\")\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2,\n",
    "    opacity=0.75\n",
    ").transform_filter(\n",
    "    alt.datum.type != 'hillslope'\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    "    alt.Color(\"type:N\")\n",
    "\n",
    ")\n",
    "\n",
    "# bounding_points = alt.Chart().mark_circle(size=200).transform_filter(\n",
    "#     alt.datum.type == 'glacial debutressing'\n",
    "# ).encode(\n",
    "#     alt.X(\"end_time:T\"),\n",
    "#     alt.Y(\"cumulative volume:Q\"),\n",
    "#     alt.Color(\"type:N\")\n",
    "# )\n",
    "\n",
    "alt.layer(\n",
    "    error_bars, \n",
    "    cum_plot,\n",
    "    # bounding_points.transform_filter(alt.datum.bounding == True), \n",
    "    data=cum_dv_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ad8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_dv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_plot = alt.Chart().mark_line(point=True).encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        title='Cumulative net change, in m³/yr'\n",
    "    ),\n",
    "    alt.Color(\"type:N\")\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2,\n",
    "    opacity=0.75\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    "    alt.Color(\"type:N\")\n",
    "\n",
    ")\n",
    "\n",
    "alt.layer(\n",
    "    error_bars, \n",
    "    cum_plot, \n",
    "    data=cum_dv_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de87590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cum_dv_df.copy()\n",
    "src2 = cum_dv_df.copy()\n",
    "src2 = src2[~src2.type.isin(['hillslope', 'fluvial'])].groupby([\"end_time\", \"Average Date\"]).sum().reset_index()\n",
    "src2['type'] = 'sum of hillslope processes'\n",
    "src = pd.concat([src, src2])\n",
    "\n",
    "cum_plot = alt.Chart().mark_line(point=True).encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        title='Cumulative net change, in m³/yr'\n",
    "    ),\n",
    "    alt.Color(\"type:N\")\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2,\n",
    "    opacity=0.75\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    "    alt.Color(\"type:N\")\n",
    "\n",
    ")\n",
    "\n",
    "alt.layer(\n",
    "    error_bars, \n",
    "    cum_plot, \n",
    "    data=src\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_plot = alt.Chart().mark_line(point=True).encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        title='Cumulative net change, in m³/yr'\n",
    "    ),\n",
    "    alt.Color(\"type:N\")\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=1\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    "    alt.Color(\"type:N\")\n",
    "\n",
    ")\n",
    "\n",
    "alt.layer(\n",
    "    error_bars, \n",
    "    cum_plot, \n",
    "    data=cum_dv_df\n",
    ").facet(\"type:N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a48caa3",
   "metadata": {},
   "source": [
    "## Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    dv_df_process_sums,\n",
    "    threshold_neg_dv_df_process_sums,\n",
    "    threshold_pos_dv_df_process_sums,\n",
    "    cum_dv_df\n",
    "    \n",
    "]\n",
    "\n",
    "names = [\n",
    "    'dv_df_process_sums_process_bounding',\n",
    "    'threshold_neg_dv_df_process_sums_process_bounding',\n",
    "    'threshold_pos_dv_df_process_sums_process_bounding',\n",
    "    'cum_dv_df_process_bounding', \n",
    "]\n",
    "for df,name in zip(dfs, names):\n",
    "    df['valley'] = valley_name\n",
    "    if RUN_LARGER_AREA:\n",
    "        outdir = os.path.join(\"outputs\", \"larger_area\", name)\n",
    "    else:\n",
    "        outdir = os.path.join(\"outputs\", name)\n",
    "    outfile = os.path.join(outdir, valley_name + \".pickle\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    print(outfile)\n",
    "    df.to_pickle(outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22b7dc50fb8286be51844dc7799cfbbdb6bfe743b9c42cc7dfa69df0fcb613a9"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('xdem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
