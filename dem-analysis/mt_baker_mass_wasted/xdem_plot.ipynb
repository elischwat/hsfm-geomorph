{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import geoutils as gu\n",
    "import xdem\n",
    "from pprint import pprint\n",
    "import altair as alt    \n",
    "from rasterio.enums import Resampling\n",
    "import json \n",
    "import seaborn as sns\n",
    "from shapely import wkt\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xdem outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_files = glob.glob('outputs/cum_dv_df/*.pickle') ###\n",
    "largerarea_cum_files = glob.glob('outputs/larger_area/cum_dv_df/*.pickle') ###\n",
    "bounding_cum_files = glob.glob('outputs/bounding_cum_dv_df/*.pickle') ###\n",
    "threshold_pos_files = glob.glob('outputs/threshold_pos_dv_df/*.pickle') \n",
    "threshold_neg_files = glob.glob('outputs/threshold_neg_dv_df/*.pickle')\n",
    "hillslope_threshold_pos_files = glob.glob('outputs/hillslope_threshold_pos_dv_df/*.pickle') \n",
    "hillslope_threshold_neg_files = glob.glob('outputs/hillslope_threshold_neg_dv_df/*.pickle')\n",
    "fluvial_threshold_pos_files = glob.glob('outputs/fluvial_threshold_pos_dv_df/*.pickle') \n",
    "fluvial_threshold_neg_files = glob.glob('outputs/fluvial_threshold_neg_dv_df/*.pickle')\n",
    "\n",
    "cum_process_files = glob.glob('outputs/cum_dv_df_process/*.pickle')\n",
    "cum_process_bounding_files = glob.glob('outputs/cum_dv_df_process_bounding/*.pickle')\n",
    "# dv_df_process_sums_process/\n",
    "process_threshold_neg_files = glob.glob('outputs/threshold_neg_dv_df_process_sums_process/*.pickle')\n",
    "process_threshold_pos_files = glob.glob('outputs/threshold_pos_dv_df_process_sums_process/*.pickle')\n",
    "\n",
    "process_sums_files = glob.glob(\"outputs/dv_df_process_sums_process/*.pickle\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xsection outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_halfkm_files = glob.glob('outputs/slope_grouped_halfkm/*.pickle')\n",
    "slope_km_files = glob.glob('outputs/slope_grouped_km/*.pickle')\n",
    "elevation_files = glob.glob('outputs/elevation_profiles/*.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xdem outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross (pos and neg) datasets thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_pos_df = pd.concat(\n",
    "    [pd.read_pickle(f) for f in threshold_pos_files]\n",
    ")\n",
    "threshold_pos_df['type'] = \"positive\"\n",
    "threshold_neg_df = pd.concat(\n",
    "    [pd.read_pickle(f) for f in threshold_neg_files]\n",
    ")\n",
    "threshold_neg_df['type'] = \"negative\"\n",
    "gross_data_df = pd.concat([threshold_neg_df, threshold_pos_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative net and bounding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cum_df = pd.concat(\n",
    "    [pd.read_pickle(f) for f in cum_files]\n",
    ")\n",
    "\n",
    "bounding_cum_df = pd.concat(\n",
    "    [pd.read_pickle(f) for f in bounding_cum_files]\n",
    ")\n",
    "\n",
    "bounding_cum_df['bounding'] = True\n",
    "cum_df['bounding'] = False\n",
    "\n",
    "cum_and_bounding_cum_df = pd.concat([bounding_cum_df, cum_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative net dataset for larger area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "largerarea_cum_df = pd.concat(\n",
    "    [pd.read_pickle(f) for f in largerarea_cum_files]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross (pos and neg) datasets thresholded, by erosion type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillslope_threshold_pos_df = pd.concat([pd.read_pickle(f) for f in hillslope_threshold_pos_files])\n",
    "hillslope_threshold_pos_df['type'] = \"positive\"\n",
    "hillslope_threshold_pos_df['process'] = \"hillslope\"\n",
    "\n",
    "hillslope_threshold_neg_df = pd.concat([pd.read_pickle(f) for f in hillslope_threshold_neg_files])\n",
    "hillslope_threshold_neg_df['type'] = \"negative\"\n",
    "hillslope_threshold_neg_df['process'] = \"hillslope\"\n",
    "\n",
    "fluvial_threshold_pos_df = pd.concat([pd.read_pickle(f) for f in fluvial_threshold_pos_files])\n",
    "fluvial_threshold_pos_df['type'] = \"positive\"\n",
    "fluvial_threshold_pos_df['process'] = \"fluvial\"\n",
    "\n",
    "fluvial_threshold_neg_df = pd.concat([pd.read_pickle(f) for f in fluvial_threshold_neg_files])\n",
    "fluvial_threshold_neg_df['type'] = \"negative\"\n",
    "fluvial_threshold_neg_df['process'] = \"fluvial\"\n",
    "\n",
    "\n",
    "gross_data_bytype_df = pd.concat([hillslope_threshold_pos_df, hillslope_threshold_neg_df, fluvial_threshold_pos_df, fluvial_threshold_neg_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert \"Annual Mass Wasted\" into 1000s of cubic meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_data_df[\"Annual Mass Wasted\"] = gross_data_df[\"Annual Mass Wasted\"]/1000\n",
    "gross_data_df[\"Upper CI\"] = gross_data_df[\"Upper CI\"]/1000\n",
    "gross_data_df[\"Lower CI\"] = gross_data_df[\"Lower CI\"]/1000\n",
    "\n",
    "cum_and_bounding_cum_df[\"volume\"] = cum_and_bounding_cum_df[\"volume\"]/1000\n",
    "cum_and_bounding_cum_df[\"cumulative volume\"] = cum_and_bounding_cum_df[\"cumulative volume\"]/1000\n",
    "cum_and_bounding_cum_df[\"Upper CI\"] = cum_and_bounding_cum_df[\"Upper CI\"]/1000\n",
    "cum_and_bounding_cum_df[\"Lower CI\"] = cum_and_bounding_cum_df[\"Lower CI\"]/1000\n",
    "\n",
    "largerarea_cum_df[\"volume\"] = largerarea_cum_df[\"volume\"]/1000\n",
    "largerarea_cum_df[\"cumulative volume\"] = largerarea_cum_df[\"cumulative volume\"]/1000\n",
    "largerarea_cum_df[\"Upper CI\"] = largerarea_cum_df[\"Upper CI\"]/1000\n",
    "largerarea_cum_df[\"Lower CI\"] = largerarea_cum_df[\"Lower CI\"]/1000\n",
    "\n",
    "gross_data_bytype_df[\"Annual Mass Wasted\"] = gross_data_bytype_df[\"Annual Mass Wasted\"]/1000\n",
    "gross_data_bytype_df[\"Upper CI\"] = gross_data_bytype_df[\"Upper CI\"]/1000\n",
    "gross_data_bytype_df[\"Lower CI\"] = gross_data_bytype_df[\"Lower CI\"]/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xsection outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_halfkm_df = pd.concat(\n",
    "    [pd.read_pickle(f) for f in slope_halfkm_files]\n",
    ")\n",
    "slope_km_df = pd.concat(\n",
    "    [pd.read_pickle(f) for f in slope_km_files]\n",
    ")\n",
    "elevation_df = pd.concat(\n",
    "    [pd.read_pickle(f) for f in elevation_files]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gross erosion/accumulation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart().mark_bar(\n",
    "    strokeWidth = 2,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y('Annual Mass Wasted'),\n",
    "    alt.Color('type',\n",
    "        scale=alt.Scale(\n",
    "            domain=['negative', 'positive'],\n",
    "            range=['#d62728', '#1f77b4']\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X('Average Date:T'),\n",
    "    alt.Y(\"Lower CI\", title=\"\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "alt.layer(\n",
    "    bars, \n",
    "    error_bars.transform_filter(alt.datum.type == 'negative'), \n",
    "    error_bars.transform_filter(alt.datum.type == 'positive'),\n",
    "    data=gross_data_df.drop(columns=['index'])\n",
    ").properties(\n",
    "    height=100\n",
    ").facet(\n",
    "    row=alt.Row(\n",
    "        'valley:N', \n",
    "        header=alt.Header(labelOrient='top',labelFontWeight=\"bold\"),\n",
    "        title=\"Annualized rate of volumetric change, in 1,000s of mÂ³/yr\"\n",
    "    )\n",
    ").resolve_scale(y='shared')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative net erosion plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_plot = alt.Chart().mark_line(point=True).encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q'),\n",
    "    alt.Color(\"valley:N\")\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Cumulative net change, in 1,000s of mÂ³/yr\",),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    "    alt.Color(\"valley:N\")\n",
    ")\n",
    "\n",
    "alt.layer(\n",
    "    cum_plot,\n",
    "    error_bars,\n",
    "    data=cum_df.drop(columns='index')\n",
    ").properties(\n",
    "    # height=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative net erosion with 2015 data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_and_bounding_cum_df['larger_area'] = False\n",
    "largerarea_cum_df['larger_area'] = True\n",
    "largerarea_cum_df['bounding'] = False\n",
    "\n",
    "cum_and_bounding_cum_w_largerarea_df = pd.concat([cum_and_bounding_cum_df, largerarea_cum_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_and_bounding_cum_w_largerarea_df.loc[\n",
    "    cum_and_bounding_cum_w_largerarea_df['bounding'],\n",
    "    'end_time'\n",
    "] = cum_and_bounding_cum_w_largerarea_df.loc[\n",
    "    cum_and_bounding_cum_w_largerarea_df['bounding'],\n",
    "    'end_time'\n",
    "].apply(lambda date: date + timedelta(days=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_plot = alt.Chart().mark_line(point=True).encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q')\n",
    ").transform_filter(\n",
    "    (alt.datum.bounding == False) & (alt.datum.larger_area == False)\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", title=\"\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ").transform_filter(\n",
    "    (alt.datum.bounding == False) & (alt.datum.larger_area == False)\n",
    ")\n",
    "\n",
    "largerarea_cum_plot = alt.Chart().mark_line(\n",
    "    point=alt.OverlayMarkDef(color=\"grey\", opacity=0.5),\n",
    "    color='grey',\n",
    "    opacity=0.5\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q')\n",
    ").transform_filter(\n",
    "    (alt.datum.bounding == False) & (alt.datum.larger_area == True)\n",
    ")\n",
    "\n",
    "largerarea_error_bars = alt.Chart().mark_bar(\n",
    "    width=6,\n",
    "    color='grey',\n",
    "    opacity=0.5\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Cumulative net change, in 1,000s of mÂ³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ").transform_filter(\n",
    "    (alt.datum.bounding == False) & (alt.datum.larger_area == True)\n",
    ")\n",
    "\n",
    "bounding_point = alt.Chart().mark_square(shape='triangle', color='red', size=50).encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"volume:Q\"),\n",
    ").transform_filter(\n",
    "    (alt.datum.bounding == True) & (alt.datum.larger_area == False)\n",
    ")\n",
    "\n",
    "bounding_point_error_bars = alt.Chart().mark_bar(\n",
    "    color=\"red\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\", title=\"\"),\n",
    "    alt.Y(\"Lower CI\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ").transform_filter(\n",
    "    (alt.datum.bounding == True) & (alt.datum.larger_area == False)\n",
    ")\n",
    "\n",
    "y_chart = alt.layer(\n",
    "    largerarea_cum_plot,\n",
    "    largerarea_error_bars,\n",
    "    bounding_point,\n",
    "    bounding_point_error_bars,\n",
    "    cum_plot,\n",
    "    error_bars,\n",
    "    data=cum_and_bounding_cum_w_largerarea_df.drop(columns='index')\n",
    ").properties(\n",
    "    width=300, height=100\n",
    ").facet(\n",
    "    column=alt.Column(\n",
    "        'valley:N', \n",
    "        header=alt.Header(\n",
    "            labelOrient='top',\n",
    "            labelFontWeight=\"bold\",\n",
    "            # labelPadding=-10\n",
    "        ),\n",
    "        title=\"Cumulative net change, in 1,000s of mÂ³/yr\",  \n",
    "        \n",
    "    ),\n",
    "    spacing=1\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "\n",
    "y_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_plot = alt.Chart().mark_line(point=True).encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q')\n",
    ").transform_filter(\n",
    "    (alt.datum.bounding == False) & (alt.datum.larger_area == False)\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Cumulative net change, in 1,000s of mÂ³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ").transform_filter(\n",
    "    (alt.datum.bounding == False) & (alt.datum.larger_area == False)\n",
    ")\n",
    "\n",
    "largerarea_cum_plot = alt.Chart().mark_line(\n",
    "    point=alt.OverlayMarkDef(color=\"grey\", opacity=0.5),\n",
    "    color='grey',\n",
    "    opacity=0.5\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q')\n",
    ").transform_filter(\n",
    "    (alt.datum.bounding == False) & (alt.datum.larger_area == True)\n",
    ")\n",
    "\n",
    "largerarea_error_bars = alt.Chart().mark_bar(\n",
    "    width=6,\n",
    "    color='grey',\n",
    "    opacity=0.5\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", title=\"\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ").transform_filter(\n",
    "    (alt.datum.bounding == False) & (alt.datum.larger_area == True)\n",
    ")\n",
    "\n",
    "alt.layer(\n",
    "    largerarea_cum_plot,\n",
    "    largerarea_error_bars,\n",
    "    cum_plot,\n",
    "    error_bars,\n",
    "    data=cum_and_bounding_cum_w_largerarea_df.drop(columns='index')\n",
    ").properties(\n",
    "    width=300, height=100\n",
    ").facet(\n",
    "    column=alt.Column(\n",
    "        'valley:N', \n",
    "        header=alt.Header(\n",
    "            labelOrient='top',\n",
    "            labelFontWeight=\"bold\",\n",
    "            # labelPadding=-10\n",
    "        ),\n",
    "        title=\"Cumulative net change, in 1,000s of mÂ³/yr\",\n",
    "        \n",
    "    ),\n",
    "    spacing=1\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_plot = alt.Chart().mark_line(point=True).encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q')\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Cumulative net change, in 1,000s of mÂ³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "simple_cum_chart = alt.layer(\n",
    "    cum_plot.transform_filter((alt.datum.bounding == False) & (alt.datum.larger_area == False)),\n",
    "    error_bars.transform_filter((alt.datum.bounding == False) & (alt.datum.larger_area == False)),\n",
    "    data=cum_and_bounding_cum_w_largerarea_df.drop(columns='index')\n",
    ").properties(\n",
    "    width=300, height=100\n",
    ").facet(\n",
    "    column=alt.Column(\n",
    "        'valley:N', \n",
    "        header=alt.Header(\n",
    "            labelOrient='top',\n",
    "            labelFontWeight=\"bold\",\n",
    "            # labelPadding=-10\n",
    "        ),\n",
    "        title=\"Cumulative net change, in 1,000s of mÂ³/yr\",\n",
    "        \n",
    "    ),\n",
    "    # spacing=1\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "\n",
    "simple_cum_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Lithology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With erosion polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terrain_attrs_erosionarea = pd.read_csv(\"outputs/terrain_attributes_erosionarea.csv\")\n",
    "terrain_attrs_erosionarea = terrain_attrs_erosionarea.rename(columns={'name': 'Valley Name'})\n",
    "terrain_attrs_erosionarea['drainage area (km)'] = terrain_attrs_erosionarea['drainage area'] / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bareground_polys_gdf = gpd.GeoDataFrame(\n",
    "    terrain_attrs_erosionarea,\n",
    "    geometry = terrain_attrs_erosionarea['geometry'].apply(wkt.loads),\n",
    "    crs='EPSG:32610'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erosion_polys_fns = glob.glob(\"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/**/erosion.geojson\", recursive=True)\n",
    "erosion_gdf = pd.concat([gpd.read_file(f) for f in erosion_polys_fns])\n",
    "\n",
    "gully_polygons_fn = \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/gully.shp\"\n",
    "wasting_polygons_fn = \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/mass_wasting.shp\"\n",
    "glacial_debutressing_polygons_fn = \"/data2/elilouis/hsfm-geomorph/data/mt_baker_mass_wasted/whole_mountain/debutressing.shp\"\n",
    "\n",
    "erosion_polygons_gdf = pd.concat([\n",
    "    gpd.read_file(gully_polygons_fn),\n",
    "    gpd.read_file(wasting_polygons_fn),\n",
    "    gpd.read_file(glacial_debutressing_polygons_fn)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "lithology_gdf = gpd.read_file(\"/data2/elilouis/geology/q111shp_cropped_mtbaker/gunit_polygon.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_gdf.plot(column='GUNIT_LABE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bareground_lithology_by_valley = bareground_polys_gdf.groupby(\"Valley Name\").apply(lithology_gdf.clip).reset_index()\n",
    "bareground_lithology_by_valley['AREA'] = bareground_lithology_by_valley.geometry.area\n",
    "bareground_lithology_by_valley = bareground_lithology_by_valley.query(\"GUNIT_LABE != 'ice'\")\n",
    "\n",
    "\n",
    "erosion_lithology_by_valley = erosion_polygons_gdf.groupby('name').apply(lithology_gdf.clip).reset_index()\n",
    "\n",
    "erosion_lithology_by_valley['AREA'] = erosion_lithology_by_valley.geometry.area\n",
    "erosion_lithology_by_valley = erosion_lithology_by_valley.query(\"GUNIT_LABE != 'ice'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create lithology dataset for each valley and combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gunit_convert = {\n",
    "#     'Qat': \"Pleistocene,at,alpine glacial till,loose material\",\n",
    "#     'Qva(b)': \"Holocene-Pleistocene,va,andesite flows,igneous\",\n",
    "#     'KJm(n1)': \"Cretaceous-Jurassic,m,marine sedimentary rocks,sedimentary\",\n",
    "#     'Qad': \"Pleistocene,ad,alpine glacial drift,loose material\",\n",
    "#     'Kigb': \"Cretaceous,igb,gabbro,volcanic\",\n",
    "#     'JPMhmc(b)': \"Jurassic-Permian,hmc,heterogeneous metamorphic rocks,metamorphic\",\n",
    "#     'Qvx(b)': \"Quaternary,vx,volcanic breccia,volcanic\",\n",
    "#     'KJm(n2)': \"Cretaceous-Jurassic,m,marine sedimentary rocks,sedimentary\",\n",
    "#     'Qls': \"Quaternary,ls,mass-wasting deposits,loose material\",\n",
    "#     'Migd': \"Miocene,igd,granodiorite,igneous\",\n",
    "#     'Qta': \"Holocene-Pleistocene,ta,talus deposits,loose material\",\n",
    "#     'Qva(p)': \"Quaternary,va,andesite flows,igneous\",\n",
    "#     'Qvt(ks)': \"Pleistocene,vt,tuffs and tuff breccias,volcanic\",\n",
    "#     'KJm(n4)': \"Cretaceous-Jurassic,m,marine sedimentary rocks,sedimentary\",\n",
    "#     'Qva(ld)': \"Quaternary,va,andesite flows,igneous\",\n",
    "#     'PMvb(c)': \"Permian,vb,basalt flows,igneous\",\n",
    "#     'Qva(bb)': \"Quaternary,va,andesite flows,igneous\",\n",
    "# }\n",
    "\n",
    "gunit_convert = {\n",
    "    'Qat': \"Pleistocene,at,alpine glacial till,Pleistocene glaciogenic material\",\n",
    "    'Qva(b)': \"Holocene-Pleistocene,va,andesite flows,Igneous\",\n",
    "    'KJm(n1)': \"Cretaceous-Jurassic,m,marine sedimentary rocks,Sedimentary\",\n",
    "    'Qad': \"Pleistocene,ad,alpine glacial drift,Pleistocene glaciogenic material\",\n",
    "    'Kigb': \"Cretaceous,igb,gabbro,Igneous\",\n",
    "    'JPMhmc(b)': \"Jurassic-Permian,hmc,heterogeneous metamorphic rocks,Metamorphic\",\n",
    "    'Qvx(b)': \"Quaternary,vx,volcanic breccia,Volcanic\",\n",
    "    'KJm(n2)': \"Cretaceous-Jurassic,m,marine sedimentary rocks,Sedimentary\",\n",
    "    'Qls': \"Quaternary,ls,mass-wasting deposits,Quaternary mass wasting deposits\",\n",
    "    'Migd': \"Miocene,igd,granodiorite,Igneous\",\n",
    "    'Qta': \"Holocene-Pleistocene,ta,talus deposits,Holocene-pleistocene talus\",\n",
    "    'Qva(p)': \"Quaternary,va,andesite flows,Igneous\",\n",
    "    'Qvt(ks)': \"Pleistocene,vt,tuffs and tuff breccias,Volcanic\",\n",
    "    'KJm(n4)': \"Cretaceous-Jurassic,m,marine sedimentary rocks,Sedimentary\",\n",
    "    'Qva(ld)': \"Quaternary,va,andesite flows,Igneous\",\n",
    "    'PMvb(c)': \"Permian,vb,basalt flows,Igneous\",\n",
    "    'Qva(bb)': \"Quaternary,va,andesite flows,Igneous\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bareground_lithology_by_valley['unit description'] = bareground_lithology_by_valley['GUNIT_LABE'].apply(gunit_convert.get)\n",
    "bareground_lithology_by_valley['description'] = bareground_lithology_by_valley['unit description'].apply(lambda s: s.split(\",\")[-1])\n",
    "\n",
    "erosion_lithology_by_valley['unit description'] = erosion_lithology_by_valley['GUNIT_LABE'].apply(gunit_convert.get)\n",
    "erosion_lithology_by_valley['description'] = erosion_lithology_by_valley['unit description'].apply(lambda s: s.split(\",\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot proportion of each lithology by area in each valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = erosion_lithology_by_valley.copy()\n",
    "# normalize area of of each unit by total area measured in each valley\n",
    "src['AREA'] = src['AREA'] / src.groupby('name')['AREA'].transform('sum')\n",
    "\n",
    "alt.Chart(src).mark_arc().encode(\n",
    "    alt.Theta(\"AREA:Q\"),\n",
    "    alt.Color(\"unit description:N\"),\n",
    "    alt.Facet(\"name:N\", columns=5)\n",
    ").properties(width=100, height=100).configure_view(strokeWidth=0).configure_legend(labelLimit=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(\n",
    "    pd.DataFrame(bareground_lithology_by_valley.groupby(['description', 'unit description'])['AREA'].sum()).reset_index()\n",
    ").mark_bar().encode(\n",
    "    alt.Y(\"unit description:N\", sort='x', axis=alt.Axis(labelLimit=500, title='')),\n",
    "    alt.X(\"AREA:Q\", title='Area (mÂ²)'),\n",
    "    alt.Color(\"description:N\")\n",
    ").properties(\n",
    "    title = 'Total area covered by surface lithology units in measured bareground area in 10 watersheds'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_common_5_units = bareground_lithology_by_valley.groupby(['description', 'unit description'])['AREA'].sum().sort_values().reset_index()['unit description'].head(5)\n",
    "\n",
    "least_common_5_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = bareground_lithology_by_valley.copy()\n",
    "\n",
    "# REMOVE LEAST COMMON 5 UNITS\n",
    "src = src[~src['unit description'].isin(least_common_5_units)]\n",
    "\n",
    "# normalize area of of each unit by total area measured in each valley\n",
    "src['AREA'] = src['AREA'] / src.groupby('Valley Name')['AREA'].transform('sum')\n",
    "\n",
    "alt.Chart(src).mark_arc().encode(\n",
    "    alt.Theta(\"AREA:Q\"),\n",
    "    alt.Color(\"unit description:N\"),\n",
    "    alt.Facet(\"Valley Name:N\", columns=5)\n",
    ").properties(\n",
    "    width=100, height=100,\n",
    "    title = 'Relative prevalence of surface lithology units in measured bareground area'\n",
    ").configure_view(strokeWidth=0).configure_legend(labelLimit=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bareground_lithology_by_valley['description'].unique()\n",
    "res = bareground_lithology_by_valley.groupby([\"Valley Name\", \"description\"])[['AREA']].sum()\n",
    "denom = res.groupby('Valley Name')['AREA'].sum()\n",
    "res['AREA'] = res['AREA'] / denom\n",
    "res = res.reset_index()\n",
    "alt.Chart(\n",
    "    res\n",
    ").mark_arc().encode(\n",
    "    theta=alt.Theta(field=\"AREA\", type=\"quantitative\"),\n",
    "    color=alt.Color(field=\"description\", type=\"nominal\"),\n",
    "    facet=alt.Facet(\"Valley Name:N\", columns=5),\n",
    "    # groupby = \"Valley Name:N\"\n",
    ").properties(width=100, height=100).configure_view(strokeWidth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_data =  res.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With process polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_polys_gdf = gpd.GeoDataFrame(\n",
    "    pd.read_csv(\"outputs/terrain_attributes_processpolygons.csv\"),\n",
    "    geometry = pd.read_csv(\"outputs/terrain_attributes_processpolygons.csv\")['geometry'].apply(wkt.loads),\n",
    "    crs='EPSG:32610'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_polys_gdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_polys_gdf.geometry.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_geometry(row):\n",
    "    res = lithology_gdf.clip(row.geometry)\n",
    "    res['AREA'] = res.geometry.area\n",
    "    res['volume'] = res['AREA']*row['ddem']\n",
    "    res['type'] = row['type']\n",
    "    return res\n",
    "\n",
    "results = process_polys_gdf.apply(process_geometry, axis=1)\n",
    "lithology_by_process = pd.concat(list(results))\n",
    "lithology_by_process = lithology_by_process.query(\"GUNIT_LABE != 'ice'\")\n",
    "lithology_by_process['unit description'] = lithology_by_process['GUNIT_LABE'].apply(gunit_convert.get)\n",
    "lithology_by_process['description'] = lithology_by_process['unit description'].apply(lambda s: s.split(\",\")[-1])\n",
    "lithology_by_process = lithology_by_process.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot proportion of each lithology by eroded volume, organized by process typem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lithology_by_process.groupby([\"type\", \"description\"])[['volume']].sum()\n",
    "denom = res.groupby('type')['volume'].sum()\n",
    "res['volume'] = res['volume'] / denom\n",
    "res = res.reset_index()\n",
    "alt.Chart(\n",
    "    res\n",
    ").mark_arc().encode(\n",
    "    theta=alt.Theta(field=\"volume\", type=\"quantitative\"),\n",
    "    color=alt.Color(field=\"description\", type=\"nominal\"),\n",
    "    facet=alt.Facet(\"type:N\", columns=5),\n",
    "    # groupby = \"Valley Name:N\"\n",
    ").properties(width=100, height=100).configure_view(strokeWidth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot proportion of each lithology by area, organized by process type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lithology_by_process.groupby([\"type\", \"description\"])[['AREA']].sum()\n",
    "denom = res.groupby('type')['AREA'].sum()\n",
    "res['AREA'] = res['AREA'] / denom\n",
    "res = res.reset_index()\n",
    "alt.Chart(\n",
    "    res\n",
    ").mark_arc().encode(\n",
    "    theta=alt.Theta(field=\"AREA\", type=\"quantitative\"),\n",
    "    color=alt.Color(field=\"description\", type=\"nominal\"),\n",
    "    facet=alt.Facet(\"type:N\", columns=5),\n",
    "    # groupby = \"Valley Name:N\"\n",
    ").properties(width=100, height=100).configure_view(strokeWidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gross erosion/accumulation plots by process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart().mark_bar(\n",
    "    strokeWidth = 2,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y('Annual Mass Wasted'),\n",
    "    alt.Color('type',\n",
    "        scale=alt.Scale(\n",
    "            domain=['negative', 'positive'],\n",
    "            range=['#d62728', '#1f77b4']\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X('Average Date:T'),\n",
    "    alt.Y(\"Lower CI\", title=\"\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "alt.layer(\n",
    "    bars, \n",
    "    error_bars.transform_filter(alt.datum.type == 'negative'), \n",
    "    error_bars.transform_filter(alt.datum.type == 'positive'),\n",
    "    data=gross_data_bytype_df.drop(columns=['index'])\n",
    ").properties(\n",
    "    height=200\n",
    ").facet(\n",
    "    row=alt.Row(\n",
    "        'valley:N', \n",
    "        header=alt.Header(\n",
    "            # labelOrient='top',\n",
    "            labelFontWeight=\"bold\", \n",
    "            labelPadding=0\n",
    "        ),\n",
    "        title=\"Annualized rate of volumetric change, in 1,000s of mÂ³/yr\"\n",
    "    ),\n",
    "    column=alt.Column(\"process:N\"),\n",
    "    spacing=1\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "gross_data_bytype_df['Average Date Plus'] = gross_data_bytype_df['Average Date'] + timedelta(days=120)\n",
    "gross_data_bytype_df['Average Date Minus'] = gross_data_bytype_df['Average Date'] - timedelta(days=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart().mark_bar(\n",
    "    strokeWidth = 2,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y('Annual Mass Wasted'),\n",
    "    alt.Color('type',\n",
    "        scale=alt.Scale(\n",
    "            domain=['negative', 'positive'],\n",
    "            range=['#d62728', '#1f77b4']\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().encode(\n",
    "    alt.X('Average Date:T'),\n",
    "    alt.Y(\"Lower CI\", title=\"\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "alt.layer(\n",
    "    bars, \n",
    "    error_bars.transform_filter(alt.datum.type == 'negative').mark_bar(color=\"black\", width=2).encode(alt.X('Average Date Minus:T')), \n",
    "    error_bars.transform_filter(alt.datum.type == 'positive').mark_bar(color=\"black\", width=2).encode(alt.X('Average Date Plus:T')),\n",
    "    data=gross_data_bytype_df.drop(columns=['index'])\n",
    ").properties(\n",
    "    height=200\n",
    ").facet(\n",
    "    row=alt.Row(\n",
    "        'valley:N', \n",
    "        header=alt.Header(\n",
    "            # labelOrient='top',\n",
    "            labelFontWeight=\"bold\", \n",
    "            labelPadding=0\n",
    "        ),\n",
    "        title=\"Annualized rate of volumetric change, in 1,000s of mÂ³/yr\"\n",
    "    ),\n",
    "    column=alt.Column(\"process:N\"),\n",
    "    spacing=1\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart().mark_bar(\n",
    "    strokeWidth = 2,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y('Annual Mass Wasted'),\n",
    "    alt.Color('type',\n",
    "        scale=alt.Scale(\n",
    "            domain=['negative', 'positive'],\n",
    "            range=['#d62728', '#1f77b4']\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X('Average Date:T'),\n",
    "    alt.Y(\"Lower CI\", title=\"\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "layer = alt.layer(\n",
    "    bars, \n",
    "    error_bars.transform_filter(alt.datum.type == 'negative'), \n",
    "    error_bars.transform_filter(alt.datum.type == 'positive'),\n",
    "    data=gross_data_bytype_df.drop(columns=['index'])\n",
    ").properties(\n",
    "    height=100\n",
    ").facet(\n",
    "    row=alt.Row(\n",
    "        'valley:N', \n",
    "        header=alt.Header(\n",
    "            # labelOrient='top',\n",
    "            labelFontWeight=\"bold\", \n",
    "            # labelPadding=-10\n",
    "        ),\n",
    "        title=\"Annualized rate of volumetric change, in 1,000s of mÂ³/yr\"\n",
    "    ),\n",
    "    spacing=1\n",
    ")\n",
    "\n",
    "layer.transform_filter(\n",
    "    alt.datum.process == 'fluvial'\n",
    ").properties(title='fluvial') | layer.transform_filter(\n",
    "    alt.datum.process == 'hillslope'\n",
    ").properties(title='hillslope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valley slope plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainbow: 0 - 72\n",
    "# Mazama: 0 - 63\n",
    "# Deming: 0 - 69\n",
    "# Coleman: 0 - ...\n",
    "# Easton: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = elevation_df.query(\"valley == 'Rainbow'\").query(\n",
    "    \"n_from_glacial_max > 0\"\n",
    ").query(\n",
    "    \"n_from_glacial_max < 72\"\n",
    ").sort_values(\n",
    "    \"n_from_glacial_max\"\n",
    ").query(\"time == '2015_09_01'\")\n",
    "\n",
    "np.degrees(np.arctan(\n",
    "    (df.iloc[-1]['elevation'] - df.iloc[0]['elevation']) / (\n",
    "        df.iloc[-1]['path_distance_from_glacier'] - df.iloc[0]['path_distance_from_glacier']\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = elevation_df.query(\"valley == 'Mazama'\").query(\n",
    "    \"n_from_glacial_max > 0\"\n",
    ").query(\n",
    "    \"n_from_glacial_max < 63\"\n",
    ").sort_values(\n",
    "    \"n_from_glacial_max\"\n",
    ").query(\"time == '2015_09_01'\")\n",
    "\n",
    "np.degrees(np.arctan(\n",
    "    (df.iloc[-1]['elevation'] - df.iloc[0]['elevation']) / (\n",
    "        df.iloc[-1]['path_distance_from_glacier'] - df.iloc[0]['path_distance_from_glacier']\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = elevation_df.query(\"valley == 'Deming'\").query(\n",
    "    \"n_from_glacial_max > 0\"\n",
    ").query(\n",
    "    \"n_from_glacial_max < 69\"\n",
    ").sort_values(\n",
    "    \"n_from_glacial_max\"\n",
    ").query(\"time == '2015_09_01'\")\n",
    "\n",
    "np.degrees(np.arctan(\n",
    "    (df.iloc[-1]['elevation'] - df.iloc[0]['elevation']) / (\n",
    "        df.iloc[-1]['path_distance_from_glacier'] - df.iloc[0]['path_distance_from_glacier']\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = elevation_df.query(\"valley == 'Coleman'\").query(\n",
    "    \"n_from_glacial_max > 0\"\n",
    ").sort_values(\n",
    "    \"n_from_glacial_max\"\n",
    ").query(\"time == '2015_09_01'\")\n",
    "\n",
    "np.degrees(np.arctan(\n",
    "    (df.iloc[-1]['elevation'] - df.iloc[0]['elevation']) / (\n",
    "        df.iloc[-1]['path_distance_from_glacier'] - df.iloc[0]['path_distance_from_glacier']\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(slope_halfkm_df).mark_line(point=True).encode(\n",
    "    alt.X('time:T', title=\"\"),\n",
    "    alt.Y('slope:Q', title=\"Valley floor slope\"),\n",
    "    alt.Color(\"valley:N\"),\n",
    "    alt.Facet('Half kilometer downstream from glacier:O', title='Half kilometer downstream from glacier')\n",
    ").properties(width=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(slope_km_df).mark_line(point=True).encode(\n",
    "    alt.X('time:T', title=\"\"),\n",
    "    alt.Y('slope:Q', title=\"Valley floor slope\"),\n",
    "    alt.Color(\"valley:N\"),\n",
    "    alt.Facet('Kilometer downstream from glacier:O', title='Kilometer downstream from glacier')\n",
    ").properties(width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_df.time.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_df[elevation_df['time'].isin(['1947_09_14', '1970_09_29', '1977_09_27', '1979_10_06', '2015_09_01'])].time.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_df.query(\"valley =='Rainbow'\").time.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_df.query(\"valley =='Mazama'\").time.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_df.query(\"valley =='Deming'\").time.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_df = elevation_df[elevation_df['time'].isin(['1947_09_14', '1970_09_29', '1977_09_27', '1979_10_06', '2015_09_01'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_df = elevation_df[elevation_df['time'].isin(['1947_09_14', '1970_09_29', '1977_09_27', '1979_10_06', '2015_09_01'])]\n",
    "# elevation_df = elevation_df[~(\n",
    "#     (elevation_df['time'] == '1979_10_06') & (elevation_df['valley'] == 'Rainbow')\n",
    "# )]\n",
    "# elevation_df = elevation_df[~(\n",
    "#     (elevation_df['time'] == '1979_10_06') & (elevation_df['valley'] == 'Mazama')\n",
    "# )]\n",
    "elevation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = elevation_df[[ \"time\", \"path_distance_from_glacier\", \"elevation\", \"glacial\", \"valley\"]].reset_index()\n",
    "alt.Chart(\n",
    "    src\n",
    ").mark_line(\n",
    "    strokeWidth=1, clip=True\n",
    ").transform_filter(\n",
    "    {'field': 'valley', 'oneOf': ['Deming', 'Mazama', 'Rainbow']}\n",
    ").transform_filter(\n",
    "    alt.datum.glacial==False\n",
    ").encode(\n",
    "    alt.X(\n",
    "        \"path_distance_from_glacier:Q\", \n",
    "        title=\"Distance downstream from observed glacial maximum\", \n",
    "        scale=alt.Scale(domain=[-800, 1800])\n",
    "    ),\n",
    "    alt.Y(\n",
    "        \"elevation:Q\", \n",
    "        scale=alt.Scale(domain=[950, 1200]), \n",
    "        title=\"Valley floor elevation, in meters\"\n",
    "    ),\n",
    "    alt.StrokeDash(\"time:O\", scale=alt.Scale(\n",
    "            domain = ['1947_09_14', '1970_09_29', '1977_09_27', '1979_10_06', '2015_09_01'],\n",
    "            range= [[6,1.5], [4, 2.5], [1, 2],[1, 2], [1, 0]]\n",
    "        )\n",
    "    ),\n",
    "    alt.Color(\"valley:N\"),\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 500,\n",
    "    title='Elevation profile of main stream channel downstream of glaciers'\n",
    ").configure_axis(grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = elevation_df[[ \"time\", \"path_distance_from_glacier\", \"elevation\", \"glacial\", \"valley\"]].reset_index()\n",
    "src.loc\n",
    "src.loc[src.valley == 'Rainbow', 'elevation'] = src.loc[src.valley == 'Rainbow', 'elevation'] + 30\n",
    "src.loc[src.valley == 'Deming', 'elevation'] = src.loc[src.valley == 'Deming', 'elevation'] + 10\n",
    "alt.Chart(\n",
    "    src\n",
    ").mark_line(\n",
    "    strokeWidth=1, clip=True\n",
    ").transform_filter(\n",
    "    {'field': 'valley', 'oneOf': ['Deming', 'Mazama', 'Rainbow']}\n",
    ").transform_filter(\n",
    "    alt.datum.glacial==False\n",
    ").encode(\n",
    "    alt.X(\n",
    "        \"path_distance_from_glacier:Q\", \n",
    "        title=\"Distance downstream from observed glacial maximum\", \n",
    "        scale=alt.Scale(domain=[-400, 1900])\n",
    "    ),\n",
    "    alt.Y(\n",
    "        \"elevation:Q\", \n",
    "        scale=alt.Scale(domain=[950, 1200]), \n",
    "        title=\"Valley floor elevation, in meters\"\n",
    "    ),\n",
    "    alt.StrokeDash(\"time:O\", scale=alt.Scale(\n",
    "            domain = ['1947_09_14', '1970_09_29', '1977_09_27', '1979_10_06', '2015_09_01'],\n",
    "            range= [[8,1.5], [4, 2.5], [1, 2],[1, 2], [1, 0]]\n",
    "        )\n",
    "    ),\n",
    "    alt.Color(\"valley:N\"),\n",
    ").properties(\n",
    "    width = 1000,\n",
    "    height = 500,\n",
    "    title={\n",
    "        \"text\":'Elevation profile of main stream channel downstream of glaciers',\n",
    "        \"subtitle\": \"Rainbow data shifted upwards 30 meters, Deming data shifted upwards 10 meters for clarity.\"\n",
    "    }\n",
    ").configure_axis(grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in [\n",
    "    '2015_09_01',\n",
    "    '1970_09_29',\n",
    "    '1947_09_14'\n",
    "]:\n",
    "    rainbow_slope = src.query(\"valley == 'Rainbow'\").query(f\"time == '{date}'\").query(\n",
    "    \"path_distance_from_glacier < 1200\").query(\"path_distance_from_glacier > 0\").sort_values(\"path_distance_from_glacier\")\n",
    "\n",
    "    print(np.degrees(np.arctan(\n",
    "    (rainbow_slope['elevation'].iloc[-1] - rainbow_slope['elevation'].iloc[0]) / (\n",
    "        rainbow_slope['path_distance_from_glacier'].iloc[-1] - rainbow_slope['path_distance_from_glacier'].iloc[0]\n",
    "    )\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in [\n",
    "    '2015_09_01',\n",
    "    '1970_09_29',\n",
    "    '1947_09_14'\n",
    "]:\n",
    "    mazama_slope = src.query(\"valley == 'Mazama'\").query(f\"time == '{date}'\").query(\n",
    "        \"path_distance_from_glacier < 400\").query(\"path_distance_from_glacier > 0\").sort_values(\"path_distance_from_glacier\")\n",
    "\n",
    "    print(np.degrees(np.arctan(\n",
    "    (mazama_slope['elevation'].iloc[-1] - mazama_slope['elevation'].iloc[0]) / (\n",
    "        mazama_slope['path_distance_from_glacier'].iloc[-1] - mazama_slope['path_distance_from_glacier'].iloc[0]\n",
    "    )\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in [\n",
    "    '2015_09_01',\n",
    "    '1979_10_06',\n",
    "    '1970_09_29',\n",
    "    '1947_09_14'\n",
    "]:\n",
    "    deming_slope = src.query(\"valley == 'Deming'\").query(f\"time == '{date}'\").query(\n",
    "        \"path_distance_from_glacier < 1400\").query(\"path_distance_from_glacier > 0\").sort_values(\"path_distance_from_glacier\")\n",
    "    print(\n",
    "    np.degrees(np.arctan(\n",
    "    (deming_slope['elevation'].iloc[-1] - deming_slope['elevation'].iloc[0]) / (\n",
    "        deming_slope['path_distance_from_glacier'].iloc[-1] - deming_slope['path_distance_from_glacier'].iloc[0]\n",
    "    )\n",
    "    )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative net erosion plots by process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_process_df = pd.concat(\n",
    "    [pd.read_pickle(f) for f in cum_process_files]\n",
    ")\n",
    "\n",
    "cum_process_df['cumulative volume'] = cum_process_df['cumulative volume']/1000\n",
    "cum_process_df['Lower CI'] = cum_process_df['Lower CI']/1000\n",
    "cum_process_df['Upper CI'] = cum_process_df['Upper CI']/1000\n",
    "\n",
    "cum_process_df['type'] = cum_process_df['type'].apply(lambda existing_type: 'glacial' if existing_type == 'glacial debutressing' else existing_type)\n",
    "cum_process_df['bounding'] = False\n",
    "\n",
    "cum_process_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_process_bounding_df = pd.concat(\n",
    "    [pd.read_pickle(f) for f in cum_process_bounding_files]\n",
    ")\n",
    "\n",
    "cum_process_bounding_df['cumulative volume'] = cum_process_bounding_df['cumulative volume']/1000\n",
    "cum_process_bounding_df['Lower CI'] = cum_process_bounding_df['Lower CI']/1000\n",
    "cum_process_bounding_df['Upper CI'] = cum_process_bounding_df['Upper CI']/1000\n",
    "\n",
    "cum_process_bounding_df['type'] = cum_process_bounding_df['type'].apply(lambda existing_type: 'glacial' if existing_type == 'glacial debutressing' else existing_type)\n",
    "\n",
    "cum_process_bounding_df['bounding'] = True\n",
    "\n",
    "cum_process_bounding_df = cum_process_bounding_df.dropna(subset='start_time')\n",
    "\n",
    "cum_process_bounding_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = cum_and_bounding_cum_w_largerarea_df.query(\"~bounding\").query(\"~larger_area\").query(\"valley != 'Easton'\")[['valley', 'end_time','cumulative volume','Lower CI','Upper CI']]\n",
    "all_data['type'] = 'all'\n",
    "bytype_data = cum_process_df.query(\"valley != 'Easton'\")\n",
    "bytype_bounding_data = cum_process_bounding_df.query(\"valley != 'Easton'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_data_alltogether = pd.concat([bytype_data, bytype_bounding_data, all_data])[['valley', 'start_time', 'end_time','cumulative volume','type','Lower CI','Upper CI', 'bounding', 'Annual Mass Wasted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_plot = alt.Chart().mark_line(point=True).encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q'),\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Cumulative net change, in 1,000s of mÂ³/yr\"),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "simple_cum_chart = alt.layer(\n",
    "    cum_plot.transform_filter((alt.datum.bounding == False) & (alt.datum.larger_area == False)),\n",
    "    error_bars.transform_filter((alt.datum.bounding == False) & (alt.datum.larger_area == False)),\n",
    "    data=cum_and_bounding_cum_w_largerarea_df.drop(columns='index')\n",
    ").properties(\n",
    "    width=300, height=100\n",
    ").facet(\n",
    "    column=alt.Column(\n",
    "        'valley:N', \n",
    "        header=alt.Header(\n",
    "            labelOrient='top',\n",
    "            labelFontWeight=\"bold\",\n",
    "            # labelPadding=-10\n",
    "        ),\n",
    "        title=\"Cumulative net change, in 1,000s of mÂ³/yr\",\n",
    "        \n",
    "    ),\n",
    "    # spacing=1\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "\n",
    "simple_cum_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cum_and_bounding_cum_w_largerarea_df.drop(columns='index')\n",
    "src = src[src['Upper CI'] != 0].query(\"bounding == False\")\n",
    "src = src.dropna()\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "src = cum_and_bounding_cum_w_largerarea_df.drop(columns='index')\n",
    "src['cumulative volume'] = - src['cumulative volume']/1000\n",
    "\n",
    "# domain = [\"1940-01-01\", \"2020-01-01\"]\n",
    "domain = [\"1940\", \"2020\"]\n",
    "\n",
    "cum_plot = alt.Chart().mark_line().encode(\n",
    "    # alt.X('end_time:T', title='Time', timeUnit='yearmonthdate', scale=alt.Scale(domain=domain)),\n",
    "    alt.X('end_time:T', title='Time', timeUnit='year', scale=alt.Scale(domain=domain)),\n",
    "    alt.Y('cumulative volume:Q'),\n",
    "    alt.Color('valley:N')\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Cumulative net change, in 1,000s of mÂ³/yr\"),\n",
    "    # alt.Color('valley:N')\n",
    ")\n",
    "\n",
    "simple_cum_chart = alt.layer(\n",
    "    cum_plot.transform_filter((alt.datum.bounding == False) & (alt.datum.larger_area == False)),\n",
    "    # error_bars.transform_filter((alt.datum.bounding == False) & (alt.datum.larger_area == False)),\n",
    "    data=src\n",
    ").properties(\n",
    "    width=350, height=200\n",
    ")\n",
    "\n",
    "simple_cum_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale=alt.Scale(domain=['fluvial', 'hillslope', 'all'], range=['#1f77b4', '#ff7f0e', '#d62728'])\n",
    "\n",
    "src = gross_data_bytype_df.drop(columns=['index'])\n",
    "src = src.query(\"valley != 'Easton'\")\n",
    "# src = src.query(\"valley != 'Coleman'\")\n",
    "\n",
    "src['Annual Mass Wasted'] = src['Annual Mass Wasted'].apply(lambda x: 1 if x == 0 else x)\n",
    "src['Upper CI'] = src['Upper CI'].apply(lambda x: 1 if x == 0 else x)\n",
    "src['Lower CI'] = src['Lower CI'].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "src['Annual Mass Wasted'] = - src['Annual Mass Wasted']\n",
    "src['Upper CI'] = - src['Upper CI']\n",
    "src['Lower CI'] = - src['Lower CI']\n",
    "\n",
    "bars_neg = alt.Chart().mark_bar(\n",
    "    strokeWidth = 2,\n",
    "    stroke=\"white\",\n",
    ").encode(\n",
    "    alt.X('start_time:T', axis=alt.Axis(labels=False), title=None),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y('Annual Mass Wasted', scale=alt.Scale(domain=[-50,100], nice=False)),\n",
    "    alt.Color(\"valley:N\")\n",
    ")\n",
    "\n",
    "bars_pos = alt.Chart().mark_bar(\n",
    "    strokeWidth = 2,\n",
    "    stroke=\"white\",\n",
    "    opacity=0.5\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y('Annual Mass Wasted'),\n",
    "    alt.Color(\"valley:N\")\n",
    "\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X('Average Date:T'),\n",
    "    alt.Y(\"Lower CI\", title=[\n",
    "        # \"Annualized rate of volumetric change,\", \"in 1,000s of mÂ³/yr\"\n",
    "        ]),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "hillslope_combo_gross_bars_chart = alt.layer(\n",
    "    bars_neg.transform_filter(alt.datum.type == 'negative').transform_filter(alt.datum.process == 'hillslope'), \n",
    "    bars_pos.transform_filter(alt.datum.type == 'positive').transform_filter(alt.datum.process == 'hillslope'), \n",
    "    error_bars.transform_filter(alt.datum.type == 'negative').transform_filter(alt.datum.process == 'hillslope'), \n",
    "    error_bars.transform_filter(alt.datum.type == 'positive').transform_filter(alt.datum.process == 'hillslope'),\n",
    "    data=src\n",
    ").properties(\n",
    "    width=350,\n",
    "    height=150\n",
    ").facet(\n",
    "    row=alt.Column(\n",
    "        'valley:N', \n",
    "        header=alt.Header(\n",
    "            labels=False,\n",
    "            labelFontSize=16\n",
    "            # labelOrient='top',\n",
    "            # labelFontWeight=\"bold\", \n",
    "            # labelPadding=0\n",
    "        ),\n",
    "        title=None\n",
    "    ),\n",
    "    # spacing=1\n",
    ")\n",
    "\n",
    "hillslope_combo_gross_bars_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hillslope_combo_gross_bars_chart.transform_filter(alt.datum.valley != 'Coleman') & simple_cum_chart).configure_legend(\n",
    "(hillslope_combo_gross_bars_chart.transform_filter(alt.datum.valley != 'Coleman').transform_filter(alt.datum.valley != 'Rainbow')\n",
    " & simple_cum_chart).configure_legend(\n",
    "    titleFontSize=18, \n",
    "    labelFontSize=18, \n",
    "    orient='top', \n",
    "    symbolSize=1000, \n",
    "    symbolStrokeWidth=2\n",
    ").configure_axis(\n",
    "    labelFontSize=18, \n",
    "    titleFontSize=18,\n",
    ").resolve_scale(\n",
    "    x='shared'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_data_alltogether.loc[cum_data_alltogether['bounding'] == True, 'end_time'] = cum_data_alltogether.loc[cum_data_alltogether['bounding'] == True, 'end_time'].apply(lambda date: date + timedelta(days=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_data_alltogether.loc[cum_data_alltogether[\"type\"] == 'all', 'bounding'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cum_data_alltogether\n",
    "src = src.query(\"valley != 'Easton'\")\n",
    "part1 = src[~src['type'].isin(['gully', 'mass wasting'])]\n",
    "part2 = src[src['type'].isin(['gully', 'mass wasting'])].groupby(['valley', 'start_time', 'end_time', 'bounding']).sum().reset_index().assign(type='not glacial')\n",
    "src = pd.concat([part1, part2])\n",
    "\n",
    "src = src.append(pd.DataFrame({\n",
    "    'valley': ['Coleman', 'Deming', 'Mazama', 'Rainbow'],\n",
    "    'end_time': [datetime(1947, 9, 14), datetime(1947, 9, 14), datetime(1947, 9, 14), datetime(1947, 9, 14)],\n",
    "    'cumulative volume': [0,0,0,0],\n",
    "    'bounding': [False, False, False, False],\n",
    "    'type': ['not glacial', 'not glacial', 'not glacial', 'not glacial']\n",
    "}))\n",
    "\n",
    "cum_plot = alt.Chart().mark_line(\n",
    "    point={'size':20},\n",
    "     strokeWidth=2.5,\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        scale=alt.Scale(domain=[-4000, 500])\n",
    "    ),\n",
    "    # alt.Color(\"type:N\")\n",
    "    alt.Color(\n",
    "        \"type:N\", \n",
    "        scale=alt.Scale(domain=['fluvial', 'hillslope', 'all', 'glacial', 'not glacial'], range=['#1f77b4', '#d62728', '#000000', '#17becf', '#2ca02c'])\n",
    "    ),\n",
    "    alt.StrokeDash(\"type:N\")\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", ),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    "    alt.Color(\n",
    "        \"type:N\", \n",
    "        scale=alt.Scale(domain=['fluvial', 'hillslope', 'all', 'glacial', 'not glacial'], range=['#1f77b4', '#d62728', '#000000', '#17becf', '#2ca02c'])\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "cumulative_by_process_chart_no_glacial = alt.layer(\n",
    "    cum_plot.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['all', 'fluvial', 'hillslope'])).transform_filter(alt.datum.bounding == False),\n",
    "    error_bars.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['all', 'fluvial', 'hillslope'])).transform_filter(alt.datum.bounding == False),\n",
    "    # cum_plot2.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['not glacial', 'glacial'])).transform_filter(alt.datum.bounding == False),\n",
    "    # error_bars2.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['not glacial', 'glacial'])).transform_filter(alt.datum.bounding == False),\n",
    "    # bounding_points.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['fluvial', 'hillslope', 'all'])).transform_filter(alt.datum.bounding == True),\n",
    "    # bounding_points2.transform_filter(alt.datum.type == 'glacial').transform_filter(alt.datum.bounding == True),\n",
    "    data=src\n",
    ").properties(\n",
    "    width=350\n",
    ").facet(\n",
    "    column=alt.Column('valley:N', header=alt.Header(labels=False, labelFontSize=16), title=''),\n",
    ")\n",
    "cumulative_by_process_chart_no_glacial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cum_data_alltogether\n",
    "src = src.query(\"valley != 'Easton'\")\n",
    "part1 = src[~src['type'].isin(['gully', 'mass wasting'])]\n",
    "part2 = src[src['type'].isin(['gully', 'mass wasting'])].groupby(['valley', 'start_time', 'end_time', 'bounding']).sum().reset_index().assign(type='not glacial')\n",
    "src = pd.concat([part1, part2])\n",
    "\n",
    "src = src.append(pd.DataFrame({\n",
    "    'valley': ['Coleman', 'Deming', 'Mazama', 'Rainbow'],\n",
    "    'end_time': [datetime(1947, 9, 14), datetime(1947, 9, 14), datetime(1947, 9, 14), datetime(1947, 9, 14)],\n",
    "    'cumulative volume': [0,0,0,0],\n",
    "    'bounding': [False, False, False, False],\n",
    "    'type': ['not glacial', 'not glacial', 'not glacial', 'not glacial']\n",
    "}))\n",
    "\n",
    "cum_plot = alt.Chart().mark_line(\n",
    "        point={'size':20},\n",
    "     strokeWidth=2.5,\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        scale=alt.Scale(domain=[-4000, 500])\n",
    "    ),\n",
    "    # alt.Color(\"type:N\")\n",
    "    alt.Color(\n",
    "        \"type:N\", \n",
    "        scale=alt.Scale(domain=['fluvial', 'hillslope', 'all', 'glacial', 'not glacial'], range=['#1f77b4', '#d62728', '#000000', '#17becf', '#2ca02c'])\n",
    "    )\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", ),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    "    alt.Color(\n",
    "        \"type:N\", \n",
    "        scale=alt.Scale(domain=['fluvial', 'hillslope', 'all', 'glacial', 'not glacial'], range=['#1f77b4', '#d62728', '#000000', '#17becf', '#2ca02c'])\n",
    "    )\n",
    ")\n",
    "\n",
    "cum_plot2 = alt.Chart().mark_line(\n",
    "    # color='black', \n",
    "    strokeWidth=2.5, \n",
    "    strokeDash=[7, 7]\n",
    "    # point={'size':20, 'color': 'black'}\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        # scale=alt.Scale(domain=[-5000, 1000])\n",
    "    ),\n",
    "    alt.Color(\n",
    "        \"type:N\",\n",
    "    )\n",
    ")\n",
    "\n",
    "error_bars2 = alt.Chart().mark_bar(\n",
    "    width=2,\n",
    "    # color='black'\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", title=[\"Cumulative net volumetric change\", \"(10Â³ mÂ³/yr)\"]),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    "    alt.Color(\n",
    "        \"type:N\",\n",
    "    )\n",
    ")\n",
    "\n",
    "bounding_points = alt.Chart().mark_circle(size=100, ).encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"cumulative volume:Q\"),\n",
    "    alt.Color(\n",
    "        \"type:N\", \n",
    "        scale=alt.Scale(domain=['fluvial', 'hillslope', 'all', 'glacial', 'not glacial'], range=['#1f77b4', '#d62728', '#000000', '#17becf', '#2ca02c'])\n",
    "    )\n",
    ")\n",
    "\n",
    "bounding_points2 = alt.Chart().mark_circle(size=100, color='#17becf').encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"cumulative volume:Q\")\n",
    ")\n",
    "\n",
    "cumulative_by_process_chart = alt.layer(\n",
    "    cum_plot.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['all', 'fluvial', 'hillslope'])).transform_filter(alt.datum.bounding == False),\n",
    "    error_bars.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['all', 'fluvial', 'hillslope'])).transform_filter(alt.datum.bounding == False),\n",
    "    cum_plot2.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['not glacial', 'glacial'])).transform_filter(alt.datum.bounding == False),\n",
    "    error_bars2.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['not glacial', 'glacial'])).transform_filter(alt.datum.bounding == False),\n",
    "    # bounding_points.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['fluvial', 'hillslope', 'all'])).transform_filter(alt.datum.bounding == True),\n",
    "    bounding_points2.transform_filter(alt.datum.type == 'glacial').transform_filter(alt.datum.bounding == True),\n",
    "    data=src\n",
    ").properties(\n",
    "    width=350\n",
    ").facet(\n",
    "    column=alt.Column('valley:N', header=alt.Header(labels=False, labelFontSize=16), title=''),\n",
    ")\n",
    "cumulative_by_process_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.to_csv('outputs/final_figures_data/time_series_cumulative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cum_data_alltogether\n",
    "src = src.query(\"valley != 'Easton'\")\n",
    "part1 = src[~src['type'].isin(['gully', 'mass wasting'])]\n",
    "part2 = src[src['type'].isin(['gully', 'mass wasting'])].groupby(['valley', 'start_time', 'end_time', 'bounding']).sum().reset_index().assign(type='not glacial')\n",
    "src = pd.concat([part1, part2])\n",
    "\n",
    "src = src.append(pd.DataFrame({\n",
    "    'valley': ['Coleman', 'Deming', 'Mazama', 'Rainbow'],\n",
    "    'end_time': [datetime(1947, 9, 14), datetime(1947, 9, 14), datetime(1947, 9, 14), datetime(1947, 9, 14)],\n",
    "    'cumulative volume': [0,0,0,0],\n",
    "    'bounding': [False, False, False, False],\n",
    "    'type': ['not glacial', 'not glacial', 'not glacial', 'not glacial']\n",
    "}))\n",
    "\n",
    "cum_plot = alt.Chart().mark_line(\n",
    "     strokeWidth=2.5,\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        scale=alt.Scale(domain=[-4000, 500])\n",
    "    ),\n",
    "    # alt.Color(\"type:N\")\n",
    "    alt.Color(\n",
    "        \"type:N\", \n",
    "        scale=alt.Scale(domain=['fluvial', 'hillslope', 'all', 'glacial', 'not glacial'], range=['#1f77b4', '#d62728', '#000000', '#17becf', '#2ca02c'])\n",
    "    )\n",
    ")\n",
    "\n",
    "cum_plot2 = alt.Chart().mark_line(\n",
    "    # color='black', \n",
    "    strokeWidth=2.5, \n",
    "    strokeDash=[7, 7]\n",
    "\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        # scale=alt.Scale(domain=[-5000, 1000])\n",
    "    ),\n",
    "    alt.Color(\n",
    "        \"type:N\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "alt.layer(\n",
    "    cum_plot.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['all', 'fluvial', 'hillslope'])).transform_filter(alt.datum.bounding == False),\n",
    "    \n",
    "    cum_plot2.transform_filter(alt.FieldOneOfPredicate(field='type', oneOf=['not glacial', 'glacial'])).transform_filter(alt.datum.bounding == False),\n",
    "    \n",
    "    data=src\n",
    ").properties(\n",
    "    width=350\n",
    ").facet(\n",
    "    column=alt.Column('valley:N', header=alt.Header(labels=False, labelFontSize=16), title=''),\n",
    ").configure_legend(\n",
    "    titleFontSize=16, \n",
    "    labelFontSize=14, \n",
    "    orient='top', \n",
    "    symbolSize=1000, \n",
    "    symbolStrokeWidth=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cum_data_alltogether\n",
    "src = src.query(\"valley != 'Easton'\")\n",
    "# src = src.query(\"valley != 'Coleman'\")\n",
    "\n",
    "\n",
    "cum_plot = alt.Chart().mark_line(\n",
    "        point={'size':20},\n",
    "     strokeWidth=2.5,\n",
    ").encode(\n",
    "    alt.X('end_time:T', title='Time'),\n",
    "    alt.Y('cumulative volume:Q', \n",
    "        # scale=alt.Scale(domain=[-4000, 500])\n",
    "    ),\n",
    "    # alt.Color(\"type:N\")\n",
    "    alt.Color(\"valley:N\")\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X(\"end_time:T\"),\n",
    "    alt.Y(\"Lower CI\", title=\"Cumulative net change, in 1,000s of mÂ³/yr\"),\n",
    "    alt.Y2(\"Upper CI\"),\n",
    "    alt.Color(\"valley:N\")\n",
    ")\n",
    "\n",
    "\n",
    "cumulative_process_facet_valley_color = alt.layer(\n",
    "    cum_plot.transform_filter(alt.datum.bounding == False),\n",
    "    error_bars.transform_filter(alt.datum.bounding == False),\n",
    "    data=src\n",
    ").properties(\n",
    "    width=350\n",
    ").facet(\n",
    "    column=alt.Column('type:N', \n",
    "    header=alt.Header(labelFontSize=16), title=''),\n",
    ").resolve_scale(y='independent')\n",
    "cumulative_process_facet_valley_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = gross_data_bytype_df.drop(columns=['index'])\n",
    "src = src.query(\"valley != 'Easton'\")\n",
    "# src = src.query(\"valley != 'Coleman'\")\n",
    "\n",
    "src['Annual Mass Wasted'] = src['Annual Mass Wasted'].apply(lambda x: 1 if x == 0 else x)\n",
    "src['Upper CI'] = src['Upper CI'].apply(lambda x: 1 if x == 0 else x)\n",
    "src['Lower CI'] = src['Lower CI'].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "bars_neg = alt.Chart().mark_bar(\n",
    "    strokeWidth = 2,\n",
    "    stroke=\"white\",\n",
    "    color='#1f77b4'\n",
    ").encode(\n",
    "    alt.X('start_time:T', axis=alt.Axis(labels=False), title=None),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y('Annual Mass Wasted'),\n",
    ")\n",
    "\n",
    "bars_pos = alt.Chart().mark_bar(\n",
    "    strokeWidth = 2,\n",
    "    stroke=\"white\",\n",
    "    color='#1f77b4',\n",
    "    opacity=0.3\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y('Annual Mass Wasted'),\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X('Average Date:T'),\n",
    "    alt.Y(\"Lower CI\", title=[\"Annual volumetric change\", \"(10Â³ mÂ³/yr)\"]),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "fluvial_combo_gross_bars_chart = alt.layer(\n",
    "    bars_neg.transform_filter(alt.datum.type == 'negative').transform_filter(alt.datum.process == 'fluvial'), \n",
    "    bars_pos.transform_filter(alt.datum.type == 'positive').transform_filter(alt.datum.process == 'fluvial'), \n",
    "    error_bars.transform_filter(alt.datum.type == 'negative').transform_filter(alt.datum.process == 'fluvial'), \n",
    "    error_bars.transform_filter(alt.datum.type == 'positive').transform_filter(alt.datum.process == 'fluvial'),\n",
    "    data=src\n",
    ").properties(\n",
    "    width=350,\n",
    "    height=200\n",
    ").facet(\n",
    "    column=alt.Column(\n",
    "        'valley:N', \n",
    "        header=alt.Header(\n",
    "            labelFontSize=24\n",
    "            # labelOrient='top',\n",
    "            # labelFontWeight=\"bold\", \n",
    "            # labelPadding=0\n",
    "        ),\n",
    "        title=None\n",
    "    ),\n",
    "    # spacing=1\n",
    ")\n",
    "fluvial_combo_gross_bars_chart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale=alt.Scale(domain=['fluvial', 'hillslope', 'all'], range=['#1f77b4', '#ff7f0e', '#d62728'])\n",
    "\n",
    "src = gross_data_bytype_df.drop(columns=['index'])\n",
    "src = src.query(\"valley != 'Easton'\")\n",
    "# src = src.query(\"valley != 'Coleman'\")\n",
    "\n",
    "src['Annual Mass Wasted'] = src['Annual Mass Wasted'].apply(lambda x: 1 if x == 0 else x)\n",
    "src['Upper CI'] = src['Upper CI'].apply(lambda x: 1 if x == 0 else x)\n",
    "src['Lower CI'] = src['Lower CI'].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "bars_neg = alt.Chart().mark_bar(\n",
    "    strokeWidth = 2,\n",
    "    stroke=\"white\",\n",
    "    color='#d62728'\n",
    ").encode(\n",
    "    alt.X('start_time:T', axis=alt.Axis(labels=False), title=None),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y('Annual Mass Wasted'),\n",
    ")\n",
    "\n",
    "bars_pos = alt.Chart().mark_bar(\n",
    "    strokeWidth = 2,\n",
    "    stroke=\"white\",\n",
    "    color='#d62728',\n",
    "    opacity=0.3\n",
    ").encode(\n",
    "    alt.X('start_time:T'),\n",
    "    alt.X2('end_time:T'),\n",
    "    alt.Y('Annual Mass Wasted'),\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart().mark_bar(\n",
    "    color=\"black\",\n",
    "    width=2\n",
    ").encode(\n",
    "    alt.X('Average Date:T'),\n",
    "    alt.Y(\"Lower CI\", title=[\n",
    "        # \"Annualized rate of volumetric change,\", \"in 1,000s of mÂ³/yr\"\n",
    "        ]),\n",
    "    alt.Y2(\"Upper CI\")\n",
    ")\n",
    "\n",
    "hillslope_combo_gross_bars_chart = alt.layer(\n",
    "    bars_neg.transform_filter(alt.datum.type == 'negative').transform_filter(alt.datum.process == 'hillslope'), \n",
    "    bars_pos.transform_filter(alt.datum.type == 'positive').transform_filter(alt.datum.process == 'hillslope'), \n",
    "    error_bars.transform_filter(alt.datum.type == 'negative').transform_filter(alt.datum.process == 'hillslope'), \n",
    "    error_bars.transform_filter(alt.datum.type == 'positive').transform_filter(alt.datum.process == 'hillslope'),\n",
    "    data=src\n",
    ").properties(\n",
    "    width=350,\n",
    "    height=200\n",
    ").facet(\n",
    "    column=alt.Column(\n",
    "        'valley:N', \n",
    "        header=alt.Header(\n",
    "            labels=False,\n",
    "            labelFontSize=16\n",
    "            # labelOrient='top',\n",
    "            # labelFontWeight=\"bold\", \n",
    "            # labelPadding=0\n",
    "        ),\n",
    "        title=None\n",
    "    ),\n",
    "    # spacing=1\n",
    ")\n",
    "\n",
    "hillslope_combo_gross_bars_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    fluvial_combo_gross_bars_chart & hillslope_combo_gross_bars_chart & cumulative_by_process_chart\n",
    ").configure_legend(\n",
    "    titleFontSize=16, \n",
    "    labelFontSize=14, \n",
    "    orient='top', \n",
    "    symbolSize=1000, \n",
    "    symbolStrokeWidth=2\n",
    ").configure_axis(\n",
    "    labelFontSize=24, \n",
    "    titleFontSize=24\n",
    ").resolve_scale(\n",
    "    x='shared'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    fluvial_combo_gross_bars_chart & hillslope_combo_gross_bars_chart & cumulative_by_process_chart_no_glacial\n",
    ").configure_legend(\n",
    "    titleFontSize=16, \n",
    "    labelFontSize=14, \n",
    "    orient='top', \n",
    "    symbolSize=1000, \n",
    "    symbolStrokeWidth=2\n",
    ").configure_axis(\n",
    "    labelFontSize=24, \n",
    "    titleFontSize=24\n",
    ").resolve_scale(\n",
    "    x='shared'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cum_data_alltogether\n",
    "src = src.query(\"valley != 'Easton'\")\n",
    "# src = src.query(\"valley != 'Coleman'\")\n",
    "src.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_chart(\n",
    "    src, \n",
    "    type, \n",
    "    x_axis=alt.Axis(), \n",
    "    facet_column=alt.Column('valley:N')\n",
    "):\n",
    "    return alt.Chart(src).transform_filter(\n",
    "        alt.datum.bounding == False\n",
    "    ).transform_filter(\n",
    "        alt.datum.type == type\n",
    "    ).mark_bar(\n",
    "        strokeWidth = 2,\n",
    "        stroke=\"white\"\n",
    "    ).encode(\n",
    "        alt.X('start_time:T', axis=x_axis),\n",
    "        alt.X2('end_time:T'),\n",
    "        alt.Y('Annual Mass Wasted')\n",
    "    ).properties(\n",
    "        height=175\n",
    "    ).facet(\n",
    "        column=facet_column\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_col = alt.Column('valley:N', title=None, header=alt.Header(labels=False))\n",
    "alt.vconcat(\n",
    "    type_chart(src.sample(src.shape[0]), 'hillslope', x_axis=None),\n",
    "    type_chart(src.sample(src.shape[0]), 'fluvial', x_axis=None, facet_column=alt_col),\n",
    "    type_chart(src.sample(src.shape[0]), 'mass wasting', x_axis=None, facet_column=alt_col),\n",
    "    type_chart(src.sample(src.shape[0]), 'gully', x_axis=None, facet_column=alt_col),\n",
    "    type_chart(src.sample(src.shape[0]), 'glacial', facet_column=alt_col),\n",
    ").resolve_scale(x='shared')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_bar = cum_data_alltogether.query(\"bounding == True\")\n",
    "src_bar = src_bar[src_bar['type'].isin(['glacial', 'gully', 'mass wasting'])]\n",
    "src_bar = src_bar.groupby(\"type\").sum().reset_index()\n",
    "src_bar['cumulative volume'] = - src_bar['cumulative volume']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(\n",
    "    src_bar\n",
    ").mark_bar().encode(\n",
    "    alt.X('type:N', axis=alt.Axis(labelAngle=0)),\n",
    "    alt.Y(\"cumulative volume:Q\", axis=alt.Axis(tickCount=5), title='Total volumetric change (1,000s mÂ³/yr)')\n",
    ").properties(width=600, height=400).configure_legend(\n",
    "    titleFontSize=16, \n",
    "    labelFontSize=14, \n",
    "    orient='top', \n",
    "    symbolSize=1000, \n",
    "    symbolStrokeWidth=2\n",
    ").configure_axis(\n",
    "    labelFontSize=24, \n",
    "    titleFontSize=16\n",
    ").display(renderer='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_bar['is_glacial'] = src_bar['type'] == 'glacial'\n",
    "alt.Chart(\n",
    "    src_bar\n",
    ").mark_bar().encode(\n",
    "    alt.X('is_glacial:N'),\n",
    "    alt.Color(\"type:N\", ),\n",
    "    alt.Y(\"cumulative volume:Q\", axis=alt.Axis(tickCount=5))\n",
    ").properties(width=600, height=400).configure_legend(\n",
    "    titleFontSize=16, \n",
    "    labelFontSize=14, \n",
    "    orient='top', \n",
    "    symbolSize=1000, \n",
    "    symbolStrokeWidth=2\n",
    ").configure_axis(\n",
    "    labelFontSize=24, \n",
    "    titleFontSize=16\n",
    ").display(renderer='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_df = pd.read_csv(\"/data2/elilouis/yield_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_data_bytype_df[[\n",
    "    'Annual Mass Wasted',\n",
    "    'Upper CI',\n",
    "    'Lower CI',\n",
    "    'start_time',\n",
    "    'end_time',\n",
    "    'Average Date',\n",
    "    \"process\",\n",
    "    \"type\",\n",
    "    \"valley\",\n",
    "]].to_csv('outputs/final_figures_data/time_series_annualized_gross.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save lithology data for igneous-fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_data = lithology_data.query(\"description != 'Igneous'\")\n",
    "lithology_data = lithology_data.groupby(\"Valley Name\").sum().reset_index()\n",
    "lithology_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_data.to_csv(\"outputs/lithology.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22b7dc50fb8286be51844dc7799cfbbdb6bfe743b9c42cc7dfa69df0fcb613a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('xdem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
